experName ....  24.4.3-1540-sciemd-size_100-re_org-test
Processing index 1794...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: NONLINEAR NONEQUILIBRIUM NONQUANTUM NONCHAOTIC STATISTICAL MECHANICS OF NEOCORTICAL INTERACTIONS  
Abstract: Abstract: The work in progress reported by Wright & Liley shows great promise, primarily because of their experimental and simulation paradigms. However, their tentative conclusion that macroscopic neocortex may be considered (approximately) a linear near-equilibrium system is premature and does not correspond to tentative conclusions drawn from other studies of neocortex. At this time, there exists an interdisciplinary multidimensional gradation on published studies of neocortex, with one primary dimension of mathematical physics represented by two extremes. At one extreme, there is much scientifically unsupported talk of chaos and quantum physics being responsible for many important macroscopic neocortical processes (involving many thousands to millions of neurons) (Wilczek, 1994). At another extreme, many non-mathematically trained neuroscientists uncritically lump all neocortical mathematical theory into one file, and consider only statistical averages of citations for opinions on the quality of that research (Nunez, 1995). In this context, it is important to appreciate that Wright and Liley (W&L) report on their scientifically sound studies on macroscopic neocortical function, based on simulation and a blend of sound theory and reproducible experiments. However, their pioneering work, given the absence of much knowledge of neocortex at this time, is open to criticism, especially with respect to their present inferences and conclusions. Their conclusion that EEG data exhibit linear near-equilibrium dynamics may very well be true, but only in the sense of focusing only on one local minima, possibly with individual-specific and physiological-state dependent 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 3:  <Title: Application of statistical mechanics methodol- ogy to term-structure bond-pricing models, Mathl. Comput. Modelling Application of.   Abstract: The work in reported Wright Liley shows great promise primarily their experimental and simulation paradigms However their tentative conclusion macroscopic neocortex may considered (approximately a linear near-equilibrium system premature does correspond tentative conclusions At exists an interdisciplinary multidimensional gradation on published studies neocortex one primary dimension mathematical physics represented two extremes At one extreme much scie>
Label: Neural Networks
Paper 4:  <Title: Replicability of Neural Computing Experiments  .   Abstract: If an experiment requires statistical analysis establish one do a better experiment Ernest Rutherford 1930 Most proponents cold fusion reporting excess heat their electrolysis experiments claiming one its irreproducibility | 1993 78 Abstract Amid the ever increasing research various aspects neural computing much progress evident both theoretical advances On the empirical side a wealth experimental studies>
Label: Neural Networks
Paper 6:  <Title: Statistical mechanics of neocortical interactions: Training and testing canonical momenta indicators of EEG  .   Abstract: A series papers has developed a statistical mechanics neocortical interactionsSMNI deriving aggregate behavior experimentally observed columns statistical electrical-chemical properties synaptic interactions While not useful yield insights at the single neuron level SMNI demonstrated its capability describing The necessity including nonlinear and stochastic structures this development stressed Sets EEG evoked>
Label: Neural Networks
Paper 2:  <Title: Evaluating and Improving Steady State Evolutionary Algorithms on Constraint Satisfaction Problems  .   Abstract: The work in reported Wright Liley shows great promise primarily their experimental and simulation paradigms However their tentative conclusion macroscopic neocortex may considered (approximately a linear near-equilibrium system premature does correspond tentative conclusions At exists an interdisciplinary multidimensional gradation on published studies neocortex one primary dimension mathematical physics represented two extremes At one extreme much scie>
Paper 5:  <Title: MULTIPLE SCALES OF BRAIN-MIND INTERACTIONS  .   Abstract: Posner and Raichle's Images Mind very Some aws as a scientific publication: ( the accuracy the linear subtraction method PET is subject scrutiny further research at finer spatial-temporal resolutions accuracy the experimental paradigm EEG complementary studies. Images (Posner Raichle 1994 is cognitive and imaging science Well written illustrated presents concepts well both the layman/undergr>
Paper 7:  <Title: Book Review  Introduction to the Theory of Neural Computation Reviewed by: 2  .   Abstract: Neural computation, also connectionism parallel distributed processing neural network modeling or brain-style computation grown Despite this explosion and ultimately because impressive applications a dire need a concise introduction from a theoretical perspective analyzing the strengths connectionist approaches establishing links statistics control theory The Introduction the Theory Neural Computation by Hertz Krogh Palmersubsequently referred>
Paper 8:  <Title: Studies of Neurological Transmission Analysis using Hierarchical Bayesian Mixture Models  .   Abstract: Hierarchically structured mixture models studied inference on neural synaptic transmission characteristics mammalian, and Mixture structures arise due uncertainties governing electro-chemical stimulation individual neuro-transmitter release sites at Models attempt capture scientific features such the sensitivity individual synaptic transmission sites w>
Paper 9:  <Title: Coevolving Communicative Behavior in a Linear Pursuer-Evader Game  .   Abstract: The pursuer-evader (PE) game recognized an important domain which robust adaptive behavior protean behavior (Miller Cliff Nevertheless the potential the game largely unrealized due methodological hurdles coevolutionary simulation raised PE; versions optimal solutions (Isaacs 1965 closed other formulations opaque with their solution space for the lack a rigorous metric agent behavior This inability characterize behavior, in obfuscates coevolutio>
Paper 10:  <Title: A Brief History of Connectionism  .   Abstract: Connectionist research firmly within especially cognitive science This diversity, however created which makes connectionist researchers remain aware recent advances let understand the field developed This paper attempts address a brief guide connectionist research The paper begins defining the basic tenets connectionism Next the development connectionist research traced, commencing>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Neural Networks

Neural Networks
Prediction:  Neural Networks
Is prediction correct?  True

Prediction: 1
Processing index 455...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Learning from an Automated Training Agent  
Abstract: Abstract: A learning agent employing reinforcement learning is hindered because it only receives the critic's sparse and weakly informative training information. We present an approach in which an automated training agent may also provide occasional instruction to the learner in the form of actions for the learner to perform. The learner has access to both the critic's feedback and the trainer's instruction. In the experiments, we vary the level of the trainer's interaction with the learner, from allowing the trainer to instruct the learner at almost every time step, to not allowing the trainer to respond at all. We also vary a parameter that controls how the learner incorporates the trainer's actions. The results show significant reductions in the average number of training trials necessary to learn to perform the task.
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 7:  <Title: Learning Optimal Dialogue Strategies: A Case Study of a Spoken Dialogue Agent for Email  .   Abstract: This paper by a dialogue agent learn choose an optimal dialogue strategy While widely agreed dialogue strategies should formulated terms communicative intentions little work automatically optimizing an agent's choices when there multiple ways realize Our method learning algorithms empirical evaluation techniques The learning component our method based algorithms for reinforcement learning such dynamic programming Q>
Label: Reinforcement Learning
Paper 2:  <Title: An Introspection Approach to Querying a Trainer  .   Abstract: Technical Report 96-13 January 22 Abstract introduces the Introspection Approach by a learning agent employing decide ask instruction When using our approach find trainer's responses produced significantly faster learners by having ask aid randomly received via our approach more informative random guidance Thus reduce that the training agent without whic>
Paper 3:  <Title: Modeling the Student with Reinforcement Learning  .   Abstract: We describe a methodology enabling an intelligent teaching system make high level strategy decisions on low level student modeling information This framework less costly construct superior hand coding teaching strategies as responsive In order accomplish reinforcement learning learn associate superior teaching actions certain states the student's knowledge Reinforcement learning (RL flexible handling noisy data does need expert domain knowledge A dr>
Paper 4:  <Title: Reinforcement Learning with Imitation in Heterogeneous Multi-Agent Systems  .   Abstract: The application decision making learning algorithms presents many interestingresearch challenges Among these the ability agents learn act by or We describe, the IQ-algorithm integrates imitation, a Q-learner uses the observations it an expert agent bias its exploration in promising directions This algorithm goes previous work relaxing the oft-made assumptions (observer the expe>
Paper 5:  <Title: Machine Learning,  Creating Advice-Taking Reinforcement Learners  .   Abstract: Learning reinforcements a promising approach creating intelligent agents However reinforcement learning usually training episodes We present and a design addresses allowing a connectionist Q-learner accept advice given, a natural manner In our approach the advice-giver watches occasionally makes expressed instructions a simple imperative programming language Based techniques knowledge-based neural networks insert>
Paper 6:  <Title: Improving Generalization with Active Learning  .   Abstract: Active learning differs " from examples the learning algorithm assumes what part receives In some situations active learning provably that learning examples alone giving better generalization for a fixed number training examples In consider learning a binary concept noiseValiant 1984 We describe a formalism active concept learning called selective sampling show may approximately implemente>
Paper 8:  <Title: Learning to Predict User Operations for Adaptive Scheduling  .   Abstract: Mixed-initiative systems present the challenge finding an effective level interaction humans computers Machine learning presents in systems automatically adapt accommodate different users In learning user models an adaptive assistant crisis scheduling We describe the problem domain the scheduling assistant then present an initial formulation the adaptive assistant's learning task a baseline study After this re>
Paper 9:  <Title: CABINS A Framework of Knowledge Acquisition and Iterative Revision for Schedule Improvement and Reactive Repair  .   Abstract: Mixed-initiative systems present the challenge finding an effective level interaction humans computers Machine learning presents in systems automatically adapt accommodate different users In learning user models an adaptive assistant crisis scheduling We describe the problem domain the scheduling assistant then present an initial formulation the adaptive assistant's learning task a baseline study After this re>
Paper 10:  <Title: An unsupervised neural network for low-level control of a wheeled mobile robot: noise resistance, stability,.   Abstract: We recently introduced a neural network mobile robot controllerNETMORC the forward and inverse odometry a differential drive robot through After an initial learning phase the controller move the robot an arbitrary stationary or moving target while noise other forms disturbance changes the robot's plant In addition the forward odometric map allows the robot reach targets the absence sensory feedback The controller also adapt>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Reinforcement Learning

Reinforcement Learning
Prediction:  Reinforcement Learning
Is prediction correct?  True

Prediction: 1
Processing index 1759...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Belief Maintenance in Bayesian Networks  
Abstract: Abstract: Two issues of an intelligent navigation robot have been addressed in this work. First is the robot's ability to learn a representation of the local environment and use this representation to identify which local environment it is in. This is done by first extracting features from the sensors which are more informative than just distances of obstacles in various directions. Using these features a reduced ring representation (RRR) of the local environment is derived. As the robot navigates, it learns the RRR signatures of all the new environment types it encounters. For purpose of identification, a ring matching criteria is proposed where the robot tries to match the RRR from the sensory input to one of the RRRs in its library. The second issue addressed is that of learning hill climbing control laws in the local environments. Unlike conventional neuro-controllers, a reinforcement learning framework, where the robot first learns a model of the environment and then learns the control law in terms of a neural network is proposed here. The reinforcement function is generated from the sensory inputs of the robot before and after a control action is taken. Three key results shown in this work are that (1) The robot is able to build its library of RRR signatures perfectly even with significant sensor noise for eight different local environ-mets, (2) It was able to identify its local environment with an accuracy of more than 96%, once the library is build, and (3) the robot was able to learn adequate hill climbing control laws which take it to the distinctive state of the local environment for five different environment types.
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 5:  <Title: Exploration and Model Building in Mobile Robot Domains  .   Abstract:  present first results COLUMBUS an autonomous mobile robot operates initially unknown, structured environments Its task explore model efficiently while collisions obstacles uses an instance-based learning technique modeling its environment Real-world experiences generalized via two artificial neural networks encode the characteristics the robot's sensors typical environments assumed face Once trained these networks allow knowledge transfer acr>
Label: Reinforcement Learning
Paper 10:  <Title: Incremental Class Learning approach and its application to Handwritten Digit Recognition  .   Abstract: Incremental Class Learning provides a feasible framework scalable learning systems Instead learning a complex problem at ICL focuses learning subproblems incrementally one a time | using the results prior learning for combining With respect multi-class classification problems the ICL approach presented can Initially the system focuses one category After it learns this category tries identify a compact subset>
Label: Neural Networks
Paper 2:  <Title: Simultaneous Learning of Control Laws and Local Environment Representations for Intelligent Navigation Robots  .   Abstract: Two issues an intelligent navigation robot this work First is the robot's ability learn a representation use identify This done first extracting features the sensors which more informative just distances of obstacles various directions Using these features a reduced ring representationRRR the local environment derived As the robot navigates learns the RRR signatures all the new environment types For purpose iden>
Paper 3:  <Title: An unsupervised neural network for low-level control of a wheeled mobile robot: noise resistance, stability,.   Abstract: We recently introduced a neural network mobile robot controllerNETMORC the forward and inverse odometry a differential drive robot through After an initial learning phase the controller move the robot an arbitrary stationary or moving target while noise other forms disturbance changes the robot's plant In addition the forward odometric map allows the robot reach targets the absence sensory feedback The controller also adapt>
Paper 4:  <Title: Using a Case Base of Surfaces to Speed-Up Reinforcement Learning  .   Abstract: This paper demonstrates the exploitation certain vision processing index into a case base surfaces The surfaces the result reinforcement learning represent the optimum choice actions achieve some goal from anywhere This paper how strong features that occur the system early Such features allow identify when identical, very task solved previously retrieve the relevant surface This results>
Paper 6:  <Title: A Modular Q-Learning Architecture for Manipulator Task Decomposition `Data storage in the cerebellar model ar.   Abstract: Compositional Q-Learning (Singh 1992 composite tasks made several elemental tasks by reinforcement learning Skills acquired performing elemental tasks applied solve composite tasks Individual skills compete act only winning skills included the decomposition the composite task We extend the original CQ-L concept in (1) a more general reward function the agent have We use the CQ-L architecture acquire skills performing comp>
Paper 7:  <Title: A Simulation of Adaptive Agents in a Hostile Environment  .   Abstract: In this paper the genetic programming technique evolve programs control an autonomous agent capable learning survive In order facilitate this goal agents run random environment configurations Randomly programs control the interaction the agent recombined better programs Each generation the population agents placed the Simulator with producing surviving any environment The environment that an agent presented>
Paper 8:  <Title: Dynamic Automatic Model Selection  .   Abstract: COINS Technical Report 9230 February 1992 Abstract The problem how learn examples studied throughout machine learning many successful learning algorithms A problem received how select which algorithm a given learning task The ability a chosen algorithm induce a good generalization depends appropriate the model class underlying for We define an algorithm's model class the representation language it express>
Paper 9:  <Title: Robot Shaping: Developing Situated Agents through Learning  .   Abstract: Learning plays situated agents In explore reinforcement learning "shape a robot a predefined target behavior We connect both simulated and real robots A LECSYS a parallel implementation a learning classifier system with an extended genetic algorithm After classifying different kinds Animat-like behaviors explore learning agent's architecturemonolithic flat hierarchical of training strategies In particular hierarchical architecture requires th>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Probabilistic Methods

Reinforcement Learning
Prediction:  Reinforcement Learning
Is prediction correct?  False

Prediction: 0
Processing index 512...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Fault-Tolerant Implementation of Finite-State Automata in Recurrent Neural Networks  
Abstract: Abstract: Recently, we have proven that the dynamics of any deterministic finite-state automata (DFA) with n states and m input symbols can be implemented in a sparse second-order recurrent neural network (SORNN) with n + 1 state neurons and O(mn) second-order weights and sigmoidal discriminant functions [5]. We investigate how that constructive algorithm can be extended to fault-tolerant neural DFA implementations where faults in an analog implementation of neurons or weights do not affect the desired network performance. We show that tolerance to weight perturbation can be achieved easily; tolerance to weight and/or neuron stuck-at-zero faults, however, requires duplication of the network resources. This result has an impact on the construction of neural DFAs with a dense internal representation of DFA states.
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 2:  <Title: Constructing Deterministic Finite-State Automata in Recurrent Neural Networks  .   Abstract: Recurrent neural networks that behaveDFAs show deteriorating performance tested long strings This deteriorating performance can the instability the internal representation the learned DFA states The use a sigmoidal discriminant function together the recurrent structure contribute this instability We prove a simple algorithm construct a sparse interconnection topology sigmoidal discriminant function such internal>
Label: Neural Networks
Paper 4:  <Title: Constructive Learning of Recurrent Neural Networks: Limitations of Recurrent Casade Correlation and a Simple Solution  .   Abstract: It often predict the optimal neural network size Constructive or destructive methods add neurons layers connections might offer We prove one method, Recurrent Cascade Correlation due fundamental limitations representation thus its learning capabilities It cannot represent with monotone (i.e. sigmoid) and hard-threshold activation functions certain finite state automata We give a "preliminary" approach on these limitations devis>
Label: Neural Networks
Paper 10:  <Title: Networks of Spiking Neurons: The Third Generation of Neural Network Models  .   Abstract: The computational power formal models for networks spiking neurons compared based McCulloch Pitts neuronsi.e. threshold gates respectively sigmoidal gates In particular it shown networks spiking neurons computationally these other neural network models A concrete biologically relevant function exhibited which computed a single spiking neuronfor biologically reasonable values its parameters hundreds hidden units on a sigmoidal neural net This article d>
Label: Neural Networks
Paper 3:  <Title: Learning Context-free Grammars: Capabilities and Limitations of a Recurrent Neural Network with an External Stack Memory  .   Abstract: This work describes Deterministic Context-free (DCF) Grammars a Connectionist paradigm a Recurrent Neural Network Pushdown AutomatonNNPDA The NNPDA consists a recurrent neural network connected an external stack memory through a common error function We show the NNPDA able learn the dynamics an underlying pushdown automaton examples Not the network learn the state transitions the automaton required control the stack In use>
Paper 5:  <Title: Even with Arbitrary Transfer Functions, RCC Cannot Compute Certain FSA  .   Abstract: Category algorithms and architectures recurrent. No part submitted elsewhere Preference poster Abstract Existing proofs demonstrating the computational limitations the Recurrent Cascade Correlation (RCC) Network (Fahlman 1991 explicitly limit their results units having or hard-threshold transfer functionsGiles and Kremer The proof given shows any given finite, discrete, deterministic transfer function used the units an RCC networkFSA the networ>
Paper 6:  <Title: An Analytical Framework for Local Feedforward Networks  .   Abstract: Interference neural networks occurs learning in the input space unlearning Networks less susceptible interference referred spatially local networks To these properties a theoretical framework, consisting a measure interference network localization developed These measures incorporate not only the network weights architecture Using this framework analyze sigmoidal, multi perceptron ( that employ>
Paper 7:  <Title: Local Feedforward Networks  .   Abstract: Although feedforward neural networks well function approximation in some applications networks experience a desired function One problem interference which learning in the input space causes unlearning Networks less susceptible interference referred spatially local networks To these properties a theoretical framework, consisting a measure interference network localization developed that not only the network weights architect>
Paper 8:  <Title: Pruning Recurrent Neural Networks for Improved Generalization Performance  .   Abstract: Determining the architecture any learning task For recurrent neural networks no general methods permit the estimation layers hidden neurons layers weights We present a simple pruning heuristic which significantly trained recurrent networks We illustrate this heuristic training positive and negative strings a regular grammar We also show if rules extracted networks trained re>
Paper 9:  <Title: Extraction of Rules from Discrete-Time Recurrent Neural Networks  .   Abstract: The extraction symbolic knowledge trained neural networks and the direct encoding (partial) knowledge networks prior are They allow the exchange symbolic and connectionist knowledge representations The focus the quality the rules extracted recurrent neural networks Discrete-time recurrent neural networks can correctly strings a regular language Rules defining the learned grammar can extracted networks in deterministic finite-state>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Neural Networks

Neural Networks
Prediction:  Neural Networks
Is prediction correct?  True

Prediction: 1
Processing index 1191...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Machine Learning Bias, Statistical Bias, and Statistical Variance of Decision Tree Algorithms  
Abstract: Abstract: The term "bias" is widely used|and with different meanings|in the fields of machine learning and statistics. This paper clarifies the uses of this term and shows how to measure and visualize the statistical bias and variance of learning algorithms. Statistical bias and variance can be applied to diagnose problems with machine learning bias, and the paper shows four examples of this. Finally, the paper discusses methods of reducing bias and variance. Methods based on voting can reduce variance, and the paper compares Breiman's bagging method and our own tree randomization method for voting decision trees. Both methods uniformly improve performance on data sets from the Irvine repository. Tree randomization yields perfect performance on the Letter Recognition task. A weighted nearest neighbor algorithm based on the infinite bootstrap is also introduced. In general, decision tree algorithms have moderate-to-high variance, so an important implication of this work is that variance|rather than appropriate or inappropriate machine learning bias|is an important cause of poor performance for decision tree algorithms. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 2:  <Title: An Empirical Comparison of Voting Classification Algorithms: Bagging, Boosting, and Variants  .   Abstract: Methods voting classification algorithms, Bagging AdaBoost shown successful improving certain classifiers artificial and real-world datasets We review these algorithms describe a large empirical study comparing several variants conjunction a decision tree inducerthree variants a Naive-Bayes inducer The purpose improve why and when these algorithms, perturbation reweighting combination techniques affect classification error We provide a bias and v>
Label: Theory
Paper 8:  <Title: Bias and the Quantification of Stability Bias and the Quantification of Stability Bias and the.   Abstract: Research on bias machine learning algorithms generally concerned the impact predictive accuracy We believe there other factors should also the evaluation bias One such factor the stability the algorithm; in repeatability If we obtain two sets the same phenomenon with the same underlying probability distribution would like our learning induce approximately the same concepts from This paper introduces quantifying stabil>
Label: Theory
Paper 10:  <Title: Boosting the Margin: A New Explanation for the Effectiveness of Voting Methods  .   Abstract: One the surprising recurring phenomena observed experiments boosting is the test error the generated classifier usually does increase its size becomes very often is observed even after the training error reaches In this paper show this phenomenon related the distribution margins the training examples with the generated voting classification rule where simply correct votes votes received any incorrect labe>
Label: Theory
Paper 3:  <Title: BOOSTING AND NAIVE BAYESIAN LEARNING  .   Abstract: Although so-called naive Bayesian classification makes the unrealistic assumption the attributes an example given this learning method remarkably practice no uniformly better learning method Boosting is combining multiple classifiers due Yoav Freund Rob Schapire This paper boosting applied to naive Bayesian classifiers yields combination classifiers representationally standard feedforward multilayer perceptrons (An re>
Paper 4:  <Title: Experiments with a New Boosting Algorithm  .   Abstract: In an earlier paper introduced a new boosting algorithm AdaBoost, theoretically significantly any learning algorithm consistently generates classifiers whose performance a random guessing We also introduced the related notion a pseudoloss which forcing a learning algorithm of multi-label concepts concentrate hardest discriminate In this paper experiments we well AdaBoost with and pseudoloss, performs real>
Paper 5:  <Title: Error-Correcting Output Coding Corrects Bias and Variance  .   Abstract: Previous research a technique error output coding dramatically that learn data points into one k 2 classes This paper why the ECOC technique works particularly employed decision-tree learning algorithms It shows the ECOC method| like any form voting committee|can reduce the learning algorithm Furthermore|unlike methods simply combine multiple runs the same learning algorithm|ECOC>
Paper 6:  <Title: A Weighted Nearest Neighbor Algorithm for Learning with Symbolic Features  .   Abstract:  nearest neighbor algorithms learning examples worked domains all features had numeric values such domains the examples can treated points distance metrics can use standard definitions symbolic domains a more sophisticated treatment the feature space We introduce a nearest neighbor learning in domains symbolic features Our algorithm calculates distance tables it produce instances attaches weights further modify the str>
Paper 7:  <Title: Simultaneous Evolution of Programs and their Control Structures Simultaneous Evolution of Programs and their Control.   Abstract: Previous research a technique error output coding dramatically that learn data points into one k 2 classes This paper why the ECOC technique works particularly employed decision-tree learning algorithms It shows the ECOC method| like any form voting committee|can reduce the learning algorithm Furthermore|unlike methods simply combine multiple runs the same learning algorithm|ECOC>
Paper 9:  <Title: Why Does Bagging Work? A Bayesian Account and its Implications bagging's success, both in a.   Abstract: The error rate decision-tree and other classification learners can often much reduced bagging: learning multiple models bootstrap samples the database combining by uniform voting In this paper two alternative explanations this, works an approximation the optimal procedure Bayesian model averaging with an appropriate implicit prior effectively shifts prior a more appropriate region model space All the experimental evidenc>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Theory

Theory
Prediction:  Theory
Is prediction correct?  True

Prediction: 1
Processing index 474...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Protein Structure Prediction: Selecting Salient Features from Large Candidate Pools  
Abstract: Abstract: We introduce a parallel approach, "DT-Select," for selecting features used by inductive learning algorithms to predict protein secondary structure. DT-Select is able to rapidly choose small, nonredundant feature sets from pools containing hundreds of thousands of potentially useful features. It does this by building a decision tree, using features from the pool, that classifies a set of training examples. The features included in the tree provide a compact description of the training data and are thus suitable for use as inputs to other inductive learning algorithms. Empirical experiments in the protein secondary-structure task, in which sets of complex features chosen by DT-Select are used to augment a standard artificial neural network representation, yield surprisingly little performance gain, even though features are selected from very large feature pools. We discuss some possible reasons for this result. 1 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 2:  <Title: Evolution, Learning, and Instinct: 100 Years of the Baldwin Effect Using Learning to Facilitate the.   Abstract: This paper a hybrid methodology integrates genetic algorithms decision tree learning evolve useful subsets discriminatory features recognizing complex visual concepts A genetic algorithm (GA search the space all possible subsets candidate discrimination features Candidate feature subsets evaluated using C4.5 a decision-tree learning algorithm produce the given features using a limited amount The classification performance the resulting decision tree on un>
Label: Genetic Algorithms
Paper 3:  <Title: Maximum A Posteriori Classification of DNA Structure from Sequence Information  .   Abstract: We introduce lllama simple pattern recognizers into a general method estimating the entropy Each pattern recognizer exploits a partial match subsequences build Since the primary features interest biological sequence domains subsequences small variations exact composition lllama particularly We describe two methods, lllama-lengthalone this entropy estimate perform maximum a posteriori classification We apply seve>
Label: Neural Networks
Paper 9:  <Title: The role of afferent excitatory and lateral inhibitory synaptic plasticity in visual cortical ocular dominance.   Abstract: The boosting algorithm AdaBoost, Freund Schapire exhibited several benchmark problems when C4.5 as to be " Like other ensemble learning approaches constructs a composite hypothesis by voting many individual hypotheses In practice the large amount memory these hypotheses make ensemble methods hard deploy applications This paper by selecting the hypotheses nearly the same levels performance the entire set>
Label: Neural Networks
Paper 4:  <Title: Feature Generation for Sequence Categorization  .   Abstract: The problem sequence categorization to from a corpus labeled sequences procedures for accurately labeling The choice representation sequences can this task background knowledge a good representation known straightforward representations far optimal We propose a feature generation method (called FGEN creates Boolean features check heuristically selected collections subsequences We show empirically re>
Paper 5:  <Title: Prototype and Feature Selection by Sampling and Random Mutation Hill Climbing Algorithms  .   Abstract: With computational costs without accuracy describe two algorithms find sets prototypes nearest neighbor classification Here the term prototypes the reference instances a nearest neighbor computation the instances with respect which similarity assessed assign a new data item Both algorithms rely search sets prototypes are simple first is a Monte Carlo sampling algorithm; applies random mutation hill climbing On f>
Paper 6:  <Title: Feature Subset Selection Using the Wrapper Method: Overfitting and Dynamic Search Space Topology  .   Abstract: In the wrapper approach feature subset selection a search an optimal set the induction algorithm as a black box The estimated future performance the algorithm the heuristic guiding Statistical methods feature subset selection including forward selection backward elimination their stepwise variants viewed the space feature subsets We utilize best-first search a good feature subset discuss overfitting problems associated searching too many feature>
Paper 7:  <Title: Stochastic Propositionalization of Non-Determinate Background Knowledge  .   Abstract: It propositional learning algorithms require "good" features perform So a major step data engineering inductive learning the construction good features by domain experts These features often represent properties structured objects where typically the occurrence having To partly automate "feature engineering devised searches defined such substructures The algorithm stochastically conducts a top-dow>
Paper 8:  <Title: Compression-Based Feature Subset Selection  Keywords: Minimum Description Length Principle, Cross Validation, Noise  .   Abstract: Irrelevant and redundant features may reduce both predictive accuracy comprehensibility induced concepts Most common Machine approaches selecting a good subset relevant features rely cross As present a particular Minimum Description Length (MDL) measure feature subset selection Using the MDL principle allows taking at The new measure information plausible yet simple therefore efficiently We show empiricall>
Paper 10:  <Title: A Weighted Nearest Neighbor Algorithm for Learning with Symbolic Features  .   Abstract:  nearest neighbor algorithms learning examples worked domains all features had numeric values such domains the examples can treated points distance metrics can use standard definitions symbolic domains a more sophisticated treatment the feature space We introduce a nearest neighbor learning in domains symbolic features Our algorithm calculates distance tables it produce instances attaches weights further modify the str>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Neural Networks

Genetic Algorithms
Prediction:  Genetic Algorithms
Is prediction correct?  False

Prediction: 0
Processing index 1065...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: A Survey of Parallel Genetic Algorithms  
Abstract: Abstract: IlliGAL Report No. 97003 May 1997 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 6:  <Title: The Role of Transfer in Learning (extended abstract)  .   Abstract: Technical Report No 670 December,>
Label: Reinforcement Learning
Paper 8:  <Title: A Comparison of Selection Schemes used in Genetic Algorithms  .   Abstract: TIK-Report Nr December 1995 Version2 Edition)>
Label: Genetic Algorithms
Paper 9:  <Title: Heuristic for Improved Genetic Bin Packing  .   Abstract: University Tulsa Technical Report UTULSA-MCS-938 May Submitted Information Processing Letters May, 1993>
Label: Genetic Algorithms
Paper 2:  <Title: Genetic Algorithms, Tournament Selection, and the Effects of Noise  .   Abstract: IlliGAL Report July 1995>
Paper 3:  <Title: The Bayesian Approach to Tree-Structured Regression  .   Abstract:  REPORT NO 967 August>
Paper 4:  <Title: Finding Analogues for Innovative Design  .   Abstract: Knowledge Systems Laboratory March Report KSL 95>
Paper 5:  <Title: Environments with Classifier Systems (Experiments on Adding Memory to XCS)  .   Abstract: Pier Luca Lanzi Technical Report N. 97.45 October th>
Paper 7:  <Title: Model of the Environment to Avoid Local Learning  .   Abstract: Pier Luca Lanzi Technical Report N. 97.46 December 20 th>
Paper 10:  <Title: EXPERIMENTING WITH THE CHEESEMAN-STUTZ EVIDENCE APPROXIMATION FOR PREDICTIVE MODELING AND DATA MINING  .   Abstract:  REPORT NO 947 June 5>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Genetic Algorithms

Genetic Algorithms
Prediction:  Genetic Algorithms
Is prediction correct?  True

Prediction: 1
Processing index 2561...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: MDL Learning of Probabilistic Neural Networks for Discrete Problem Domains  
Abstract: Abstract: Given a problem, a case-based reasoning (CBR) system will search its case memory and use the stored cases to find the solution, possibly modifying retrieved cases to adapt to the required input specifications. In discrete domains CBR reasoning can be based on a rigorous Bayesian probability propagation algorithm. Such a Bayesian CBR system can be implemented as a probabilistic feedforward neural network with one of the layers representing the cases. In this paper we introduce a Minimum Description Length (MDL) based learning algorithm to obtain the proper network structure with the associated conditional probabilities. This algorithm together with the resulting neural network implementation provide a massively parallel architecture for solving the efficiency bottleneck in case-based reasoning. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 2:  <Title: Bayesian Case-Based Reasoning with Neural Networks  .   Abstract: Given a problem a case-based reasoning (CBR) system search its case memory use the stored cases possibly modifying retrieved cases adapt the required input specifications In efficient case-based reasoning We show a rigorous Bayesian probability propagation as and adapted CBR In our approach the efficient indexing problem of CBR naturally implemented the parallel architecture heuristic matching replaced>
Label: Probabilistic Methods
Paper 4:  <Title: Massively Parallel Case-Based Reasoning with Probabilistic Similarity Metrics  .   Abstract: We propose a probabilistic case-space metric the case matching case adaptation tasks Central a probability propagation adopted Bayesian reasoning systems perform The same probability propagation mechanism actually offers a uniform solution both the case matching and case adaptation problems We also show the algorithm a connectionist network efficient massively parallel case retrieval an inherent property th>
Label: Probabilistic Methods
Paper 6:  <Title: ADAPtER: an Integrated Diagnostic System Combining Case-Based and Abductive Reasoning  .   Abstract: The aim the ADAPtER system a diagnostic architecture combining case-based reasoning abductive reasoning exploiting the adaptation the solution old episodes focus the reasoning process Domain knowledge represented via a logical model basic mechanisms based abductive reasoning with consistency constraints have defined solving complex diagnostic problems involving multiple faults The model-based component supplemented a case memory adaptation mechanisms have mak>
Label: Case Based
Paper 3:  <Title: Adaptive Tuning of Numerical Weather Prediction Models: Simultaneous Estimation of Weighting, Smoothing and Physical Parameters 1  .   Abstract: In case-based reasoning demonstrated problem complex domains Also mixed paradigm approaches emerged combining CBR and induction techniques aiming verifying the knowledge building an efficient case memory However in complex domains induction over the whole problem space or too time In this paper an approach which (owing a close interaction the CBR part attempts induce rules only a particular context a problem just beingor>
Paper 5:  <Title: FONN: Combining First Order Logic with Connectionist Learning  .   Abstract: This paper manage structured data refine knowledge bases expressed a first order logic language The presented framework well classification problems concept de scriptions depend numerical features In fact the main goal the neural architecture that refining the numerical part without changing In particular discuss a method translate a set classification rules neural computation units Here, focus the translat>
Paper 7:  <Title: Computation and Psychophysics of Sensorimotor Integration  .   Abstract: In learning classification rules data We sketch two modules our architecture namely LINNEO + and GAR LINNEO +, which a knowledge acquisition tool ill-structured domains automatically generating classes examples incrementally works LINNEO + 's output, a representation the conceptual structure classes the input GAR that classification rules GAR can generate both conjunctive and disjunctive r>
Paper 8:  <Title: Lazy Induction Triggered by CBR  .   Abstract: In case-based reasoning demonstrated problem complex domains Also mixed paradigm approaches emerged combining CBR and induction techniques aiming verifying the knowledge building an efficient case memory However in complex domains induction over the whole problem space or too time In this paper an approach which (owing a close interaction the CBR part attempts induce rules only a particular context a problem just beingor>
Paper 9:  <Title: Case-Based Probability Factoring in Bayesian Belief Networks  .   Abstract: Bayesian network inference can formulated concerning in the computation an optimal factoring for the distribution represented the net Since the determination an optimal factoring is heuristic greedy strategies able find usually adopted In the present paper investigate based a combination genetic algorithmsGA case-based reasoning We show the use genetic algorithms the compu>
Paper 10:  <Title: Rule Generation and Compaction in the wwtp  .   Abstract: In learning classification rules data We sketch two modules our architecture namely LINNEO + and GAR LINNEO +, which a knowledge acquisition tool ill-structured domains automatically generating classes examples incrementally works LINNEO + 's output, a representation the conceptual structure classes the input GAR that classification rules GAR can generate both conjunctive and disjunctive r>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Probabilistic Methods

Probabilistic Methods
Prediction:  Probabilistic Methods
Is prediction correct?  True

Prediction: 1
Processing index 2676...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Models of perceptual learning in vernier hyperacuity  
Abstract: Abstract: Performance of human subjects in a wide variety of early visual processing tasks improves with practice. HyperBF networks (Poggio and Girosi, 1990) constitute a mathematically well-founded framework for understanding such improvement in performance, or perceptual learning, in the class of tasks known as visual hyperacuity. The present article concentrates on two issues raised by the recent psychophysical and computational findings reported in (Poggio et al., 1992b; Fahle and Edelman, 1992). First, we develop a biologically plausible extension of the HyperBF model that takes into account basic features of the functional architecture of early vision. Second, we explore various learning modes that can coexist within the HyperBF framework and focus on two unsupervised learning rules which may be involved in hyperacuity learning. Finally, we report results of psychophysical experiments that are consistent with the hypothesis that activity-dependent presynaptic amplification may be involved in perceptual learning in hyperacuity. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 7:  <Title: Implicit learning in 3D object recognition: The importance of temporal context  .   Abstract: A novel architecture and set learning rules cortical self-organization The model the idea multiple information channels modulate plasticity Features learned bottom-up information sources can thus influenced those learned contextual pathways vice A maximum likelihood cost function allows this scheme a biologically feasible, hierarchical neural circuit In simulations the model first demonstrate the utility temporal context plasticity The model learns repr>
Label: Neural Networks
Paper 8:  <Title: VISIT: An Efficient Computational Model of Human Visual Attention  .   Abstract: One the challenges models cognitive phenomena the development efficient and exible interfaces low level sensory information For visual processing researchers argued an attentional mechanism perform many high level vision This thesis presents VISIT, a connectionist model covert visual attention has as a vehicle studying this interface The model efficient exible is biologically plausible The complexity the network linear the number pixe>
Label: Neural Networks
Paper 2:  <Title: Cortical Mechanisms of Visual Recognition and Learning: A Hierarchical Kalman Filter Model  .   Abstract: We describe dynamic recognition based the statistical theory Kalman filtering from optimal control theory The model utilizes a hierarchical network whose successive levels implement Kalman filters operating over successively spatial Each hierarchical level predicts the current visual recognition state adapts using the residual error between>
Paper 3:  <Title: A Model of Invariant Object Recognition in the Visual System  .   Abstract: Neurons the ventral stream the primate visual system exhibit responses the images objects which invariant with natural transformations such translation size view Anatomical and neurophysiological evidence this achieved hierarchical processing areas In elucidate the manner such representations established constructed cortical visual processing which seeks parallel many features this system specifically the multi-stage hierarchy with its topologically constra>
Paper 4:  <Title: Computational Models of Sensorimotor Integration  Computational Maps and Motor Control.  .   Abstract: The sensorimotor integration system can viewed an observer attempting estimate its own state and by integrating multiple sources We describe a computational framework capturing this notion some specific models integration adaptation result Psychophysical results two sensorimotor systems subserving the integration adaptation visuo-auditory maps estimation the state the hand arm movements and within this framework These results: Spatia>
Paper 5:  <Title: A Neural Network Model of Memory Consolidation  .   Abstract: Some forms memory rely temporarily a system brain structures located includes The recall recent events one task relies crucially this system As the event becomes recent the medial temporal lobe becomes critical the recall the recollection appears rely It proposed a process called consolidation responsible transfer memory the medial temporal lobe We examine a network model proposed>
Paper 6:  <Title: Multiassociative Memory  .   Abstract: This paper how implement connectionist models Traditional symbolic approaches wield explicit representation all alternatives via stored links or implicitly enumerative algorithms Classical pattern association models ignore generating multiple outputs while recent research recurrent networks clearly focused upon multiassociativity as In define multiassociative memory MM, several possibl>
Paper 9:  <Title: Objective Function Formulation of the BCM Theory of Visual Cortical Plasticity: Statistical Connections, Stability Conditions  .   Abstract: In this paper an objective function formulation the BCM theory visual cortical plasticity permits demonstrate the connection various statistical methods in that Projection Pursuit This formulation provides a general method stability analysis the fixed points the theory enables the evolution the network under various visual rearing conditions It also allows comparison many existing unsupervised methods This model shown succes>
Paper 10:  <Title: 3D Object Recognition Using Unsupervised Feature Extraction  .   Abstract: Intrator (1990 proposed a feature extraction method is related recent statistical theoryHuber 1985 Friedman a biologically motivated modelBienenstock 1982 This method has recently applied feature extraction in the context recognizing single 2D viewsIntrator Gold, 1991 Here describe experiments designed analyze the nature the extracted features, their relevance the theory psychophysics object recognition>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Neural Networks

Neural Networks
Prediction:  Neural Networks
Is prediction correct?  True

Prediction: 1
Processing index 2612...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Models of Parallel Adaptive Logic  
Abstract: Abstract: This paper overviews a proposed architecture for adaptive parallel logic referred to as ASOCS (Adaptive Self-Organizing Concurrent System). The ASOCS approach is based on an adaptive network composed of many simple computing elements which operate in a parallel asynchronous fashion. Problem specification is given to the system by presenting if-then rules in the form of boolean conjunctions. Rules are added incrementally and the system adapts to the changing rule-base. Adaptation and data processing form two separate phases of operation. During processing the system acts as a parallel hardware circuit. The adaptation process is distributed amongst the computing elements and efficiently exploits parallelism. Adaptation is done in a self-organizing fashion and takes place in time linear with the depth of the network. This paper summarizes the overall ASOCS concept and overviews three specific architectures. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 5:  <Title: ASOCS: A Multilayered Connectionist Network with Guaranteed Learning of Arbitrary Mappings  .   Abstract: This paper features multilayer connectionist architectures known ASOCS ASOCS similar most decision-making neural network models attempts learn an adaptive set arbitrary vector mappings However differs dramatically its mechanisms ASOCS based networks adaptive digital elements which self using local information Function specification is entered incrementally use rules rather complete input-output vectors such a processing network extr>
Label: Neural Networks
Paper 7:  <Title: A VLSI Implementation of a Parallel, Self-Organizing Learning Model  .   Abstract: This paper a VLSI implementation the Priority Adaptive Self-Organizing Concurrent System learning model built a multi-chip module Many current hardware implementations neural network learning models direct implementations structures|a large number simple computing nodes connected a dense number weighted links PASOCS is one a class ASOCSAdaptive Self-Organizing Concurrent System connectionist models whose overall goal the same classical neural networks models whose func>
Label: Neural Networks
Paper 2:  <Title: A Self-Adjusting Dynamic Logic Module  .   Abstract: This paper an ASOCSAdaptive Self-Organizing Concurrent System massively parallel processing incrementally defined rule systems such areas adaptive logic robotics logical inference dynamic control An ASOCS is an adaptive network composed many simple computing elements operating This paper focuses Adaptive Algorithm 2 details learning AA2 has significant memory and knowledge maintenance advantages previous ASOCS models An ASOCS can operate either a data proces>
Paper 3:  <Title: A Self-Organizing Binary Decision Tree For Incrementally Defined Rule Based  .   Abstract: This paper an ASOCS (adaptive self-organizing concurrent system massively parallel processing incrementally defined rule systems such areas adaptive logic robotics logical inference dynamic control An ASOCS is an adaptive network composed many simple computing elements operating This paper focuses adaptive algorithm 3 details learning It has advantages previous ASOCS models in simplicity implementability An ASOCS can operate either a data processing>
Paper 4:  <Title: Digital Neural Networks  .   Abstract: Demands applications requiring massive parallelism symbolic environments given rebirth research models labeled neura l networks These models many simple nodes highly interconnected such that computation data amongst To present most models proposed nodes based simple analog functions inputs multiplied weights summed the total then optionally being transformed at Learning in these systems accomplished adjusting>
Paper 6:  <Title: Priority ASOCS  ASOCS models have two significant advantages over other learning models:  .   Abstract: This paper an ASOCS massively parallel processing incrementally defined rule systems such areas adaptive logic robotics logical inference dynamic control An ASOCS is an adaptive network composed many simple computing elements operating An ASOCS can operate either a data processing mode During data processing mode an ASOCS acts a parallel hardware circuit During learning mode an ASOCS incorporates a rule expressed a Boolean conjunction>
Paper 8:  <Title: Massively Parallel Matching of Knowledge Structures  .   Abstract: As knowledge bases used for AI systems increase access relevant information the dominant factor the cost inference This especially analogical (or case-based) reasoning in the ability perform inference efficient and flexible access a large base exemplarscases judged likely solving at In this chapter a novel algorithm efficient associative matching relational structures in large semantic networks The structure matching algorithm uses massively>
Paper 9:  <Title: Planning by Incremental Dynamic Programming  .   Abstract: This paper ideas dynamic programming as they most the concerns planning AI These form the incremental planning methods the integrated architecture Dyna These incremental planning methods continually an evaluation function the situation-action mapping a reactive system Actions generated the reactive system thus involve minimal delay while the incremental planning process guarantees the actions and evaluation function eventually optimal|n>
Paper 10:  <Title: Multiple Network Systems (Minos) Modules: Task Division and Module Discrimination 1  .   Abstract: It widely considered an ultimate connectionist objective incorporate neural networks intelligent systems These systems intended possess a varied repertoire enabling adaptable interaction The first step this direction develop various neural network algorithms models combine such networks a modular structure might incorporated In consider one aspect the second point: processing reliability hiding wetware details Pre- se>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Neural Networks

Neural Networks
Prediction:  Neural Networks
Is prediction correct?  True

Prediction: 1
Processing index 34...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Using a Case Base of Surfaces to Speed-Up Reinforcement Learning  
Abstract: Abstract: This paper demonstrates the exploitation of certain vision processing techniques to index into a case base of surfaces. The surfaces are the result of reinforcement learning and represent the optimum choice of actions to achieve some goal from anywhere in the state space. This paper shows how strong features that occur in the interaction of the system with its environment can be detected early in the learning process. Such features allow the system to identify when an identical, or very similar, task has been solved previously and to retrieve the relevant surface. This results in an orders of magnitude increase in learning rate. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 2:  <Title: Expectation-Based Selective Attention for Visual Monitoring and Control of a Robot Vehicle  .   Abstract: Reliable vision-based control requires focus an input scene Previous work with an autonomous lane following system, ALVINN [Pomerleau 1993 yielded uncluttered conditions This paper based learning approach handling difficult scenes which will confuse the ALVINN system This work presents a mechanism achieving task-specific focus exploiting A saliency map a computed expectation t>
Label: Neural Networks
Paper 7:  <Title: Using Mixtures of Factor Analyzers for Segmentation and Pose Estimation  Category: Visual Processing Preference: Oral  .   Abstract: To a hand-written digit string it segment separate digits Bottom-up segmentation heuristics often neighboring digits overlap substantially We describe has each digit class we the only knowledge required segmentation The system uses Gibbs sampling construct a perceptual interpretation a digit string segmentation arises naturally "explaining away By using conditional mixtures of factor analyzers>
Label: Neural Networks
Paper 3:  <Title: Finding Structure in Reinforcement Learning  .   Abstract: Reinforcement learning addresses select actions order unknown environments To scale reinforcement learning such typically studied AI one ultimately discover in in abstract the myriad details to operate more tractable problem spaces This paper the SKILLS algorithm. SKILLS discovers skills which partially defined action policies arise multiple, related tasks Skills collapse whole actio>
Paper 4:  <Title: Reinforcement Learning with Hierarchies of Machines  .   Abstract: We present reinforcement learning the policies considered constrained hierarchies partially specified machines This allows prior knowledge reduce the search space provides in transferred problems in component solutions recombined Our approach can providing a link reinforcement learning behavior- or teleo-reactive approaches control. We present provably alg>
Paper 5:  <Title: Experiments on the Transfer of Knowledge between Neural Networks Reprinted from: Computational Learning Theory and.   Abstract: This chapter three studies address neural network learning via the incorporation information extracted other networks This general problem network transfer encompasses relationships source target networks Our focus the utilization weights from source networks which solve a subproblem the target network task with speeding learning on We demonstrate the approach described improve learning speed up ten learning>
Paper 6:  <Title: Modeling the Student with Reinforcement Learning  .   Abstract: We describe a methodology enabling an intelligent teaching system make high level strategy decisions on low level student modeling information This framework less costly construct superior hand coding teaching strategies as responsive In order accomplish reinforcement learning learn associate superior teaching actions certain states the student's knowledge Reinforcement learning (RL flexible handling noisy data does need expert domain knowledge A dr>
Paper 8:  <Title: Learning One More Thing  .   Abstract: Most research machine learning scenarios faces The lifelong learning framework assumes encounters related learning tasks over providing the transfer among these. This paper studies lifelong learning binary classification It presents the invariance approach knowledge transferred via a learned model the invariances Results on learning recognize objects color images demonstrate supe>
Paper 9:  <Title: Learning Viewpoint Invariant Face Representations from Visual Experience by Temporal Association  .   Abstract: In natural visual experience different views face tend appear close temporal proximity A set simulations presented demonstrate how viewpoint invariant representations faces developed visual experience by capturing the input patterns The simulations explored temporal smoothing activity signals Hebbian learningFoldiak 1991 both a feed-forward system The recurrent system was a Hopfield network a lowpass temporal filter on>
Paper 10:  <Title: Planning by Incremental Dynamic Programming  .   Abstract: This paper ideas dynamic programming as they most the concerns planning AI These form the incremental planning methods the integrated architecture Dyna These incremental planning methods continually an evaluation function the situation-action mapping a reactive system Actions generated the reactive system thus involve minimal delay while the incremental planning process guarantees the actions and evaluation function eventually optimal|n>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Reinforcement Learning

Reinforcement Learning
Prediction:  Reinforcement Learning
Is prediction correct?  True

Prediction: 1
Processing index 1082...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Specialization of Logic Programs by Pruning SLD-Trees  
Abstract: Abstract: program w.r.t. positive and negative examples can be viewed as the problem of pruning an SLD-tree such that all refutations of negative examples and no refutations of positive examples are excluded. It is shown that the actual pruning can be performed by applying unfolding and clause removal. The algorithm spectre is presented, which is based on this idea. The input to the algorithm is, besides a logic program and positive and negative examples, a computation rule, which determines the shape of the SLD-tree that is to be pruned. It is shown that the generality of the resulting specialization is dependent on the computation rule, and experimental results are presented from using three different computation rules. The experiments indicate that the computation rule should be formulated so that the number of applications of unfolding is kept as low as possible. The algorithm, which uses a divide-and-conquer method, is also compared with a covering algorithm. The experiments show that a higher predictive accuracy can be achieved if the focus is on discriminating positive from negative examples rather than on achieving a high coverage of positive examples only. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 4:  <Title: Predicate Invention and Learning from Positive Examples Only  .   Abstract: Previous bias shift approaches predicate invention applicable learning positive examples only if a complete hypothesis found the given language negative examples required determine new predicates should One approach presented, MERLIN a successor a system in predicate invention guided sequences input clauses in SLD-refutations positive and negative examples w.r.t an overly general theory In contrast which searches the minimal finite-state>
Label: Rule Learning
Paper 5:  <Title: Bottom-up induction of logic programs with more than one recursive clause  .   Abstract: In this paper called MRI induce logic programs from their examples This method induce programs with a base clause from examples MRI is based saturations examples It first generates a path structure an expression a stream processed predicates The concept path structure Identam-Almquist used TIM [ Idestam-Almquist 1996 In this paper introduce extension difference path>
Label: Rule Learning
Paper 6:  <Title: Learning Decision Trees from Decision Rules:  .   Abstract: A method and initial results from ABSTRACT A standard approach determining decision trees learn them examples A disadvantage once a decision tree learned modify different decision making situations Such problems arise, for an attribute assigned some node measured there a significant change the costs measuring attributes or in the frequency distribution events from different decision classes An attractive approach resolving le>
Label: Rule Learning
Paper 7:  <Title: Some studies in machine learning using the game of checkers. IBM Journal, 3(3):211-229, 1959. Some.   Abstract: covering has used and compared the covering technique a logic programming framework Covering works repeatedly specializing an overly general hypothesis on each iteration focusing finding a clause a high coverage positive examples Divide- works specializing an overly general hypothesis once focusing discriminating positive from Experimental results demonstrating cases more accurate hypotheses>
Label: Genetic Algorithms
Paper 2:  <Title: Specialization of Recursive Predicates  .   Abstract: When specializing a recursive predicate order exclude a set negative examples without excluding not specialize or remove a refutation without any positive exam ples A previously proposed solution apply program transformation in order obtain non-recursive target predicates from recursive ones However the application prevents recursive specializations found In this work the algorithm spectre ii which not l>
Paper 3:  <Title: Theory-Guided Induction of Logic Programs by Inference of Regular Languages recursive clauses. merlin on the.   Abstract: resent allowed sequences resolution steps for the initial theory There, many characterizations allowed sequences resolution steps expressed a set resolvents One approach presented, the system mer-lin, an earlier technique learning finite-state automata that represent allowed sequences resolution steps merlin extends the previous technique in i negative examples considered in addition a new strategy performing generalization techn>
Paper 8:  <Title: THE DISCOVERY OF ALGORITHMIC PROBABILITY  .   Abstract: covering has used and compared the covering technique a logic programming framework Covering works repeatedly specializing an overly general hypothesis on each iteration focusing finding a clause a high coverage positive examples Divide- works specializing an overly general hypothesis once focusing discriminating positive from Experimental results demonstrating cases more accurate hypotheses>
Paper 9:  <Title: Covering vs. Divide-and-Conquer for Top-Down Induction of Logic Programs  .   Abstract: covering has used and compared the covering technique a logic programming framework Covering works repeatedly specializing an overly general hypothesis on each iteration focusing finding a clause a high coverage positive examples Divide- works specializing an overly general hypothesis once focusing discriminating positive from Experimental results demonstrating cases more accurate hypotheses>
Paper 10:  <Title: ILP with Noise and Fixed Example Size: A Bayesian Approach  .   Abstract: Current inductive logic programming systems limited their handling noise as employ a greedy covering approach constructing the hypothesis one clause This approach also causes difficulty learning recursive predicates Additionally many current systems an implicit expectation the cardinality reflect the concept the instance space A framework learning noisy data fixed example size is A Bayesian heuristic finding this genera>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Rule Learning

Rule Learning
Prediction:  Rule Learning
Is prediction correct?  True

Prediction: 1
Processing index 2082...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: %A L. Ingber %T Adaptive simulated annealing (ASA): Lessons learned %J Control and Cybernetics Annealing
Abstract: Abstract:  
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 2:  <Title: Genetic Algorithms, Tournament Selection, and the Effects of Noise  .   Abstract: IlliGAL Report July 1995>
Paper 3:  <Title: CABeN: A Collection of Algorithms for Belief Networks  Correspond with:  .   Abstract: Portions this report have the Fifteenth Annual Symposium Computer Applications in Medical CareNovember, 1991>
Paper 4:  <Title: A Survey of Parallel Genetic Algorithms  .   Abstract: IlliGAL Report May 1997>
Paper 5:  <Title: EXPERIMENTING WITH THE CHEESEMAN-STUTZ EVIDENCE APPROXIMATION FOR PREDICTIVE MODELING AND DATA MINING  .   Abstract:  REPORT NO 947 June 5>
Paper 6:  <Title: Cognitive Computation (Extended Abstract)  .   Abstract: Cognitive computation discussed a discipline links together neurobiology artificial intelligence>
Paper 7:  <Title: MML mixture modelling of multi-state, Poisson, von Mises circular and Gaussian distributions  .   Abstract: 11] M.H. Overmars. A random approach motion planning. Technical Report RUU-CS-92-32, Department October 1992>
Paper 8:  <Title: References Linear Controller Design, Limits of Performance, "The parallel projection operators of a nonlinear feedback.   Abstract: 13] Yang, Sussmann Sontag linear systems bounded controls Proc Nonlinear Control Systems Design Symp., Bordeaux June 1992 Fliess Ed IFAC Publications pp 1520 Journal version to appear IEEE Trans Autom. Control.>
Paper 9:  <Title: 99-113. Construction of Phylogenetic Trees, Science, Fitting the Gene Lineage Into Its Species Lineage. A.   Abstract: 6] Farach and Thorup, 1993. Fast Comparison Evolutionary Trees, Technical Report 9346 DIMACS Rutgers University>
Paper 10:  <Title: OBSERVABILITY IN RECURRENT NEURAL NETWORKS  .   Abstract: Report SYCON-92-07rev ABSTRACT We obtain a characterization observability appear neural networks research>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Genetic Algorithms

Probabilistic Methods
Prediction:  Probabilistic Methods
Is prediction correct?  False

Prediction: 0
Processing index 466...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: On the Computational Economics of Reinforcement Learning  
Abstract: Abstract: Following terminology used in adaptive control, we distinguish between indirect learning methods, which learn explicit models of the dynamic structure of the system to be controlled, and direct learning methods, which do not. We compare an existing indirect method, which uses a conventional dynamic programming algorithm, with a closely related direct reinforcement learning method by applying both methods to an infinite horizon Markov decision problem with unknown state-transition probabilities. The simulations show that although the direct method requires much less space and dramatically less computation per control action, its learning ability in this task is superior to, or compares favorably with, that of the more complex indirect method. Although these results do not address how the methods' performances compare as problems become more difficult, they suggest that given a fixed amount of computational power available per control action, it may be better to use a direct reinforcement learning method augmented with indirect techniques than to devote all available resources to a computation-ally costly indirect method. Comprehensive answers to the questions raised by this study depend on many factors making up the eco nomic context of the computation.
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 2:  <Title: Reinforcement Learning Algorithms for Average-Payoff Markovian Decision Processes  .   Abstract: Reinforcement learningRL solving learning-control problems RL researchers focussed almost problems where the controller maximize the discounted sum payoffs However as emphasized Schwartz (1993 in many problems those for the optimal behavior a limit cycle more natural and computationally advantageous formulate tasks so the controller's objective the average payoff received per In derive new average-pay>
Label: Reinforcement Learning
Paper 6:  <Title: Machine Learning,  Reinforcement Learning with Replacing Eligibility Traces  .   Abstract: The eligibility trace one reinforcement learning handle delayed reward In this paper introduce eligibility trace the replacing trace analyze it theoretically results faster, more reliable learning Both kinds trace assign credit prior events according how recently only the conventional trace gives greater credit repeated events Our analysis for conventional and replace-trace versions the o*ine TD(1 algorithm applied undiscounted absorbin>
Label: Reinforcement Learning
Paper 9:  <Title: Gain Adaptation Beats Least Squares?  .   Abstract: I present computational results suggesting gain-adaptation algorithms based in part connectionist learning methods improve over least squares other classical parameter-estimation methods The new algorithms evaluated with respect classical methods along three dimensions computational complexity required prior knowledge about The new algorithms all of LMS methods O(n whereas least-squares methods and t>
Label: Neural Networks
Paper 10:  <Title: INCREMENTAL POLYNOMIAL CONTROLLER NETWORKS: two self-organising non-linear controllers  .   Abstract: Temporal differenceTD) methods constitute learning predictions parameterized a recency factor. Currently the most important application these methods temporal credit assignment reinforcement learning Well known reinforcement learning algorithms AHC may viewed instances TD learning This paper the efficient and general implementation TD() arbitrary, for use reinforcement learning algorithms optimizing the discounted sum rewards The tradi>
Label: Neural Networks
Paper 3:  <Title: A Teaching Strategy for Memory-Based Control  .   Abstract: Combining different machine learning algorithms produce benefits above either method alone This paper demonstrates genetic algorithms conjunction lazy learning solve examples a difficult class delayed reinforcement learning problems better either method This class, the class differential games includes numerous important control problems arise robotics planning game playing, solutions differential games suggest solution strategies the general>
Paper 4:  <Title: Toward Learning Systems That Integrate Different Strategies and Representations TR93-22  .   Abstract: Temporal differenceTD) methods constitute learning predictions parameterized a recency factor. Currently the most important application these methods temporal credit assignment reinforcement learning Well known reinforcement learning algorithms AHC may viewed instances TD learning This paper the efficient and general implementation TD() arbitrary, for use reinforcement learning algorithms optimizing the discounted sum rewards The tradi>
Paper 5:  <Title: A Statistical Approach to Solving the EBL Utility Problem  .   Abstract: Many "learning from systems information problem solving experiences modify a performance element PE forming PE 0 that solve more However transformations that improve one set problems degrade the new PE 0 is; depends We therefore seek the performance element whose expected performance, over this distribution optimal Unfortunately, the actual distribution,>
Paper 7:  <Title: Reinforcement Learning Algorithm for Partially Observable Markov Decision Problems  .   Abstract: Increasing attention reinforcement learning algorithms partly successes the theoretical analysis their behavior Markov environments If the Markov assumption removed however neither generally the algorithms continue usable We propose and analyze a new learning algorithm a certain class non-Markov decision problems Our algorithm applies to problems the environment Markov, the learner restricted state information The algorithm involves a Monte-Carlo policy evaluati>
Paper 8:  <Title: NEUROCONTROL BY REINFORCEMENT LEARNING  .   Abstract: Reinforcement learningRL a model-free tuning adaptation method control dynamic systems Contrary supervised learning based usually gradient descent techniques RL does any model or sensitivity function of the process Hence RL can applied systems poorly uncertain nonlinear for untractable with In reinforcement learning the overall controller performance evaluated a scalar measure reinforcement. Depending the control task reinforcement represent>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Reinforcement Learning

Reinforcement Learning
Prediction:  Reinforcement Learning
Is prediction correct?  True

Prediction: 1
Processing index 2598...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Duplication of Coding Segments in Genetic Programming  
Abstract: Abstract: Research into the utility of non-coding segments, or introns, in genetic-based encodings has shown that they expedite the evolution of solutions in domains by protecting building blocks against destructive crossover. We consider a genetic programming system where non-coding segments can be removed, and the resultant chromosomes returned into the population. This parsimonious repair leads to premature convergence, since as we remove the naturally occurring non-coding segments, we strip away their protective backup feature. We then duplicate the coding segments in the repaired chromosomes, and place the modified chromosomes into the population. The duplication method significantly improves the learning rate in the domain we have considered. We also show that this method can be applied to other domains.
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 4:  <Title: Hybridized Crossover-Based Search Techniques for Program Discovery  .   Abstract: In address program discovery as Genetic Programming [10 We two major results by combining a hierarchical crossover operator two traditional single point search algorithms: Simulated Annealing Stochastic Iterated Hill Climbing solved some problems fewer fitness evaluations a success Genetic Programming Second managed enhance Genetic Programming hybridizing the simple scheme hill climbing from a few individuals at a fixed interval generations T>
Label: Genetic Algorithms
Paper 5:  <Title: Empirical studies of the genetic algorithm with non-coding segments  .   Abstract: The genetic algorithm (GA a problem solving method modelled the process We interested studying the GA: non-coding segments GA performance Non-coding segments segments bits an individual that provide no contribution, positive the fitness Previous research non-coding segments suggests including these structures the GA may improve GA performance Understanding when this improvement will use the GA to>
Label: Genetic Algorithms
Paper 6:  <Title: Testing the Robustness of the Genetic Algorithm on the Floating Building Block Representation.  .   Abstract: Recent studies on a floating building block representation for the genetic algorithm (GA suggest there many advantages This paper the behavior the GA on floating representation problems response three different types pressures genetic material during the problem solving process which have negative-valued building blocks randomizing non-coding segments Results indicate the GA's performance floating representation problems ve>
Label: Genetic Algorithms
Paper 8:  <Title: Culling Teaching -1 Culling and Teaching in Neuro-evolution  .   Abstract: The evolving population neural nets contains information not in genes the collection behaviors of the population members Such information thought culture of the population Two ways exploiting that culture explored Culling overlarge litters: Generate offspring different crossovers quickly evaluate by, throw appear poor (2) Teaching: Use backpropagation train offspring toward the performance the populat>
Label: Genetic Algorithms
Paper 2:  <Title: Recognizing Handwritten Digit Strings Using Modular Spatio-temporal Connectionist Networks  .   Abstract: Research the utility non-coding segments introns genetic-based encodings expedite solutions domains by protecting building against destructive crossover We consider a genetic programming system where removed the resultant chromosomes returned This parsimonious repair leads premature convergence since as remove strip their protective backup feature We then duplicate the coding segments the repaired chromoso>
Paper 3:  <Title: A comparison of the fixed and floating building block representation in the genetic algorithm  .   Abstract:  compares the traditional, fixed problem representation style a genetic algorithmGA a new floating representation in the building blocks not specific locations the individuals of In addition the effects non-coding segments both of these representations is studied Non-coding segments a computational model floating building blocks mimic the location independence genes The fact these structures prevalent natural genetic systems provide some a>
Paper 7:  <Title: TD Learning of Game Evaluation Functions with Hierarchical Neural Architectures  .   Abstract: Genetic algorithms solve hard optimization problems ranging the Travelling Salesman problem the Quadratic Assignment problem We show the Simple Genetic Algorithm solve derived the 3-Conjunctive Normal Form problem By separating the populations small sub parallel genetic algorithms exploits the inherent parallelism prevents premature convergence Genetic algorithms using hill-climbing conduct genetic search in the space local optima can less>
Paper 9:  <Title: Induction of decision trees using RELIEFF  .   Abstract: An investigation the dynamics Genetic Programming applied chaotic time series prediction reported An interesting characteristic adaptive search techniques perform well many problem domains while failing Because Genetic Programming's flexible tree structure any particular problem represented myriad forms These representations variegated effects search performance Therefore an aspect fundamental engineering significance find a representation, acted Genetic Programming operators opt>
Paper 10:  <Title: Learning Representations for Evolutionary Computation an example from the domain of two-dimensional shape designs. In.   Abstract: Evolutionary systems applications turbine design scheduling problems The basic algorithms similar all these applications the representation always problem specific Unfortunately the search time evolutionary systems very efficient codings using problem specific domain knowledge reduce This paper, where the user only specifies a very general, basic coding that a larger variety problems The system then learns more efficient, problem s>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Genetic Algorithms

Genetic Algorithms
Prediction:  Genetic Algorithms
Is prediction correct?  True

Prediction: 1
Processing index 1217...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: A game theoretic approach to moving horizon control  
Abstract: Abstract: A control law is constructed for a linear time varying system by solving a two player zero sum differential game on a moving horizon, the game being that which is used to construct an H 1 controller on a finite horizon. Conditions are given under which this controller results in a stable system and satisfies an infinite horizon H 1 norm bound. A risk sensitive formulation is used to provide a state estimator in the observation feedback case.
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 8:  <Title: Incremental methods for computing bounds in partially observable Markov decision processes  .   Abstract: Partially observable Markov decision processes allow one model complex dynamic decision or problems include both action outcome uncertainty imperfect observabil-ity The control problem formulated combining costs or rewards from In, analyse various incremental methods computing bounds control problems infinite discounted horizon criteria The methods and include novel incremental versions grid-bas>
Label: Reinforcement Learning
Paper 10:  <Title: Rapid Quality Estimation of Neural Network Input Representations  .   Abstract: FURTHER RESULTS CONTROLLABILITY PROPERTIES DISCRETE-TIME NONLINEAR SYSTEMS fl ABSTRACT Controllability questions In particular continue the search conditions the group-like notion transitivity implies the stronger and semigroup-like property forward accessibility We show this implication holds, pointwise states which a weak Poisson stability property globally if exists a global "attractor the system>
Label: Neural Networks
Paper 2:  <Title: Robust performance and adaptation using receding horizon H 1 control of time varying systems.  .   Abstract: In construct suboptimal H 1 controllers satisfy a new robust performance condition the receding horizon technique A method described the synthesis H 1 controllers online making the exact plant model only on a finite interval extending Inequalities based the two Riccati differential equation solution to the finite horizon H 1 problem derived the resulting freedom exploited construct H 1 controllers a closed loop induced norm less all plants d>
Paper 3:  <Title: Adaptive Wavelet Control of Nonlinear Systems  .   Abstract: This paper considers the design analysis adaptive wavelet control algorithms uncertain nonlinear dynamical systems The Lyapunov synthesis approach a state-feedback adaptive control scheme nonlinearly parametrized wavelet network models Semi-global stability results are obtained under the key assumption the system uncertainty satisfies The localization properties adaptive networks and formal definitions interference localization measures>
Paper 4:  <Title: CONTROL-LYAPUNOV FUNCTIONS FOR TIME-VARYING SET STABILIZATION  .   Abstract: This paper, time varying systems global asymptotic controllability to a given closed subset the state space equivalent the existence a continuous control-Lyapunov function respect.>
Paper 5:  <Title: On Finite Gain Stabilizability of Linear Systems Subject to Input Saturation  .   Abstract:  paper deals (global) finite-gain input/output stabilization linear systems saturated controls For neutrally stable systems shown the linear feedback law suggested the passivity approach indeed every L p -norm Explicit bounds closed-loop gains obtained related the norms the respective systems without saturation These results do extend systems the state matrix on with nonsimple (size &gt; 1) Jordan blocks contradicting wha>
Paper 6:  <Title: Avoiding Saturation By Trajectory Reparameterization  .   Abstract: The problem trajectory tracking the presence input constraints considered The desired trajectory reparameterized on a slower time scale in input saturation that the reparameterizing function derived. The deviation the nominal trajectory minimized formulating an optimal control problem>
Paper 7:  <Title: Identification in H 1 with Nonuniformly Spaced Frequency Response Measurements  .   Abstract: In the problem "system identification in H 1 " investigated when the given frequency response data necessarily on a uniformly spaced grid A large class robustly convergent identification algorithms derived A particular algorithm further examined explicit worst error boundsin the H 1 norm derived Examples are the algorithms>
Paper 9:  <Title: An Optimal Weighting Criterion of Case Indexing for Both Numeric and Symbolic Attributes  .   Abstract: A General Result on Linear Systems Using Bounded Controls 1 We controllers globally stabilize subject control saturation We allow essentially arbitrary saturation functions The only conditions imposed the system the obvious necessary ones the uncontrolled system positive real part standard stabilizability rank condition hold One is in a "neural-network type" one-hidden layer architecture while t>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Reinforcement Learning

Reinforcement Learning
Prediction:  Reinforcement Learning
Is prediction correct?  True

Prediction: 1
Processing index 2470...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Induction and Recapitulation of Deep Musical Structure  
Abstract: Abstract: We describe recent extensions to our framework for the automatic generation of music-making programs. We have previously used genetic programming techniques to produce music-making programs that satisfy user-provided critical criteria. In this paper we describe new work on the use of connectionist techniques to automatically induce musical structure from a corpus. We show how the resulting neural networks can be used as critics that drive our genetic programming system. We argue that this framework can potentially support the induction and recapitulation of deep structural features of music. We present some initial results produced using neural and hybrid symbolic/neural critics, and we discuss directions for future work.
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 2:  <Title: ILA: Combining Inductive Learning with Prior Knowledge and Reasoning  .   Abstract: We describe recent extensions music-making programs We previously used genetic programming techniques music-making programs satisfy user-provided critical criteria new work connectionist techniques automatically induce musical structure. We show the resulting neural networks critics that drive our genetic programming system We argue this framework potentially support the induction deep structural features o>
Label: Case Based
Paper 5:  <Title: Learning Optimal Dialogue Strategies: A Case Study of a Spoken Dialogue Agent for Email  .   Abstract: This paper by a dialogue agent learn choose an optimal dialogue strategy While widely agreed dialogue strategies should formulated terms communicative intentions little work automatically optimizing an agent's choices when there multiple ways realize Our method learning algorithms empirical evaluation techniques The learning component our method based algorithms for reinforcement learning such dynamic programming Q>
Label: Reinforcement Learning
Paper 6:  <Title: Knowledge Acquisition with a Knowledge-Intensive Machine Learning System  .   Abstract: In this paper investigate the integration knowledge acquisition. We argue existing machine learning techniques made as knowledge acquisition tools allowing the expert have and interaction We describe extensions FOCLa multistrategy Horn-clause learning program have greatly its power paying the utility maintaining a connection a rule the set examples explained th>
Label: Rule Learning
Paper 10:  <Title: Evolution of Pseudo-colouring Algorithms for Image Enhancement with Interactive Genetic Programming  .   Abstract: Technical Report CSRP-97-5 School The University Birmingham Abstract the interactive development programs image enhancement Genetic Programming pseudo-colour transformations In our approach the user drives GP by deciding should the winner tournament selection The presence the user does only running GP without a fitness function transforms a very efficient search procedure capable producing effective solutions o>
Label: Genetic Algorithms
Paper 3:  <Title: Cultural Transmission of Information in Genetic Programming  .   Abstract: This paper shows the performance a genetic programming system through the addition mechanisms individuals (culture Teller has previously shown genetic programming systems enhanced the addition memory mechanisms for individual programs [Teller 1994 in changed communication individuals within and across generations We show the effects indexed memory culture a genetic programming>
Paper 4:  <Title: Reinforcement Learning with Hierarchies of Machines  .   Abstract: We present reinforcement learning the policies considered constrained hierarchies partially specified machines This allows prior knowledge reduce the search space provides in transferred problems in component solutions recombined Our approach can providing a link reinforcement learning behavior- or teleo-reactive approaches control. We present provably alg>
Paper 7:  <Title: Robot Shaping: Developing Situated Agents through Learning  .   Abstract: Learning plays situated agents In explore reinforcement learning "shape a robot a predefined target behavior We connect both simulated and real robots A LECSYS a parallel implementation a learning classifier system with an extended genetic algorithm After classifying different kinds Animat-like behaviors explore learning agent's architecturemonolithic flat hierarchical of training strategies In particular hierarchical architecture requires th>
Paper 8:  <Title: Evolution of Mapmaking: Learning, planning, and memory using Genetic Programming  .   Abstract: An essential component an intelligent agent observe encode use Traditional approaches Genetic Programming evolving functional or reactive programs only a minimal use state This paper investigating learning, planning memory Genetic Programming The approach uses a multi-phasic fitness environment enforces the use memory allows fairly straightforward comprehension the evolved representations. An illustrative problem 'gold' col>
Paper 9:  <Title: Computation and Psychophysics of Sensorimotor Integration  .   Abstract: In learning classification rules data We sketch two modules our architecture namely LINNEO + and GAR LINNEO +, which a knowledge acquisition tool ill-structured domains automatically generating classes examples incrementally works LINNEO + 's output, a representation the conceptual structure classes the input GAR that classification rules GAR can generate both conjunctive and disjunctive r>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Genetic Algorithms

Neural Networks
Prediction:  Neural Networks
Is prediction correct?  False

Prediction: 0
Processing index 600...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Separating hippocampal maps  Spatial Functions of the Hippocampal Formation and the  
Abstract: Abstract: The place fields of hippocampal cells in old animals sometimes change when an animal is removed from and then returned to an environment [ Barnes et al., 1997 ] . The ensemble correlation between two sequential visits to the same environment shows a strong bimodality for old animals (near 0, indicative of remapping, and greater than 0.7, indicative of a similar representation between experiences), but a strong unimodality for young animals (greater than 0.7, indicative of a similar representation between experiences). One explanation for this is the multi-map hypothesis in which multiple maps are encoded in the hippocampus: old animals may sometimes be returning to the wrong map. A theory proposed by Samsonovich and McNaughton (1997) suggests that the Barnes et al. experiment implies that the maps are pre-wired in the CA3 region of hippocampus. Here, we offer an alternative explanation in which orthogonalization properties in the dentate gyrus (DG) region of hippocampus interact with errors in self-localization (reset of the path integrator on re-entry into the environment) to produce the bimodality. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 2:  <Title: A Model of Visually Guided Plasticity of the Auditory Spatial Map in the Barn Owl  .   Abstract: In the barn owl the self-organization the auditory map of space in the external nucleusICx strongly vision the nature this interaction In this paper a biologically plausible and mini-malistic model ICx self-organization where receives a learn signal based the owl's visual attention When the visual attention in the same spatial location the learn signal turned the map allowed adapt A two-dimensional Kohonen map used model the ICx,>
Label: Neural Networks
Paper 6:  <Title: Implicit learning in 3D object recognition: The importance of temporal context  .   Abstract: A novel architecture and set learning rules cortical self-organization The model the idea multiple information channels modulate plasticity Features learned bottom-up information sources can thus influenced those learned contextual pathways vice A maximum likelihood cost function allows this scheme a biologically feasible, hierarchical neural circuit In simulations the model first demonstrate the utility temporal context plasticity The model learns repr>
Label: Neural Networks
Paper 3:  <Title: Learning Harmonic Progression Using Markov Models EECS545 Project  .   Abstract: It argued the memorization events situations (episodic memory requires the rapid formation neural circuits responsive binding errors binding matches While the formation circuits responsive binding matches modeled associative learning mechanisms the rapid formation binding errors difficult given their seemingly paradoxical behavior; such a circuit must formed the occurrence in subsequent to it fire anymor>
Paper 4:  <Title: Learning Viewpoint Invariant Representations of Faces in an Attractor Network  .   Abstract:  natural visual experience different views tend close temporal proximity as an animal We investigated an attractor network acquire view invariant visual representations first neighbors a pattern sequence The pattern sequence contains successive views faces ten individuals as pose Under the network dynamics developed Griniasty Tsodyks & Amit1993 multiple views the same basin attraction We use an independent compone>
Paper 5:  <Title: Task and Spatial Frequency Effects on Face Specialization  .   Abstract: There face processing localized The double dissociation a face recognition deficit occurring brain damage visual object agnosia difficulty other kinds indicates served partially independent mechanisms in Is neural specialization innate learned We suggest this specialization could the result a competitive learning mechanism, during development devotes neural resources the tasks they best a>
Paper 7:  <Title: Rapid learning of binding-match and binding-error detector circuits via long-term potentiation  .   Abstract: It argued the memorization events situations (episodic memory requires the rapid formation neural circuits responsive binding errors binding matches While the formation circuits responsive binding matches modeled associative learning mechanisms the rapid formation binding errors difficult given their seemingly paradoxical behavior; such a circuit must formed the occurrence in subsequent to it fire anymor>
Paper 8:  <Title: A Neural Network Model of Visual Tilt Aftereffects  .   Abstract: RF-LISSOM, a self-organizing model laterally connected orientation maps in the psychological phenomenon known the tilt aftereffect The same self-organizing processes are the map its lateral connections shown result tilt aftereffects over the adult. The model allows observing large numbers neurons connections simultaneously making relate higher-level phenomena which difficult experimentally The re>
Paper 9:  <Title: A Neural Network Model of Memory Consolidation  .   Abstract: Some forms memory rely temporarily a system brain structures located includes The recall recent events one task relies crucially this system As the event becomes recent the medial temporal lobe becomes critical the recall the recollection appears rely It proposed a process called consolidation responsible transfer memory the medial temporal lobe We examine a network model proposed>
Paper 10:  <Title: Topography And Ocular Dominance: A Model Exploring Positive Correlations  .   Abstract: The map from eye brain in topographic i.e. neighbouring points to In addition when two eyes innervate the same target structure fibres segregate ocular dominance stripes Experimental evidence the frog goldfish suggests these two phenomena subserved We present addresses the formation both topography ocular dominance The model a form competitive learning with subtractive enforcement a weight norm>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Neural Networks

Neural Networks
Prediction:  Neural Networks
Is prediction correct?  True

Prediction: 1
Processing index 2647...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Using Local Trajectory Optimizers To Speed Up Global Optimization In Dynamic Programming  
Abstract: Abstract: Dynamic programming provides a methodology to develop planners and controllers for nonlinear systems. However, general dynamic programming is computationally intractable. We have developed procedures that allow more complex planning and control problems to be solved. We use second order local trajectory optimization to generate locally optimal plans and local models of the value function and its derivatives. We maintain global consistency of the local models of the value function, guaranteeing that our locally optimal plans are actually globally optimal, up to the resolution of our search procedures.
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 4:  <Title: Incremental methods for computing bounds in partially observable Markov decision processes  .   Abstract: Partially observable Markov decision processes allow one model complex dynamic decision or problems include both action outcome uncertainty imperfect observabil-ity The control problem formulated combining costs or rewards from In, analyse various incremental methods computing bounds control problems infinite discounted horizon criteria The methods and include novel incremental versions grid-bas>
Label: Reinforcement Learning
Paper 9:  <Title: Exploiting Structure in Policy Construction  .   Abstract: Markov decision processes ( recently applied the problem modeling decision-theoretic planning While traditional methods solving MDPs practical small states spaces their effectiveness large AI planning problems We present, structured policy iteration constructs without explicit enumeration the state space The algorithm retains the fundamental computational steps the commonly used modified policy iteration algorithm exploits the variable and propositional independe>
Label: Probabilistic Methods
Paper 2:  <Title: Approximating Value Trees in Structured Dynamic Programming  .   Abstract: We propose and examine approximate dynamic programming Markov decision processes structured problem representations We assume an MDP represented construct value functions decision trees our function representation The size the representation kept pruning these value trees leaves represent possible ranges thus produced during optimization We propose convergence prove errors bounds resulting a>
Paper 3:  <Title: Generalized Queries on Probabilistic Context-Free Grammars  on Pattern Analysis and Machine Intelligence  .   Abstract: In methods efficiently computing better solutions control problems continuous state spaces We provide algorithms exploit online search boost very approximate value functions discovered traditional reinforcement learning techniques We examine local searches where the agent performs a finite-depth lookahead search global searches a trajectory all a goal state The key the global methods lies using aggressive state-space search t>
Paper 5:  <Title: Reinforcement Learning with Hierarchies of Machines  .   Abstract: We present reinforcement learning the policies considered constrained hierarchies partially specified machines This allows prior knowledge reduce the search space provides in transferred problems in component solutions recombined Our approach can providing a link reinforcement learning behavior- or teleo-reactive approaches control. We present provably alg>
Paper 6:  <Title: Applying Online Search Techniques to Continuous-State Reinforcement Learning key to the success of the local.   Abstract: In methods efficiently computing better solutions control problems continuous state spaces We provide algorithms exploit online search boost very approximate value functions discovered traditional reinforcement learning techniques We examine local searches where the agent performs a finite-depth lookahead search global searches a trajectory all a goal state The key the global methods lies using aggressive state-space search t>
Paper 7:  <Title: Spurious Solutions to the Bellman Equation  .   Abstract: Reinforcement learning algorithms often work finding functions satisfy the Bellman equation This yields an optimal solution prediction with for controlling a Markov decision process states actions This approach frequently Markov chains MDPs with infinite states We show, the Bellman equation may multiple solutions many lead erroneous predictions policies (Baird Algorithms conditions presented that guarantee>
Paper 8:  <Title: Intelligent Gradient-Based Search of Incompletely Defined Design Spaces  .   Abstract: Gradient-based numerical optimization complex engineering designs offers rapidly producing However such methods generally assume the objective function and constraint functions continuous smooth defined everywhere Unfortunately realistic simulators tend violate these assumptions We present a rule-based technique intelligently computing gradients the presence such pathologies the simulators this gradient computation method part We tested>
Paper 10:  <Title: Structured Reachability Analysis for Markov Decision Processes  .   Abstract: Recent research in decision theoretic planning focussed making the solution Markov decision processes ( feasible We develop a family algorithms structured reachability analysis MDPs that suitable when an initial state ( set Using compact, structured representations MDPs our methods, vary the tradeoff complexity produce structured descriptions (estimated) reachable states that eliminate variables variable values the problem description reduc>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Reinforcement Learning

Reinforcement Learning
Prediction:  Reinforcement Learning
Is prediction correct?  True

Prediction: 1
Processing index 335...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Incremental Reduced Error Pruning  
Abstract: Abstract: This paper outlines some problems that may occur with Reduced Error Pruning in relational learning algorithms, most notably efficiency. Thereafter a new method, Incremental Reduced Error Pruning, is proposed that attempts to address all of these problems. Experiments show that in many noisy domains this method is much more efficient than alternative algorithms, along with a slight gain in accuracy. However, the experiments show as well that the use of the algorithm cannot be recommended for domains which require a very specific concept description.
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 2:  <Title: Incremental Reduced Error Pruning  .   Abstract: This paper outlines some problems Reduced Error Pruning Inductive Logic Programming most notably efficiency Thereafter a new method, Incremental Reduced Error Pruning that attempts address all Experiments show in many noisy domains this method much alternative algorithms along a slight gain However the experiments show as well the use this algorithm recommended domains a very specific concept description>
Paper 3:  <Title: More Efficient Windowing  .   Abstract: Windowing has proposed a procedure efficient memory use the ID3 decision tree learning However previous work windowing may often a decrease performance In this work try argue separate-and-conquer rule learning algorithms more appropriate windowing learn rules independently changes class distributions In particular we will present a new windowing algorithm achieves additional gains efficiency exploiting this property>
Paper 4:  <Title: Top-Down Pruning in Relational Learning  .   Abstract: Pruning an effective method dealing noise Machine Learning Recently pruning algorithms, in particular Reduced Error Pruning also attracted Inductive Logic Programming However shown these methods inefficient most is for generating clauses explain noisy examples subsequently pruning We introduce which searches good theories to get the pruning algorithm Experiments show this approach>
Paper 5:  <Title: Instance Pruning Techniques  .   Abstract: The nearest neighbor algorithm and its derivatives often quite successful learning a concept providing on subsequent input vectors However these techniques often retain the entire training memory resulting large memory requirements a sensitivity noise This paper issues reducing instances retained memory while ( sometimes improving generalization accuracy mentions algorithms other researchers add>
Paper 6:  <Title: Fossil: A Robust Relational Learner  .   Abstract: The research reported describes Fossil, an ILP system a search heuristic based statistical correlation This algorithm implements learning useful concepts the presence noise In contrast Foil's stopping criterion allows theories grow the size the training sets propose independent training examples Instead Fossil's stopping criterion depends a search heuristic estimates literals on a uniform scale In we ou>
Paper 7:  <Title: The Utility of Feature Weighting in Nearest-Neighbor Algorithms  .   Abstract: Nearest-neighbor algorithms known depend their distance metric In this paper investigate a weighted Euclidean metric which for each feature comes options We describe Diet, an algorithm directs search through a space discrete weights using as its evaluation function Although a large set possible weights reduce the learner's bias increased variance Our empirical study, many data sets an advantage weighting features>
Paper 8:  <Title: Rule Induction with CN2: Some Recent Improvements  .   Abstract: The CN2 algorithm induces an ordered list classification rules from examples using entropy its search heuristic In this short paper two improvements this algorithm Firstly present the Laplacian error estimate an alternative evaluation function secondly unordered as ordered rules generated We experimentally demonstrate significantly improved performances resulting these changes thus enhancing CN2 Comparisons Quinlan's C4.5 are>
Paper 9:  <Title: Chapter 1 Reinforcement Learning for Planning and Control  .   Abstract: In a method feature subset selection Information Theory Initially a framework defining theoretically optimal, computation-ally intractable, method feature subset selection We show our goal should eliminate a feature if gives beyond subsumed In particular this will both irrelevant and redundant features We then give an efficient algorithm feature selection which an approximation se>
Paper 10:  <Title: Associative Reinforcement Learning: A Generate and Test Algorithm  .   Abstract: An agent that must learn act by trial faces the reinforcement learning problem quite standard concept learning Although good algorithms exist this problem in quite exhibit generalization One strategy find restricted classes action policies learned more This paper pursues that strategy developing performans an on through the space action mappings expressed Boolean formulae The algorithm compared w>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Rule Learning

Rule Learning
Prediction:  Rule Learning
Is prediction correct?  True

Prediction: 1
Processing index 815...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Genetic Algorithm based Scheduling in a Dynamic Manufacturing Environment  
Abstract: Abstract: The application of adaptive optimization strategies to scheduling in manufacturing systems has recently become a research topic of broad interest. Population based approaches to scheduling predominantly treat static data models, whereas real-world scheduling tends to be a dynamic problem. This paper briefly outlines the application of a genetic algorithm to the dynamic job shop problem arising in production scheduling. First we sketch a genetic algorithm which can handle release times of jobs. In a second step a preceding simulation method is used to improve the performance of the algorithm. Finally the job shop is regarded as a nondeterministic optimization problem arising from the occurrence of job releases. Temporal Decomposition leads to a scheduling control that interweaves both simulation in time and genetic search.
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 7:  <Title: A Genetic Algorithm for Continuous Design Space Search  .   Abstract: Genetic algorithms ( extensively as performing global optimization However some realistic engineering design optimization domains the simple, classical implementation a GA based binary encoding bit mutation and crossover often inefficient unable reach the global optimum In a GA for continuous design-space optimization new GA operators strategies tailored the structure properties engineering design domains the domains superson>
Label: Genetic Algorithms
Paper 2:  <Title: Value Function Based Production Scheduling  .   Abstract: Production scheduling the problem sequentially configuring a factory meet forecasted demands throughout The requirement maintaining product inventories unpredictable demand stochastic factory output makes standard scheduling models, job-shop, inadequate. Currently applied algorithms simulated annealing constraint propagation must employ frequent replanning cope uncertainty In produc>
Paper 3:  <Title: An evolutionary tabu search algorithm and the NHL scheduling problem  .   Abstract: We present a new evolutionary procedure solving combines efficiently the mechanisms genetic algorithms tabu search In order explore the solution space properly interaction phases periods optimization An adaptation this search principle to National Hockey problem discussed The hybrid method developed well Open Shop Scheduling problemsOSSP The results obtained appear quite satisfactory>
Paper 4:  <Title: On Genetic Programming of Fuzzy Rule-Based Systems for Intelligent Control  .   Abstract: Fuzzy logic evolutionary computation have proven convenient tools handling designing control systems respectively An approach is presented combines attributes for developing intelligent control systems The potential the genetic programming paradigm (GP learning rules use fuzzy logic controllersFLCs evaluated discovering a controller mobile robot path tracking Performance results incomplete rule-bases compare favorably a complete FL>
Paper 5:  <Title: A Sampling-Based Heuristic for Tree Search Applied to Grammar Induction  .   Abstract: In the field Operation Research and Artificial Intelligence several stochastic search algorithms designed based global random searchZhigljavsky 1991 Basically those techniques iteratively sample the search space with respect a probability distribution which updated according previous samples some predefined strategy Genetic Algorithms ( (Goldberg 1989 or Greedy Randomized Adaptive Search Procedures (Feo Resende two particular instances this paradigm In SAGE al>
Paper 6:  <Title: Solving Combinatorial Optimization Tasks by Reinforcement Learning: A General Methodology Applied to Resource-Constrained Scheduling  .   Abstract: This paper introduces a methodology solving through reinforcement learning methods The approach can cases several similar instances must The key idea analyze a set "training problem instances learn a search control policy solving new problem instances The search control policy the twin goals finding finding them Results applying this methodology a NASA scheduling problem show lea>
Paper 8:  <Title: AN APPROACH TO A PROBLEM IN NETWORK DESIGN USING GENETIC ALGORITHMS  .   Abstract: Air Traffic Control is involved aircraft trajectories This a heavily constrained optimization problem We concentrate free-route planning in aircraft not way points The choice a proper representation this real-world problem non We propose a two level representation: on the evolutionary operators work a derived level we do calculations Furthermore we show a specific choice the fitness function finding good solutions large problem instances>
Paper 9:  <Title: Evolutionary Computation in Air Traffic Control Planning  .   Abstract: Air Traffic Control is involved aircraft trajectories This a heavily constrained optimization problem We concentrate free-route planning in aircraft not way points The choice a proper representation this real-world problem non We propose a two level representation: on the evolutionary operators work a derived level we do calculations Furthermore we show a specific choice the fitness function finding good solutions large problem instances>
Paper 10:  <Title: Adaptation of Genetic Algorithms for Engineering Design Optimization  .   Abstract: Genetic algorithms extensively different domains as doing global optimization However some realistic engineering design optimization domains was observed a simple classical implementation the GA based binary encoding bit mutation and crossover sometimes inefficient unable reach the global optimum Using floating point representation alone eliminate In augmenting the GA with new operators strategies take the structur>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Genetic Algorithms

Genetic Algorithms
Prediction:  Genetic Algorithms
Is prediction correct?  True

Prediction: 1
Processing index 1841...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Learning Without State-Estimation in Partially Observable Markovian Decision Processes  
Abstract: Abstract: Reinforcement learning (RL) algorithms provide a sound theoretical basis for building learning control architectures for embedded agents. Unfortunately all of the theory and much of the practice (see Barto et al., 1983, for an exception) of RL is limited to Marko-vian decision processes (MDPs). Many real-world decision tasks, however, are inherently non-Markovian, i.e., the state of the environment is only incompletely known to the learning agent. In this paper we consider only partially observable MDPs (POMDPs), a useful class of non-Markovian decision processes. Most previous approaches to such problems have combined computationally expensive state-estimation techniques with learning control. This paper investigates learning in POMDPs without resorting to any form of state estimation. We present results about what TD(0) and Q-learning will do when applied to POMDPs. It is shown that the conventional discounted RL framework is inadequate to deal with POMDPs. Finally we develop a new framework for learning without state-estimation in POMDPs by including stochastic policies in the search space, and by defining the value or utility of a dis tribution over states.
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 3:  <Title: Generalized Markov Decision Processes: Dynamic-programming and Reinforcement-learning Algorithms  .   Abstract: The problem maximizing the expected total discounted reward a completely observable Markovian environmentmdp models a particular class sequential decision problems Algorithms have making optimal decisions mdps given either an mdp specification the opportunity interact over Recently other sequential decision-making problems studied prompting analyses We describe a new generalized model subsumes mdps as many the recent variation>
Label: Reinforcement Learning
Paper 8:  <Title: Between MDPs and Semi-MDPs: Learning, Planning, and Representing Knowledge at Multiple Temporal Scales  .   Abstract: Learning, planning representing knowledge at temporal abstraction key challenges AI In this paper develop these problems based the mathematical framework reinforcement learning Markov decision processes We extend the usual notion action options|whole courses behavior may temporally extended stochastic contingent events Examples options include picking an object going lunch traveling a distant city, primitive actions muscle twitches joint tor>
Label: Reinforcement Learning
Paper 9:  <Title: Planning with Closed-Loop Macro Actions  .   Abstract: Planning learning at multiple levels temporal abstraction In summarize based Markov decision processes reinforcement learning Conventional model-based reinforcement learning uses primitive actions last one time step that modeled independently These can generalized macro actions specified an arbitrary policy a way completing. Macro actions generalize the classical notion a macr>
Label: Reinforcement Learning
Paper 2:  <Title: Reinforcement Learning Algorithm for Partially Observable Markov Decision Problems  .   Abstract: Increasing attention reinforcement learning algorithms partly successes the theoretical analysis their behavior Markov environments If the Markov assumption removed however neither generally the algorithms continue usable We propose and analyze a new learning algorithm a certain class non-Markov decision problems Our algorithm applies to problems the environment Markov, the learner restricted state information The algorithm involves a Monte-Carlo policy evaluati>
Paper 4:  <Title: Approximating Optimal Policies for Partially Observable Stochastic Domains  .   Abstract: The problem making optimal decisions uncertain conditions central Artificial Intelligence If the state the world known at all times can modeled a Markov Decision Process MDPs studied many methods known determining optimal courses or policies The more realistic case where state information only partially observable, Partially Observable Markov Decision Processes have received The best exact algorithms these problems very inefficient in both space ti>
Paper 5:  <Title: Parallel Search for Neural Network  Under the guidance of  .   Abstract: The problem making optimal decisions uncertain conditions central Artificial Intelligence If the state the world known at all times can modeled a Markov Decision Process MDPs studied many methods known determining optimal courses or policies The more realistic case where state information only partially observable, Partially Observable Markov Decision Processes have received The best exact algorithms these problems very inefficient in both space ti>
Paper 6:  <Title: Connectionist Modeling of the Fast Mapping Phenomenon  .   Abstract: The problem making optimal decisions uncertain conditions central Artificial Intelligence If the state the world known at all times can modeled a Markov Decision Process MDPs studied many methods known determining optimal courses or policies The more realistic case where state information only partially observable, Partially Observable Markov Decision Processes have received The best exact algorithms these problems very inefficient in both space ti>
Paper 7:  <Title: TD Models: Modeling the World at a Mixture of Time Scales  .   Abstract: Temporal-difference (TD) learning can not rewards as is commonly states, to a model the world's dynamics We present theory algorithms intermixing TD models the world at temporal abstraction within Such multi-scale TD models model-based reinforcement-learning architectures and dynamic programming methods in place conventional Markov models This enables planning at higher and varied levels abstraction, may pr>
Paper 10:  <Title: Markov games as a framework for multi-agent reinforcement learning  .   Abstract: In the Markov decision process (MDP) formalization reinforcement learning single adaptive interacts defined In this solipsistic view secondary agents part the environment fixed The framework Markov games allows widen this view multiple adaptive agents with interacting or competing goals This paper considers a step exactly two agents share an environment It describes a Q-learning-l>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Reinforcement Learning

Reinforcement Learning
Prediction:  Reinforcement Learning
Is prediction correct?  True

Prediction: 1
Processing index 658...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Hill Climbing with Learning (An Abstraction of Genetic Algorithm)  
Abstract: Abstract: Simple modification of standard hill climbing optimization algorithm by taking into account learning features is discussed. Basic concept of this approach is the socalled probability vector, its single entries determine probabilities of appearance of '1' entries in n-bit vectors. This vector is used for the random generation of n-bit vectors that form a neighborhood (specified by the given probability vector). Within the neighborhood a few best solutions (with smallest functional values of a minimized function) are recorded. The feature of learning is introduced here so that the probability vector is updated by a formal analogue of Hebbian learning rule, well-known in the theory of artificial neural networks. The process is repeated until the probability vector entries are close either to zero or to one. The resulting probability vector unambiguously determines an n-bit vector which may be interpreted as an optimal solution of the given optimization task. Resemblance with genetic algorithms is discussed. Effectiveness of the proposed method is illustrated by an example of looking for global minima of a highly multimodal function. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 4:  <Title: 21 Using n 2 classifier in constructive induction  .   Abstract: In constructive induction The idea an improvement classification accuracy iterative modification input data space This process independently repeated n classes Finally gives (n 2 n)/2 input data subspaces attributes dedicated optimal discrimination appropriate pairs classes We use genetic algorithms as a constructive induction engine A final classification obtained a weighted majority voting rule according n 2 - classifier approach The computational exp>
Label: Theory
Paper 8:  <Title: A Generalized Permutation Approach to Job Shop Scheduling with Genetic Algorithms  .   Abstract: In sequence the tasks a job shop problemJSP on related the technological machine order jobs a new representation technique mathematically knownpermutation with repetition presented The main advantage this single chromosome representation in analogy the traveling salesman problemTSP it produce illegal sets operation sequencesinfeasible symbolic solutions As the representation scheme a crossover operator preserving the initial scheme structure permutatio>
Label: Genetic Algorithms
Paper 9:  <Title: Effects of Occam's Razor in Evolving Sigma-Pi Neural Nets  .   Abstract: Several evolutionary algorithms make hierarchical representations variable size rather linear strings Variable complexity the structures provides an additional representational power which widen evolutionary algorithms The price, however that the search space open- solutions grow arbitrarily large size In study structural complexity the solutions their generalization performance analyzing the fitness landscape sigma-pi neural networks The analys>
Label: Genetic Algorithms
Paper 2:  <Title: Towards Automatic Discovery of Building Blocks in Genetic Programming  .   Abstract: This paper building genetic programming (GP called adaptive representation through learning (ARL The central idea ARL the adaptation the problem representation, by extending terminals functions with The set extracts common knowledge emerging during acquires solving ARL supports subroutine creation deletion Subroutine creation or discovery the differe>
Paper 3:  <Title: Genetic Programming and Redundancy  .   Abstract: The Genetic Programming optimization methodGP elaborated John Koza [ Koza 1992 The search space the problem domain consists computer programs represented parse trees the crossover operator realized an exchange subtrees Empirical analyses show large parts those trees never used or evaluated which these parts irrelevant the solution or redundant This paper concerned redundancy occuring GP. It starts a mathematical description>
Paper 5:  <Title: Optimal Mutation Rates in Genetic Search  .   Abstract: The optimization a single bit string means iterated mutation selection the best (a (1+1)-Genetic Algorithm discussed with respect three simple fitness functions The counting ones problem a standard binary encoded integer a Gray coded integer optimization problem A mutation rate schedule optimal with the success probability mutation presented for the objective functions turns the standard binary code can hamper the search process even case unimodal objective functions While normally a mutation rate>
Paper 6:  <Title: Control of Parallel Population Dynamics by Social-Like Behavior of GA-Individuals  .   Abstract: A frequently observed difficulty genetic algorithms optimization arises premature convergence In preserve genotype diversity develop individuals In this model a population member an active individual assumes social-like behavior patterns Different individuals living assume By moving in a hierarchy "social states" individuals change Changes of social state controlled arguments These argume>
Paper 7:  <Title: MLC Tutorial A Machine Learning library of C classes.  .   Abstract: Several evolutionary algorithms make hierarchical representations variable size rather linear strings Variable complexity the structures provides an additional representational power which widen evolutionary algorithms The price, however that the search space open- solutions grow arbitrarily large size In study structural complexity the solutions their generalization performance analyzing the fitness landscape sigma-pi neural networks The analys>
Paper 10:  <Title: [12] J. Whittaker. Graphical Models in Applied Mathematical Multivariate Statis-  .   Abstract: Self-organizing feature maps usually implemented the low-level neural and parallel distributed processes An external supervisor finds whose weight vector closest in Euclidian distance the neighborhood weight adaptation The weights changed proportional a biologically more plausible implementation similarity measured neighborhood is selected through lateral inhibition weights changed synaptic resources The resulting self-organizin>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Genetic Algorithms

Genetic Algorithms
Prediction:  Genetic Algorithms
Is prediction correct?  True

Prediction: 1
Processing index 1733...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Decision Analysis by Augmented Probability Simulation  
Abstract: Abstract: We provide a generic Monte Carlo method to find the alternative of maximum expected utility in a decision analysis. We define an artificial distribution on the product space of alternatives and states, and show that the optimal alternative is the mode of the implied marginal distribution on the alternatives. After drawing a sample from the artificial distribution, we may use exploratory data analysis tools to approximately identify the optimal alternative. We illustrate our method for some important types of influence diagrams. (Decision Analysis, Influence Diagrams, Markov chain Monte Carlo, Simulation) 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 2:  <Title: Chaos, Fractals, and Genetic Algorithms  .   Abstract: This paper exact solutions and inferences associated finitely generated convex sets distributions Robust Bayesian inference the calculation bounds posterior values given The paper presents exact inference algorithms the circumstances becomes Two classes algorithms numeric approximations developed through transformations on the original model The first transformation reduces the robust inference problem>
Label: Genetic Algorithms
Paper 3:  <Title: Maximum Working Likelihood Inference with Markov Chain Monte Carlo  .   Abstract: Maximum working likelihood (MWL) inference missing data can quite challenging the associated marginal likelihood This problem further the number parameters involved We propose using first obtain both the MWL estimator the working Fisher information matrix and, using quadrature the remaining components the correct asymptotic MWL variance Evaluation the marginal likelihood is not needed We demonstrate consistency a>
Label: Probabilistic Methods
Paper 4:  <Title: Estimating Bayes Factors via Posterior Simulation with the Laplace-Metropolis Estimator  .   Abstract: The key quantity needed Bayesian hypothesis testing and model selection the marginal likelihood for, also the integrated likelihood the data In a way posterior simulation output marginal likelihoods We describe the basic Laplace-Metropolis estimator models without random effects For models random effects the compound Laplace-Metropolis estimator introduced This estimator applied data shown give Batching of sim>
Label: Probabilistic Methods
Paper 6:  <Title: Robustness Analysis of Bayesian Networks with Finitely Generated Convex Sets of Distributions  .   Abstract: This paper exact solutions and inferences associated finitely generated convex sets distributions Robust Bayesian inference the calculation bounds posterior values given The paper presents exact inference algorithms the circumstances becomes Two classes algorithms numeric approximations developed through transformations on the original model The first transformation reduces the robust inference problem>
Label: Probabilistic Methods
Paper 7:  <Title: Markov Chain Monte Carlo Model Determination for Hierarchical and Graphical Log-linear Models  .   Abstract: The Bayesian approach comparing models involves calculating For high-dimensional contingency tables the set plausible models very We focus attention reversible jump Markov chain Monte Carlo (Green 1995 develop strategies calculating hierarchical, graphical or decomposable log-linear models Even for tables moderate size these sets models may The choice suitable prior distributions model parameters also, two examples pr>
Label: Probabilistic Methods
Paper 5:  <Title: On Bayesian analysis of mixtures with an unknown number of components  Summary  .   Abstract: New methodology fully Bayesian mixture analysis developed making reversible jump Markov chain Monte Carlo methods that capable jumping the parameter subspaces corresponding different numbers components A sample from the full joint distribution all unknown variables thereby generated this can a thorough presentation many aspects the posterior distribution The methodology applied here the analysis univariate normal mixtures using a hierarchical prior model offers an approach>
Paper 8:  <Title: Adaptive Markov Chain Monte Carlo through Regeneration  Summary  .   Abstract: Markov chain Monte Carlo evaluating expectations functions interest under a target distribution. done calculating averages the sample path having as its stationary distribution For computational efficiency should rapidly mixing can sometimes achieved only careful design the transition kernel on. An alternative approach might the transition kernel adapt whenever are encountered during>
Paper 9:  <Title: Von Mises type statistics for single site updated local interaction random fields  .   Abstract: Random field models image analysis spatial statistics usually local interactions They simulated Markov chains which update a single site The updating rules typically condition on only a few neighboring sites If approximate the expectation a bounded function can the simulations than through the empirical estimator We describe symmetrizations which lead considerable variance reduction The method reminiscent generalized von Mi>
Paper 10:  <Title: Bayesian Mixture Modeling by Monte Carlo Simulation  .   Abstract: It shown Bayesian inference from modeled a mixture distribution feasibly via This method exhibits the true Bayesian predictive distribution implicitly integrating over An infinite number mixture components without difficulty a prior distribution mixing proportions selects a reasonable subset explain any finite training set The need decide components thereby avoided The feasibility the method sho>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Probabilistic Methods

Probabilistic Methods
Prediction:  Probabilistic Methods
Is prediction correct?  True

Prediction: 1
Processing index 2597...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Improved Heterogeneous Distance Functions  
Abstract: Abstract: Instance-based learning techniques typically handle continuous and linear input values well, but often do not handle nominal input attributes appropriately. The Value Difference Metric (VDM) was designed to find reasonable distance values between nominal attribute values, but it largely ignores continuous attributes, requiring discretization to map continuous values into nominal values. This paper proposes three new heterogeneous distance functions, called the Heterogeneous Value Difference Metric (HVDM), the Interpolated Value Difference Metric (IVDM), and the Windowed Value Difference Metric (WVDM). These new distance functions are designed to handle applications with nominal attributes, continuous attributes, or both. In experiments on 48 applications the new distance metrics achieve higher classification accuracy on average than three previous distance functions on those datasets that have both nominal and continuous attributes.
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 2:  <Title: An Optimal Weighting Criterion of Case Indexing for Both Numeric and Symbolic Attributes  .   Abstract: Indexing of cases Memory-Based Reasoning(MBR One key problem how assign weights attributes cases Although several weighting methods some methods handle numeric attributes directly so discretize numeric values by classification Furthermore existing methods no theoretical background little can optimality We propose a statistical technique Quantification Method II It can handle both numeric and symbolic attributes the same framework>
Label: Case Based
Paper 4:  <Title: AN EFFICIENT METRIC FOR HETEROGENEOUS INDUCTIVE LEARNING APPLICATIONS IN THE ATTRIBUTE-VALUE LANGUAGE 1  .   Abstract: Many inductive learning problems expressed the classical attribute-value language In learn and to generalize learning systems often some measure similarity their current knowledge base The attribute-value language defines some attributes nominal linear Defining similarity or proximity such input spaces non trivial We discuss two representative homogeneous metrics show examples why limited their own domains We then>
Label: Case Based
Paper 7:  <Title: Supervised and Unsupervised Discretization of Continuous Features  .   Abstract: Many supervised machine learning algorithms require a discrete feature space In review continuous feature discretization identify defining characteristics the methods conduct several methods We compare binning, entropy-based and purity-based methods supervised algorithms We found the performance the Naive-Bayes algorithm significantly features discretized using In fact over the 16 tested datasets discreti>
Label: Theory
Paper 10:  <Title: Using Real-Valued Genetic Algorithms to Evolve Rule Sets for Classification  .   Abstract: In evolve classification rules with real-valued attributes We show real-valued attribute ranges encoded present a new uniform method representing do cares the rules We view supervised classification evolve rule sets maximize input instances We use a variant the Pitt approach genetic-based machine learning system with a novel conflict resolution mechanism between competing rules within se>
Label: Genetic Algorithms
Paper 3:  <Title: A Weighted Nearest Neighbor Algorithm for Learning with Symbolic Features  .   Abstract:  nearest neighbor algorithms learning examples worked domains all features had numeric values such domains the examples can treated points distance metrics can use standard definitions symbolic domains a more sophisticated treatment the feature space We introduce a nearest neighbor learning in domains symbolic features Our algorithm calculates distance tables it produce instances attaches weights further modify the str>
Paper 5:  <Title: Continuous-valued Xof-N Attributes Versus Nominal Xof-N Attributes for Constructive Induction: A Case Study  .   Abstract: An Xof-N is containing For a given instance its value corresponds its attribute-value pairs that true In explore performance continuous-valued Xof-N attributes versus constructive induction Nominal Xof-Ns more representationally powerful former suffer although some mechanisms such subsetting help Two approaches constructive induction using continuo>
Paper 6:  <Title: A Review and Empirical Evaluation of Feature Weighting Methods for a Class of Lazy Learning Algorithms  .   Abstract: Many lazy learning algorithms derivatives generate predictions stored instances Several studies k-NN's performance highly sensitive the definition its distance function Many k-NN variants proposed reduce this sensitivity parameterizing the distance function with feature weights However these variants categorized nor empirically compared This paper a class weight-setting methods lazy learning algorithms We introduce a frame>
Paper 8:  <Title: Constructing Nominal Xof-N Attributes  .   Abstract: Most constructive induction researchers focus only new boolean attributes This paper a new constructive induction algorithm called XofN constructs new nominal attributes An Xof-N is containing For a given instance its value corresponds its attribute-value pairs that true The promising preliminary experimental results, on constructing new nominal attributes Xof-N representations impro>
Paper 9:  <Title: Search-based Class Discretization  .   Abstract: We present a methodology enables classification algorithms regression tasks We implement system RECLA that transforms a regression problem classification one and an existent classification system The transformation consists mapping a continuous variable grouping intervals We use misclassification costs reflect the implicit ordering among the new variable We describe alternative discretization>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Case Based

Case Based
Prediction:  Case Based
Is prediction correct?  True

Prediction: 1
Processing index 11...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Simple Genetic Programming for Supervised Learning Problems  
Abstract: Abstract: This paper presents an evolutionary approach to finding learning rules to several supervised tasks. In this approach potential solutions are represented as variable length mathematical LISP S-expressions. Thus, it is similar to Genetic Programming (GP) but it employs a fixed set of non-problem-specific functions to solve a variety of problems. In this paper three Monk's and parity problems are tested. The results indicate the usefulness of the encoding schema in discovering learning rules for supervised learning problems with the emphasis on hard learning problems. The problems and future research directions are discussed within the context of GP practices. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 2:  <Title: Evolving Compact Solutions in Genetic Programming: A Case Study  .   Abstract: Genetic programmingGP where handled trees This makes GP especially evolving functional relationships or computer programs both represented trees Symbolic regression the determination a function dependence y = g(x that i ; In the feasibility symbolic regression with GP is on taken different domains Furthermore several suggested methods from literature compared that intended GP perfor>
Label: Genetic Algorithms
Paper 7:  <Title: Using generative models for handwritten digit recognition  .   Abstract: Genetic Programming program discovery consisting a special kind capable operating nonlinear chromosomesparse trees representing programs and an interpreter run being optimised This paper PDGP (Parallel Distributed Genetic Programming suitable PDGP based a graph-like representation for parallel programs which manipulated crossover and mutation operators which guarantee the syntactic correctness>
Label: Neural Networks
Paper 3:  <Title: "What is the best thing to do right now?": getting beyond greedy exploration  .   Abstract: Genetic programming a methodology program development consisting a special form genetic algorithm capable handling parse trees representing programs that has successfully In this paper a new approach the construction genetic programming A linear chromosome combined a graph representation the network and new operators, allow the evolution the architecture the weights simultaneously without local weight optimization This paper>
Paper 4:  <Title: Knowledge-Based Genetic Learning  .   Abstract: Genetic algorithms proven within the area However there some classes problems where they seem scarcely applicable the solution consists several parts influence In that case the classic genetic operators cross mutation do work very thus preventing a good performance This paper overcome high-level genetic operators integrating task specific but domain independent knowledge guide these ope>
Paper 5:  <Title: MLC Tutorial A Machine Learning library of C classes.  .   Abstract: Several evolutionary algorithms make hierarchical representations variable size rather linear strings Variable complexity the structures provides an additional representational power which widen evolutionary algorithms The price, however that the search space open- solutions grow arbitrarily large size In study structural complexity the solutions their generalization performance analyzing the fitness landscape sigma-pi neural networks The analys>
Paper 6:  <Title: Towards Automatic Discovery of Building Blocks in Genetic Programming  .   Abstract: This paper building genetic programming (GP called adaptive representation through learning (ARL The central idea ARL the adaptation the problem representation, by extending terminals functions with The set extracts common knowledge emerging during acquires solving ARL supports subroutine creation deletion Subroutine creation or discovery the differe>
Paper 8:  <Title: Evolution of the Topology and the Weights of Neural Networks using Genetic Programming with a.   Abstract: Genetic programming a methodology program development consisting a special form genetic algorithm capable handling parse trees representing programs that has successfully In this paper a new approach the construction genetic programming A linear chromosome combined a graph representation the network and new operators, allow the evolution the architecture the weights simultaneously without local weight optimization This paper>
Paper 9:  <Title: Genetic Programming and Redundancy  .   Abstract: The Genetic Programming optimization methodGP elaborated John Koza [ Koza 1992 The search space the problem domain consists computer programs represented parse trees the crossover operator realized an exchange subtrees Empirical analyses show large parts those trees never used or evaluated which these parts irrelevant the solution or redundant This paper concerned redundancy occuring GP. It starts a mathematical description>
Paper 10:  <Title: An evolutionary tabu search algorithm and the NHL scheduling problem  .   Abstract: We present a new evolutionary procedure solving combines efficiently the mechanisms genetic algorithms tabu search In order explore the solution space properly interaction phases periods optimization An adaptation this search principle to National Hockey problem discussed The hybrid method developed well Open Shop Scheduling problemsOSSP The results obtained appear quite satisfactory>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Genetic Algorithms

Genetic Algorithms
Prediction:  Genetic Algorithms
Is prediction correct?  True

Prediction: 1
Processing index 728...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: On the Virtues of Parameterized Uniform Crossover  
Abstract: Abstract: Traditionally, genetic algorithms have relied upon 1 and 2-point crossover operators. Many recent empirical studies, however, have shown the benefits of higher numbers of crossover points. Some of the most intriguing recent work has focused on uniform crossover, which involves on the average L/2 crossover points for strings of length L. Theoretical results suggest that, from the view of hyperplane sampling disruption, uniform crossover has few redeeming features. However, a growing body of experimental evidence suggests otherwise. In this paper, we attempt to reconcile these opposing views of uniform crossover and present a framework for understanding its virtues.
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 2:  <Title: Adapting Crossover in a Genetic Algorithm  .   Abstract: Traditionally genetic algorithms upon 1 and 2-point crossover operators Many recent empirical studies, the benefits higher numbers crossover points Some the most intriguing recent work uniform crossover on the average L/2 crossover points for strings length L. Despite theoretical analysis however difficult predict when a particular crossover form optimal This paper an adaptive genetic algorithm decides, as it runs which form is optimal>
Label: Genetic Algorithms
Paper 3:  <Title: A Comparison of Crossover and Mutation in Genetic Programming  .   Abstract: This paper a large and systematic body mutation, crossover combinations mutation genetic programming (GP The literature of traditional genetic algorithms contains related studies mutation and crossover in GP differ their traditional counterparts In this paper the equivalent approximately 12,000 typical runs a GP system systematically exploring a range parameter settings The resulting data may n>
Label: Genetic Algorithms
Paper 6:  <Title: A STUDY OF CROSSOVER OPERATORS IN GENETIC PROGRAMMING  .   Abstract: Holland's analysis the sources power of genetic algorithms served guidance the applications The technique applying a recombination operatorcrossover a population individuals a key that power Neverless there contradictory results concerning crossover operators with overall performance Recently for genetic algorithms design neural network modules and their control circuits In these studies a genetic algorithm without crossover outperformed>
Label: Genetic Algorithms
Paper 9:  <Title: Generation Gaps Revisited  .   Abstract: There has recent interest so-called "steady state" genetic algorithmsGAs among replace only a few individuals ( 1 each from a fixed size population size N. Understanding the advantages replacing only a fraction each generationrather was a goal some the earliest GA research In spite considerable progress GAs since then the pros/cons overlapping generations a somewhat cloudy issue However, recent the>
Label: Genetic Algorithms
Paper 4:  <Title: An Analysis of the Interacting Roles of Population Size and Crossover in Genetic Algorithms  .   Abstract: In this paper the interacting roles population size crossover genetic algorithms We summarize recent theoretical results the disruptive effect two forms multi-point crossover n-point crossover uniform crossover We then show empirically disruption analysis alone selecting appropriate forms crossover However by taking the interacting effects population size crossover a general picture begins The implications implementation is>
Paper 5:  <Title: Crossover or Mutation?  .   Abstract: Genetic algorithms rely two genetic operators crossover mutation Although exists conventional wisdom concerning crossover mutation these roles captured a theoretical fashion For example it has never theoretically shown mutation in some sense "less powerful crossover vice This paper some answers by theoretically there some important characteristics each operator captured other>
Paper 7:  <Title: Issues in Using Function Approximation for Reinforcement Learning  .   Abstract: Reinforcement learning techniques address select actions unknown, dynamic environments It widely to of complex domains reinforcement learning techniques combined generalizing function approximation methods artificial neural networks Little, however understood the theoretical properties such combinations many researchers encountered failures practice In this paper identify a prime source such failuresnamely systematic overestimation utility values Using Watkins>
Paper 8:  <Title: Performance Enhanced Genetic Programming  .   Abstract: Genetic Programming increasing the basis learning algorithms However the technique to date successfully modest tasks because the performance overheads evolving data structures many do correspond a valid program We address directly demonstrate the evolutionary process achieved much greater efficiency through a formally-based representation and strong typing We report initial experimental results which our technique e>
Paper 10:  <Title: The Role of Development in Genetic Algorithms  .   Abstract: Technical Report Number CS94394 Computer Science Abstract The developmental mechanisms transforming to typically omitted formulationsGAs these two representational spaces identical We argue developmental mechanisms useful understanding the success several standard GA techniques can clarify more recently proposed enhancements We provide distinguishes two developmental mechanisms | learning an>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Genetic Algorithms

Genetic Algorithms
Prediction:  Genetic Algorithms
Is prediction correct?  True

Prediction: 1
Processing index 2027...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Coordinating Reactive Behaviors  keywords: reactive systems, planning and learning  
Abstract: Abstract: Combinating reactivity with planning has been proposed as a means of compensating for potentially slow response times of planners while still making progress toward long term goals. The demands of rapid response and the complexity of many environments make it difficult to decompose, tune and coordinate reactive behaviors while ensuring consistency. Neural networks can address the tuning problem, but are less useful for decomposition and coordination. We hypothesize that interacting reactions can be decomposed into separate behaviors resident in separate networks and that the interaction can be coordinated through the tuning mechanism and a higher level controller. To explore these issues, we have implemented a neural network architecture as the reactive component of a two layer control system for a simulated race car. By varying the architecture, we test whether decomposing reactivity into separate behaviors leads to superior overall performance, coordination and learning convergence. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 2:  <Title: REINFORCEMENT LEARNING FOR COORDINATED REACTIVE CONTROL  .   Abstract: The demands rapid response the complexity many environments decompose, tune coordinate reactive behaviors while ensuring consistency Reinforcement learning networks address the tuning problem do decomposition coordination We hypothesize interacting reactions can often decomposed separate control tasks resident separate networks coordinated the tuning mechanism a higher level controller To explore implemented a reinforcement le>
Paper 3:  <Title: Reinforcement Learning with Hierarchies of Machines  .   Abstract: We present reinforcement learning the policies considered constrained hierarchies partially specified machines This allows prior knowledge reduce the search space provides in transferred problems in component solutions recombined Our approach can providing a link reinforcement learning behavior- or teleo-reactive approaches control. We present provably alg>
Paper 4:  <Title: Learning to coordinate without sharing information  .   Abstract: Researchers in Distributed Artificial Intelligence efficient mechanisms coordinate multiple autonomous agents The need coordination arises agents share resources expertise required achieve Previous work in includes using sophisticated information exchange protocols investigating heuristics negotiation formal models possibilities conflict and agent interests In order handle the changing requirements continuous and dynamic en>
Paper 5:  <Title: A Model for Projection and Action  .   Abstract: In designing autonomous agents that deal competently issues involving time there guaranteed response-time reactions on flexibility expressiveness We propose action with probabilistic reasoning and decision analytic evaluation for use a layered control architecture Our model well tasks reasoning the interaction behaviors events a fixed temporal horizon Decisions continuously so there no problem plans becoming obsol>
Paper 6:  <Title: CABINS A Framework of Knowledge Acquisition and Iterative Revision for Schedule Improvement and Reactive Repair  .   Abstract: Mixed-initiative systems present the challenge finding an effective level interaction humans computers Machine learning presents in systems automatically adapt accommodate different users In learning user models an adaptive assistant crisis scheduling We describe the problem domain the scheduling assistant then present an initial formulation the adaptive assistant's learning task a baseline study After this re>
Paper 7:  <Title: Emergent Hierarchical Control Structures: Learning Reactive/Hierarchical Relationships in Reinforcement Environments  .   Abstract: The use externally imposed hierarchical structures reduce learning control common However acknowledged learning the hierarchical structure itself more general (learning of many things as required bounded specified) Presented this paper called Nested Q-learning generates a hierarchical control structure reinforcement learning domains The emergent structure combined learned bottom-up reactive reactions results>
Paper 8:  <Title: Learning Hierarchical Control Structures for Multiple Tasks and Changing Environments  .   Abstract: While the need hierarchies within control systems apparent many researchers should learned Learning both the structure the component behaviors is The benefit learning the hierarchical structures behaviors the decomposition the control structure smaller transportable chunks previously knowledge new but related tasks Presented this paper improvements Nested Q-learningNQL that allow more realistic learning control hierarchies reinforcement>
Paper 9:  <Title: Learning to Race: Experiments with a Simulated Race Car  .   Abstract: We implemented as the reactive component a simulated race car We separating has expedited gradually improving competition mult-agent interaction We ran experiments the tuning, decomposition coordination the low level behaviors We then extended our control system passing other cars tested avoid The best design used reinforcement learning separate networks coarse coded input a simple rule bas>
Paper 10:  <Title: Automated Decomposition of Model-based Learning Problems  .   Abstract: A new generation sensor rich massively distributed autonomous systems has for unprecedented performance such smart buildings reconfigurable factories adaptive traffic systems remote earth ecosystem monitoring To achieve high performance these massive systems accurately model themselves from sensor information Accomplishing on automating the art This paper a formalization decompositional, ( develope>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Reinforcement Learning

Reinforcement Learning
Prediction:  Reinforcement Learning
Is prediction correct?  True

Prediction: 1
Processing index 46...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: The Pandemonium System of Reflective Agents  
Abstract: Abstract: In IEEE Transactions on Neural Networks, 7(1):97-106, 1996 Also available as GMD report #794 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 2:  <Title: Learning from Examples, Agent Teams and the Concept of Reflection  .   Abstract: In International Journal AI272 1996 Also GMD report #766>
Label: Neural Networks
Paper 3:  <Title: The Role of Development in Genetic Algorithms  .   Abstract: A Genetic Algorithm Tutorial Darrell Whitley Technical Report CS-93103 1993>
Paper 4:  <Title: What should be minimized in a decision tree: A re-examination  .   Abstract: Computer Science Department University Massachusetts CMPSCI Technical Report 9520 September 6>
Paper 5:  <Title: Extended Selection Mechanisms in Genetic Algorithms  .   Abstract: A Genetic Algorithm Tutorial Darrell Whitley Technical Report CS-93103 1993>
Paper 6:  <Title: Experiments with the Cascade-Correlation Algorithm  .   Abstract: Technical 91 July 1991; Revised>
Paper 7:  <Title: Neural Learning of Chaotic Dynamics: The Error Propagation Algorithm trains a neural network to identify.   Abstract: Technical Report UMIACS-TR-97-77 and CS-TR-3843 Abstract>
Paper 8:  <Title: The Design and Evaluation of a Rule Induction Algorithm  .   Abstract: technical report BYUCS-93 June>
Paper 9:  <Title: Hierarchical priors and mixture models, with application in regression and density estimation  .   Abstract: A Genetic Algorithm Tutorial Darrell Whitley Technical Report CS-93103 1993>
Paper 10:  <Title: CABeN: A Collection of Algorithms for Belief Networks  Correspond with:  .   Abstract: Portions this report have the Fifteenth Annual Symposium Computer Applications in Medical CareNovember, 1991>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Neural Networks

Neural Networks
Prediction:  Neural Networks
Is prediction correct?  True

Prediction: 1
Processing index 833...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Graph Coloring with Adaptive Evolutionary Algorithms  
Abstract: Abstract: This paper presents the results of an experimental investigation on solving graph coloring problems with Evolutionary Algorithms (EA). After testing different algorithm variants we conclude that the best option is an asexual EA using order-based representation and an adaptation mechanism that periodically changes the fitness function during the evolution. This adaptive EA is general, using no domain specific knowledge, except, of course, from the decoder (fitness function). We compare this adaptive EA to a powerful traditional graph coloring technique DSatur and the Grouping GA on a wide range of problem instances with different size, topology and edge density. The results show that the adaptive EA is superior to the Grouping GA and outperforms DSatur on the hardest problem instances. Furthermore, it scales up better with the problem size than the other two algorithms and indicates a linear computational complexity. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 6:  <Title: Effects of Occam's Razor in Evolving Sigma-Pi Neural Nets  .   Abstract: Several evolutionary algorithms make hierarchical representations variable size rather linear strings Variable complexity the structures provides an additional representational power which widen evolutionary algorithms The price, however that the search space open- solutions grow arbitrarily large size In study structural complexity the solutions their generalization performance analyzing the fitness landscape sigma-pi neural networks The analys>
Label: Genetic Algorithms
Paper 2:  <Title: Feature Selection Methods: Genetic Algorithms vs. Greedy-like Search  .   Abstract: This paper two feature selection methods the Importance Score which a greedy-like search and a genetic algorithm-based (GA) method in order better their strengths limitations and their area The results our experiments a very strong relation the nature the data both systems The Importance Score method more efficient dealing little noise small number interacting features while the genetic algorithms provide at the exp>
Paper 3:  <Title: An Evolutionary Approach to Combinatorial Optimization Problems  .   Abstract:  paper reports the application genetic algorithms based the model organic evolution NP-complete combinatorial optimization problems In particular the subset sum, maximum cut minimum tardy task problems considered Except the fitness function no problem-specific changes of the genetic algorithm results of high quality even the problem instances size 100 used For constrained problems the subset sum and the minimum tardy task the constraints taken>
Paper 4:  <Title: MLC Tutorial A Machine Learning library of C classes.  .   Abstract: Several evolutionary algorithms make hierarchical representations variable size rather linear strings Variable complexity the structures provides an additional representational power which widen evolutionary algorithms The price, however that the search space open- solutions grow arbitrarily large size In study structural complexity the solutions their generalization performance analyzing the fitness landscape sigma-pi neural networks The analys>
Paper 5:  <Title: Evolving Optimal Neural Networks Using Genetic Algorithms with Occam's Razor  .   Abstract: Genetic algorithms neural networks two main ways optimize the network architecture train the weights a fixed architecture While most previous work focuses only of investigates an alternative evolutionary approach called Breeder Genetic Programming the architecture the weights optimized simultaneously The genotype each network represented whose depth and dynamically adapted the particular application by specifically defined genetic operators The weights t>
Paper 7:  <Title: Genetic algorithms with multi-parent recombination  .   Abstract: In genetic algorithms where the recombination operation In particular introduce gene scanning a reproduction mechanism generalizes classical crossovers n-point crossover uniform crossover applicable an arbitrary number (two parents We performed optimizing numerical functions the TSP graph coloring to observe different numbers parents The experiments 2-parent recombination outperformed using more parents on the cl>
Paper 8:  <Title: Incremental Co-evolution of Organisms: A New Approach for Optimization and Discovery of Strategies  .   Abstract: In the field optimization techniques some very efficient and promising tools like Hill-Climbing have designed In this same field presented an inventive way explore the space states, using simulated "incremental co some organisms remedies some drawbacks these previous techniques even allow this model them on This new model applied the sorting network problem a reference prob>
Paper 9:  <Title: Adaptation of Genetic Algorithms for Engineering Design Optimization  .   Abstract: Genetic algorithms extensively different domains as doing global optimization However some realistic engineering design optimization domains was observed a simple classical implementation the GA based binary encoding bit mutation and crossover sometimes inefficient unable reach the global optimum Using floating point representation alone eliminate In augmenting the GA with new operators strategies take the structur>
Paper 10:  <Title: Putting the Genetics back into Genetic Algorithms  .   Abstract: In genetic algorithms where the recombination operation In particular introduce gene scanning a reproduction mechanism generalizes classical crossovers n-point crossover uniform crossover applicable an arbitrary number (two parents We performed optimizing numerical functions the TSP graph coloring to observe different numbers parents The experiments 2-parent recombination outperformed using more parents on the cl>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Genetic Algorithms

Genetic Algorithms
Prediction:  Genetic Algorithms
Is prediction correct?  True

Prediction: 1
Processing index 1021...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Lemma 2.3 The system is reachable and observable and realizes the same input/output behavior as
Abstract: Abstract: Here we show a similar construction for multiple-output systems, with some modifications. Let = (A; B; C) s be a discrete-time sign-linear system with state space IR n and p outputs. Perform a change of ; where A 1 (n 1 fi n 1 ) is invertible and A 2 (n 2 fi n 2 ) is nilpotent. If (A; B) is a reachable pair and (A; C) is an observable pair, then is minimal in the sense that any other sign-linear system with the same input/output behavior has dimension at least n. But, if n 1 &lt; n, then det A = 0 and is not observable and hence not canonical. Let us find another system ~ (necessarily not sign-linear) which has the same input/output behavior as , but is canonical. Let i be the relative degree of the ith row of the Markov sequence A, and = minf i : i = 1; : : : ; pg. Let the initial state be x. There is a difference between the case when the smallest relative degree is greater or equal to n 2 and the case when &lt; n 2 . Roughly speaking, when n 2 the outputs of the sign-linear system give us information about sign (Cx), sign (CAx), : : : , sign (CA 1 x), which are the first outputs of the sys tem. After that, we can use the inputs and outputs to learn only about x 1 (the first n 1 components of x). When &lt; n 2 , we may be able to use some controls to learn more about x 2 (the last n 2 components of x) before time n 2 when the nilpotency of A 2 has finally Lemma 2.4 Two states x and z are indistinguishable for if and only if (x) = (z). Proof. In the case n 2 , we have only the equations x 1 = z 1 and the equality of the 's. The first ` output terms for are exactly the terms of . So these equalities are satisfied if and only if the first ` output terms coincide for x and z, for any input. Equality of everything but the first n 1 components is equivalent to the first n 2 output terms coinciding for x and z, since the jth row of the qth output, for initial state x, for example, is either sign (c j A q x) if j &gt; q, or sign (c j A q x + + A j j u q j +1 + ) if j q in which case we may use the control u q j +1 to identify c j A q x (using Remark 3.3 in [1]). 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 2:  <Title: Inserting the best known bounds for weighted bipar tite matching [11], with 1=2 p polynomial-time.   Abstract: we apply the reduction their two core children the total sum their matching weights becomes O(n if for each comparison a spine node a critical node we With regards the O( 2 ) comparisons of two critical nodes their sum exceed O n in total weight Thus since a total O(n edges involved the matchings in time reduce the total sum the matching weights 2 n Theorem 6.6 Let M : IR 1! be a monotone functi>
Label: Theory
Paper 6:  <Title: A Computer Scientist's View of Life, the Universe, and Everything  .   Abstract: Is the universe computable If so much cheaper in information requirements compute all computable universes instead just ours I apply basic concepts the set chat perceived and true randomness life generalization learning Assumptions. A long the Great Programmer wrote runs all possible universes His Big ComputerPossible meanscomputable ( Each universe evolves on (2 Any universe's state at i>
Label: Reinforcement Learning
Paper 9:  <Title: The Challenge of Revising an Impure Theory  .   Abstract: A pure rule-based program will return answers; set even its rules re However, an impure program the Prolog cut "!" not() operators return different answers re There also many reasoning systems return found for; too depend the rule order even A theory revision algorithm seeking a revised rule-base whose expected accuracy, over the distribution queries optimal sh>
Label: Theory
Paper 3:  <Title: 82 Lag-space estimation in time-series modelling keep track of cases where the estimation of P.   Abstract: When m = (no delays set A 0 (ffi; ; j 6= kg such P m ( depends *. The estimated probabilities above become noisy elements set A m small For estimate P m (*jffi Notice this estimate the empirical averageeither given couple satisfied on ffi * it The standard deviation then estimated easily: Generally P m (*jffi increases *laxer output test when ffi approaches>
Paper 4:  <Title: ABSTRACTION CONSIDERED HARMFUL: LAZY LEARNING OF LANGUAGE PROCESSING  .   Abstract: When m = (no delays set A 0 (ffi; ; j 6= kg such P m ( depends *. The estimated probabilities above become noisy elements set A m small For estimate P m (*jffi Notice this estimate the empirical averageeither given couple satisfied on ffi * it The standard deviation then estimated easily: Generally P m (*jffi increases *laxer output test when ffi approaches>
Paper 5:  <Title: j  .   Abstract: So applying Corollary the second equation in (47 conclude From38 we then get jg(y n + ~ k( y (51 we obtain From we see54 bounded. Since the system _ y = A 1 y k( yb 2 jyj ev N :55 Now lim sup t!1 jy(t)j = &gt; 0 Then jyj ev 2 Since j k(y)j Ljyj and using (56 obtain j~yj ev 2(~-1 +L + ffi :58Note if 1 the inequality trivial since52.) From ( + N>
Paper 7:  <Title: NP-Completeness of Searches for Smallest Possible Feature Sets a subset of the set of all.   Abstract: In many learning problems presented values for features actually irrelevant it The FOCUS algorithm due Almuallim and Dietterich performs an explicit search the smallest possible input feature set S that permits a consistent mapping from The FOCUS algorithm can seen learning determinations or functional dependencies suggested6 Another algorithm learning determinations appears [7 The FOCUS algorithm superpolynomia>
Paper 8:  <Title: Vapnik-Chervonenkis entropy of the spherical perceptron  .   Abstract: Perceptron learning of randomly labeled patterns analyzed a Gibbs distribution on the set realizable labelings The entropy this distribution an extension VapnikChervonenkis (VC reducing it exactly in the limit infinite temperature The close relationship the VC and Gardner entropies can within the replica formalism There recent progress understanding the statistical physics Vapnik-Chervonenkis (VC approaches learning theory[1 The two approaches can>
Paper 10:  <Title: Finding Promising Exploration Regions by Weighting Expected Navigation Costs continuous environments, some first-order approximations to.   Abstract: In many learning tasks data-query neither free of constant cost Often the cost a query depends the distance in state space This easiest visualize robotics environments must physically learn something there The cost this learning the time reach the new location Furthermore this cost characterized a distance relationship: When the robot moves as directly a source state states through>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Neural Networks

Category: Theory
Prediction:  Category: Theory
Is prediction correct?  False

Prediction: 0
Processing index 955...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Separating Formal Bounds from Practical Performance in Learning Systems  
Abstract: Abstract: We present a distribution model for binary vectors, called the influence combination model and show how this model can be used as the basis for unsupervised learning algorithms for feature selection. The model is closely related to the Harmonium model defined by Smolensky [RM86][Ch.6]. In the first part of the paper we analyze properties of this distribution representation scheme. We show that arbitrary distributions of binary vectors can be approximated by the combination model. We show how the weight vectors in the model can be interpreted as high order correlation patterns among the input bits. We compare the combination model with the mixture model and with principle component analysis. In the second part of the paper we present two algorithms for learning the combination model from examples. The first algorithm is based on gradient ascent. Here we give a closed form for this gradient that is significantly easier to compute than the corresponding gradient for the general Boltzmann machine. The second learning algorithm is a greedy method that creates the hidden units and computes their weights one at a time. This method is a variant of projection pursuit density estimation. In the third part of the paper we give experimental results for these learning methods on synthetic data and on natural data of handwritten digit images. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 4:  <Title: Learning a set of primitive actions with an Induction of decision trees. Machine Learning, 1(1):81-106,.   Abstract: Although probabilistic inference in a general Bayesian belief network inference computation time reduced most practical cases exploiting domain knowledge by making the knowledge representation In the property similarity states a new method approximate knowledge representation which We define two or more states a node to similar when the likelihood ratio their probabilities does depend the instantiations netwo>
Label: Theory
Paper 9:  <Title: Boosting a weak learning algorithm by majority To be published in Information and Computation  .   Abstract: We present an algorithm improving algorithms learning binary concepts The improvement achieved combining hypotheses each generated training given learning examples Our algorithm ideas presented Schapire inThe strength weak learnability represents an improvement his results The analysis our algorithm provides general upper bounds the resources required learning in Valiant's polynomial PAC learning framework are the best genera>
Label: Theory
Paper 2:  <Title: Unsupervised learning of distributions on binary vectors using two layer networks  .   Abstract: We present a distribution model binary vectors called the influence combination model and show as feature selection The model closely the Harmonium model defined Smolensky [RM86][Ch.6 In the paper analyze properties this distribution representation scheme We show arbitrary distributions binary vectors the combination model. We show the weight vectors interpreted high order correlation patterns among t>
Paper 3:  <Title: On the Learnability and Usage of Acyclic Probabilistic Finite Automata  .   Abstract: We propose and analyze a distribution learning a subclass This subclass characterized a certain distinguishability property the automata's states Though hardness results are known learning distributions generated general APFAs prove our algorithm efficiently the subclass we consider In particular show the KL-divergence the distribution generated the target source our hypothesis made arbitra>
Paper 5:  <Title: Distribution Category:  Users Guide to the PGAPack Parallel Genetic Algorithm Library  .   Abstract: The problem modeling complicated data sequences speech often practice Most the algorithms select a hypothesis within a model class assuming the observed sequence the direct output In when the output passes a memoryless noisy channel before observation In particular show the class Markov chains variable memory length learning affected factors, which, despite super still small some practical cases Markov model>
Paper 6:  <Title: EXPERIMENTING WITH THE CHEESEMAN-STUTZ EVIDENCE APPROXIMATION FOR PREDICTIVE MODELING AND DATA MINING  .   Abstract: The work discussed motivated of building decision support systems Our goal use these systems supporting Bayes optimal decision making where the action maximizing the expected utility, with respect predicted probabilities should selected For this reason the models used need probabilistic | the output has numbers For the model family we chosen the set simple discrete finite mixture mo>
Paper 7:  <Title: Constructing Bayesian finite mixture models by the EM algorithm  .   Abstract: Email: FirstnameHelsinkiFI Report C-19969, University Department Abstract In explore finite mixture models building decision support systems capable sound probabilistic inference Finite mixture models have many appealing properties computationally in the prediction (reasoning) phase universal the sense approximate any problem domain distribution handle multimod-ality well We present a formulation the model construction problem the Bayes>
Paper 8:  <Title: Training Algorithms for Hidden Markov Models Using Entropy Based Distance Functions  .   Abstract: We present new algorithms parameter estimation HMMs By adapting supervised learning construct maximize the observations while attempting stay close the current estimated parameters We use a bound on the relative entropy between the two HMMs as between The result new iterative training algorithms similar the EM (Baum-Welch training HMMs composed similar the expectation step a new upd>
Paper 10:  <Title: Support Vector Machines: Training and Applications  .   Abstract: The Support Vector Machine a new and very promising classification technique Vapnik his group [3, 6 24 This new learning algorithm can seen an alternative training technique Polynomial, Radial Basis Function and Multi-Layer Perceptron classifiers The main idea the technique separate the classes with a surface maximizes the margin An interesting property this approach an approximate implementation Structural Risk Minimization induction principle [23 The>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Theory

Neural Networks
Prediction:  Neural Networks
Is prediction correct?  False

Prediction: 0
Processing index 1528...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Using Qualitative Models to Guide Inductive Learning  
Abstract: Abstract: This paper presents a method for using qualitative models to guide inductive learning. Our objectives are to induce rules which are not only accurate but also explainable with respect to the qualitative model, and to reduce learning time by exploiting domain knowledge in the learning process. Such ex-plainability is essential both for practical application of inductive technology, and for integrating the results of learning back into an existing knowledge-base. We apply this method to two process control problems, a water tank network and an ore grinding process used in the mining industry. Surprisingly, in addition to achieving explainability the classificational accuracy of the induced rules is also increased. We show how the value of the qualitative models can be quantified in terms of their equivalence to additional training examples, and finally discuss possible extensions.
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 6:  <Title: Theory Revision in Fault Hierarchies  .   Abstract: The fault hierarchy representation widely expert systems the diagnosis complex mechanical devices On the assumption an appropriate bias a knowledge representation language also learning in a theory revision method operates directly a fault hierarchy This task presents several challenges: A typical training instance missing most feature values the pattern significant rather merely an effect noise Moreover the accuracy a candidate theory measure>
Label: Theory
Paper 9:  <Title: Hidden Markov Model Analysis of Motifs in Steroid Dehydrogenases and their Homologs  .   Abstract: Methods to build function approximators example data have gained Especially methodologies build models allow an interpretation have attracted Most existing algorithms however either complicated high-dimensional problems presents efficient algorithm construct fuzzy graphs example data The resulting fuzzy graphs based locally independent fuzzy rules operate solely selected, important attributes This enables these fuzzy>
Label: Neural Networks
Paper 2:  <Title: Rule Generation and Compaction in the wwtp  .   Abstract: In learning classification rules data We sketch two modules our architecture namely LINNEO + and GAR LINNEO +, which a knowledge acquisition tool ill-structured domains automatically generating classes examples incrementally works LINNEO + 's output, a representation the conceptual structure classes the input GAR that classification rules GAR can generate both conjunctive and disjunctive r>
Paper 3:  <Title: Computation and Psychophysics of Sensorimotor Integration  .   Abstract: In learning classification rules data We sketch two modules our architecture namely LINNEO + and GAR LINNEO +, which a knowledge acquisition tool ill-structured domains automatically generating classes examples incrementally works LINNEO + 's output, a representation the conceptual structure classes the input GAR that classification rules GAR can generate both conjunctive and disjunctive r>
Paper 4:  <Title: Constructing Intermediate Concepts by Decomposition of Real Functions  .   Abstract: In learning from examples it expand an attribute-vector representation intermediate concepts The usual advantage such structuring of the learning problem easier improves induced descriptions In develop discovering useful intermediate concepts when both the class the attributes real The technique a decomposition method originally the design switching circuits recently extended handle incompletely specified multivalued>
Paper 5:  <Title: Lookahead and Discretization in ILP  .   Abstract: We present and evaluate two methods improving ILP systems One them discretization numerical attributes based Fayyad and Irani text [9 adapted and extended cope some aspects only occur relational learning problemswhen indeterminate literals occur The second technique lookahead It ILP a learner always assess a refinement without enabled afterwards without looking ahead the refinemen>
Paper 7:  <Title: FONN: Combining First Order Logic with Connectionist Learning  .   Abstract: This paper manage structured data refine knowledge bases expressed a first order logic language The presented framework well classification problems concept de scriptions depend numerical features In fact the main goal the neural architecture that refining the numerical part without changing In particular discuss a method translate a set classification rules neural computation units Here, focus the translat>
Paper 8:  <Title: Cost-sensitive feature reduction applied to a hybrid genetic algorithm  .   Abstract: This study is concerned whether it detect what information contained the training data and background knowledge is solving irrelevant information eliminated preprocessing before starting A case study data preprocessing a hybrid genetic algorithm shows the elimination irrelevant features substantially learning In addition cost-sensitive feature elimination can effective induced hypotheses>
Paper 10:  <Title: Learning Approximate Control Rules Of High Utility  .   Abstract: One the difficult problems the area explanation based learning the utility problem; learning too many rules of low utility swamping degradation performance This paper introduces two new techniques improving the utility learned rules The first technique combine EBL inductive learning techniques learn a better set control rules; approximate control rules The two techniques synthesized an algorithm approximating abductive explanation based learning>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Rule Learning

Rule Learning
Prediction:  Rule Learning
Is prediction correct?  True

Prediction: 1
Processing index 1434...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Advantages of Decision Lists and Implicit Negatives in Inductive Logic Programming  
Abstract: Abstract: This paper demonstrates the capabilities of Foidl, an inductive logic programming (ILP) system whose distinguishing characteristics are the ability to produce first-order decision lists, the use of an output completeness assumption as a substitute for negative examples, and the use of intensional background knowledge. The development of Foidl was originally motivated by the problem of learning to generate the past tense of English verbs; however, this paper demonstrates its superior performance on two different sets of benchmark ILP problems. Tests on the finite element mesh design problem show that Foidl's decision lists enable it to produce generally more accurate results than a range of methods previously applied to this problem. Tests with a selection of list-processing problems from Bratko's introductory Prolog text demonstrate that the combination of implicit negatives and intensionality allow Foidl to learn correct programs from far fewer examples than Foil.
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 2:  <Title: on Inductive Logic Programming (ILP-95) Inducing Logic Programs without Explicit Negative Examples  .   Abstract: This paper learning logic programs without explicit negative examples by exploiting an assumption output completeness A mode declaration supplied the target predicate each training input assumed accompanied all its legal outputs Any other outputs generated an incomplete program implicitly represent negative examples; however large numbers ground negative examples never This method incorporated two ILP systems Chillin IFoil intensional background knowledge Tests>
Label: Rule Learning
Paper 3:  <Title: Combining Top-down and Bottom-up Techniques in Inductive Logic Programming  .   Abstract: This paper inducing logic programs from examples which attempts integrate existing ILP methods In particular combines similar Golem Foil It also a method predicate invention similar Champ and an elegant solution allows learn recursive programs positive examples Systematic experimental comparisons to both Golem Foil on a range problem>
Label: Rule Learning
Paper 4:  <Title: Knowledge Acquisition with a Knowledge-Intensive Machine Learning System  .   Abstract: In this paper investigate the integration knowledge acquisition. We argue existing machine learning techniques made as knowledge acquisition tools allowing the expert have and interaction We describe extensions FOCLa multistrategy Horn-clause learning program have greatly its power paying the utility maintaining a connection a rule the set examples explained th>
Label: Rule Learning
Paper 9:  <Title: Theory Revision in Fault Hierarchies  .   Abstract: The fault hierarchy representation widely expert systems the diagnosis complex mechanical devices On the assumption an appropriate bias a knowledge representation language also learning in a theory revision method operates directly a fault hierarchy This task presents several challenges: A typical training instance missing most feature values the pattern significant rather merely an effect noise Moreover the accuracy a candidate theory measure>
Label: Theory
Paper 10:  <Title: An Experimental Comparison of Genetic Programming and Inductive Logic Programming on Learning Recursive List Functions  .   Abstract: This paper experimentally three approaches program induction inductive logic programming genetic programmingGP genetic logic programminga variant GP for inducing Pro-log programs Each was induce four simple, recursive, list-manipulation functions The results indicate ILP likely a correct program from small sets random examples GP generally accurate GLP performs worst, rarely able induce a correct program Interpretations these results in terms differenc>
Label: Rule Learning
Paper 5:  <Title: Natural Language Grammatical Inference with Recurrent Neural Networks  .   Abstract: This paper the inductive inference a complex grammar specifically, the task considered is training thereby exhibiting discriminatory power provided the Principles and Parameters linguistic framework Government-and-Binding theory Neural networks trained, without the division into learned innate components assumed Chomsky produce the same judgments native speakers on sharply grammatical/ungrammatical data Ho>
Paper 6:  <Title: ILP with Noise and Fixed Example Size: A Bayesian Approach  .   Abstract: Current inductive logic programming systems limited their handling noise as employ a greedy covering approach constructing the hypothesis one clause This approach also causes difficulty learning recursive predicates Additionally many current systems an implicit expectation the cardinality reflect the concept the instance space A framework learning noisy data fixed example size is A Bayesian heuristic finding this genera>
Paper 7:  <Title: Inductive Constraint Logic and the Mutagenesis Problem  .   Abstract: A novel approach learning first order logic formulae incorporated a system named ICL In ICL examples viewed interpretations which true for the target theory present inductive logic programming systems true and false ground facts clauses Furthermore ICL uses a clausal representation corresponds a conjunctive normal form where forms a constraint positive examples classical learning techniques concentrated concept re>
Paper 8:  <Title: DISTRIBUTED GENETIC ALGORITHMS FOR PARTITIONING UNIFORM GRIDS  .   Abstract: The fault hierarchy representation widely expert systems the diagnosis complex mechanical devices On the assumption an appropriate bias a knowledge representation language also learning in a theory revision method operates directly a fault hierarchy This task presents several challenges: A typical training instance missing most feature values the pattern significant rather merely an effect noise Moreover the accuracy a candidate theory measure>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Rule Learning

Category: Rule Learning
Prediction:  Category: Rule Learning
Is prediction correct?  False

Prediction: 0
Processing index 2641...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Toward Simulated Evolution of Machine-Language Iteration  
Abstract: Abstract: We use a simulated evolution search (genetic programming) for the automatic synthesis of small iterative machine-language programs. For an integer register machine with an addition instruction as its sole arithmetic operator, we show that genetic programming can produce exact and general multiplication routines by synthesizing the necessary iterative control structures from primitive machine-language instructions. Our program representation is a virtual register machine that admits arbitrary control flow. Our evolution strategy furthermore does not artificially restrict the synthesis of any control structure; we only place an upper bound on program evaluation time. A program's fitness is the distance between the output produced by a test case and the desired output (multiplication). The test cases exhaustively cover multiplication over a finite subset of the natural numbers (N 10 ); yet the derived solutions constitute general multiplication for the positive integers. For this problem, simulated evolution with a two-point crossover operator examines significantly fewer individuals in finding a solution than random search. Introduction of a small rate of mutation fur ther increases the number of solutions.
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 2:  <Title: Learning to Play Games From Experience: An Application of Artificial Neural Networks and Temporal Difference Learning  .   Abstract: We use a simulated evolution search the automatic synthesis small iterative machine-language programs For an integer register machine an addition instruction as show genetic programming exact and general multiplication routines by Our program representation a virtual register machine admits arbitrary control flow Our evolution strategy furthermore does artificially the synthesis>
Label: Neural Networks
Paper 4:  <Title: Evolving Turing-Complete Programs for a Register Machine with Self-modifying Code  .   Abstract: The majority commercial computers today register machines of von Neumann type We developed evolve Turing-complete programs a register machine The described implementation enables most program constructs arithmetic operators large indexed memory automatic decomposition into subfunctionsADFs conditional constructs if jumps loop structures protected string and list functions Any C-function can compiled linked the function set the system The use register mac>
Label: Genetic Algorithms
Paper 3:  <Title: Learning Recursive Sequences via Evolution of Machine-Language Programs  .   Abstract: We use directed search techniques the space computer programs learn recursive sequences Specifically the integer sequences squares x 2 ; cubes factorial!; studied Given a small finite prefix show three directed searches|machine-language genetic programming with crossover exhaustive iterative hill climbing a hybrid automatically discover programs exactly reproduce the finite target prefix, moreover correctly the remaining sequen>
Paper 5:  <Title: MLC Tutorial A Machine Learning library of C classes.  .   Abstract: Several evolutionary algorithms make hierarchical representations variable size rather linear strings Variable complexity the structures provides an additional representational power which widen evolutionary algorithms The price, however that the search space open- solutions grow arbitrarily large size In study structural complexity the solutions their generalization performance analyzing the fitness landscape sigma-pi neural networks The analys>
Paper 6:  <Title: State Reconstruction for Determining Predictability in Driven Nonlinear Acoustical Systems  .   Abstract: Genetic programming distinguished other evolutionary algorithms tree representations variable size instead linear strings The flexible representation scheme very the underlying structure discovered automatically One primary difficulty, the solutions may grow too without any improvement of their generalization ability In this paper investigate the fundamental relationship the performance complexity the evolved structures. The essence the parsimony>
Paper 7:  <Title: Balancing Accuracy and Parsimony in Genetic Programming 1  .   Abstract: Genetic programming distinguished other evolutionary algorithms tree representations variable size instead linear strings The flexible representation scheme very the underlying structure discovered automatically One primary difficulty, the solutions may grow too without any improvement of their generalization ability In this paper investigate the fundamental relationship the performance complexity the evolved structures. The essence the parsimony>
Paper 8:  <Title: Inverting Implication with Small Training Sets  .   Abstract: We present inducing recursive clauses inverse implicationrather inverse resolution Our approach applies a class logic programs similar primitive recursive functions Induction performed positive examples need along Our algorithm, implemented a system named CRUSTACEAN locates matched lists generating terms determine decomposition exhibited the (target) recursive clause Our theoretical analysis defi>
Paper 9:  <Title: Optimal Mutation Rates in Genetic Search  .   Abstract: The optimization a single bit string means iterated mutation selection the best (a (1+1)-Genetic Algorithm discussed with respect three simple fitness functions The counting ones problem a standard binary encoded integer a Gray coded integer optimization problem A mutation rate schedule optimal with the success probability mutation presented for the objective functions turns the standard binary code can hamper the search process even case unimodal objective functions While normally a mutation rate>
Paper 10:  <Title: A Genome Compiler for High Performance Genetic Programming  .   Abstract: Genetic Programming very computationally For most applications time evaluating candidate solutions so desirable individual evaluation as efficient We describe a genome compiler s machine code resulting individual evaluations over standard GP systems Based performance results with symbolic regression show the execution the genome compiler system comparable the fastest alternative GP systems We also demonstrate compilati>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Genetic Algorithms

Category: Genetic Algorithms
Prediction:  Category: Genetic Algorithms
Is prediction correct?  False

Prediction: 0
Processing index 2648...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: The Task Rehearsal Method of Sequential Learning  
Abstract: Abstract: An hypothesis of functional transfer of task knowledge is presented that requires the development of a measure of task relatedness and a method of sequential learning. The task rehearsal method (TRM) is introduced to address the issues of sequential learning, namely retention and transfer of knowledge. TRM is a knowledge based inductive learning system that uses functional domain knowledge as a source of inductive bias. The representations of successfully learned tasks are stored within domain knowledge. Virtual examples generated by domain knowledge are rehearsed in parallel with the each new task using either the standard multiple task learning (MTL) or the MTL neural network methods. The results of experiments conducted on a synthetic domain of seven tasks demonstrate the method's ability to retain and transfer task knowledge. TRM is shown to be effective in developing hypothesis for tasks that suffer from impoverished training sets. Difficulties encountered during sequential learning over the diverse domain reinforce the need for a more robust measure of task relatedness. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 3:  <Title: Theory Revision in Fault Hierarchies  .   Abstract: The fault hierarchy representation widely expert systems the diagnosis complex mechanical devices On the assumption an appropriate bias a knowledge representation language also learning in a theory revision method operates directly a fault hierarchy This task presents several challenges: A typical training instance missing most feature values the pattern significant rather merely an effect noise Moreover the accuracy a candidate theory measure>
Label: Theory
Paper 8:  <Title: Knowledge Acquisition with a Knowledge-Intensive Machine Learning System  .   Abstract: In this paper investigate the integration knowledge acquisition. We argue existing machine learning techniques made as knowledge acquisition tools allowing the expert have and interaction We describe extensions FOCLa multistrategy Horn-clause learning program have greatly its power paying the utility maintaining a connection a rule the set examples explained th>
Label: Rule Learning
Paper 2:  <Title: The Functional Transfer of Knowledge for Coronary Artery Disease Diagnosis  .   Abstract: A distinction two forms task knowledge transfer representational functional reviewed followed MTL a modified version multiple ( neural network method of functional transfer The MTL method employs a separate learning rate k, each task output node k. k varies as a measure relatedness R k between the kth task An MTL network applied a diagnostic domain four levels coronary artery disease Results experiments demonstrate MTL develop>
Paper 4:  <Title: Experiments on the Transfer of Knowledge between Neural Networks Reprinted from: Computational Learning Theory and.   Abstract: This chapter three studies address neural network learning via the incorporation information extracted other networks This general problem network transfer encompasses relationships source target networks Our focus the utilization weights from source networks which solve a subproblem the target network task with speeding learning on We demonstrate the approach described improve learning speed up ten learning>
Paper 5:  <Title: Modeling the Student with Reinforcement Learning  .   Abstract: We describe a methodology enabling an intelligent teaching system make high level strategy decisions on low level student modeling information This framework less costly construct superior hand coding teaching strategies as responsive In order accomplish reinforcement learning learn associate superior teaching actions certain states the student's knowledge Reinforcement learning (RL flexible handling noisy data does need expert domain knowledge A dr>
Paper 6:  <Title: DISTRIBUTED GENETIC ALGORITHMS FOR PARTITIONING UNIFORM GRIDS  .   Abstract: The fault hierarchy representation widely expert systems the diagnosis complex mechanical devices On the assumption an appropriate bias a knowledge representation language also learning in a theory revision method operates directly a fault hierarchy This task presents several challenges: A typical training instance missing most feature values the pattern significant rather merely an effect noise Moreover the accuracy a candidate theory measure>
Paper 7:  <Title: Data Exploration with Reflective Adaptive Models  .   Abstract: Case-Based Planning provides scaling domain-independent planning solve It replaces the detailed and lengthy search a solution the retrieval adaptation previous planning experiences In general CBP demonstrated over generative (from- planning However the performance improvements it dependent adequate judgements as problem similarity In particular although CBP substantially planning effort overall subject a mis-retrieval problem T>
Paper 9:  <Title: Competitive Environments Evolve Better Solutions for Complex Tasks  .   Abstract: University Computer Technical Report 876September 1989 Abstract In explanation-based learning a specific problem's solution generalized into later Most research explanation-based learning involves relaxing constraints the variables a specific example rather the graphical structure itself However precludes the acquisition concepts where an iterative or recursive process implicitly represented the explanation by>
Paper 10:  <Title: The Utility of Knowledge in Inductive Learning  Running Head: Knowledge in Inductive Learning  .   Abstract: This paper investigates learning a lifelong context Lifelong learning addresses situations faces a whole stream Such scenarios provide transfer knowledge across multiple learning tasks more from less training data In several different approaches lifelong learning applied an object recognition domain It shown across lifelong learning approaches generalize consistently more from less training data by their ability trans>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Neural Networks

Probabilistic Methods
Prediction:  Probabilistic Methods
Is prediction correct?  False

Prediction: 0
Processing index 2431...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Multi-class problems and discretization in ICL Extended abstract  
Abstract: Abstract: Handling multi-class problems and real numbers is important in practical applications of machine learning to KDD problems. While attribute-value learners address these problems as a rule, very few ILP systems do so. The few ILP systems that handle real numbers mostly do so by trying out all real values that are applicable, thus running into efficiency or overfitting problems. This paper discusses some recent extensions of ICL that address these problems. ICL, which stands for Inductive Constraint Logic, is an ILP system that learns first order logic formulae from positive and negative examples. The main charateristic of ICL is its view on examples. These are seen as interpretations which are true or false for the clausal target theory (in CNF). We first argue that ICL can be used for learning a theory in a disjunctive normal form (DNF). With this in mind, a possible solution for handling more than two classes is given (based on some ideas from CN2). Finally, we show how to tackle problems with continuous values by adapting discretization techniques from attribute value learners. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 3:  <Title: Structural Regression Trees  .   Abstract: In many real domains the task machine learning algorithms a theory predicting numerical values In particular several standard test domains used Inductive Logic Programming concerned predicting numerical values examples relational and mostly non-determinate background knowledge However so far no ILP algorithm except can predict numbers cope non-determinate background knowledge (The only exception a covering algorithm FORS In present Structural Regression Trees>
Label: Rule Learning
Paper 2:  <Title: Inductive Constraint Logic and the Mutagenesis Problem  .   Abstract: A novel approach learning first order logic formulae incorporated a system named ICL In ICL examples viewed interpretations which true for the target theory present inductive logic programming systems true and false ground facts clauses Furthermore ICL uses a clausal representation corresponds a conjunctive normal form where forms a constraint positive examples classical learning techniques concentrated concept re>
Paper 4:  <Title: Inductive Constraint Logic  .   Abstract: A novel approach learning first order logic formulae from positive and negative examples Whereas present inductive logic programming systems employ examples as true and false ground factsor clauses we view examples interpretations which for the target theory This viewpoint allows to reconcile the inductive logic programming paradigm classical attribute value learning latter Because this property adapt AQ and CN2 type algorithms enable learning full>
Paper 5:  <Title: ILP with Noise and Fixed Example Size: A Bayesian Approach  .   Abstract: Current inductive logic programming systems limited their handling noise as employ a greedy covering approach constructing the hypothesis one clause This approach also causes difficulty learning recursive predicates Additionally many current systems an implicit expectation the cardinality reflect the concept the instance space A framework learning noisy data fixed example size is A Bayesian heuristic finding this genera>
Paper 6:  <Title: Symposium Title: Tutorial Discourse What Makes Human Explanations Effective?  .   Abstract: Many state ILP require large numbers negative examples avoid This a considerable disadvantage many ILP applications namely indu ctive program synthesis where relativelly small and sparse example sets a more realistic scenario Integrity constraints first order clauses play negative examples One integrity constraint replace ground negative examples However checking the consistency a program integrity constraints usually involves heavy the ore>
Paper 7:  <Title: Integrity Constraints in ILP using a Monte Carlo approach  .   Abstract: Many state ILP require large numbers negative examples avoid This a considerable disadvantage many ILP applications namely indu ctive program synthesis where relativelly small and sparse example sets a more realistic scenario Integrity constraints first order clauses play negative examples One integrity constraint replace ground negative examples However checking the consistency a program integrity constraints usually involves heavy the ore>
Paper 8:  <Title: The Difficulties of Learning Logic Programs with Cut  .   Abstract: As real logic programmers normally cut (! an effective learning procedure logic programs should able deal it Because the cut predicate only a procedural meaning clauses containing cut learned an extensional evaluation method most learning systems On searching a space possible programs (instead independent clauses An alternative solution generate first a candidate base program covers the positive examples make consistent inserting cut where appropriat>
Paper 9:  <Title: Lookahead and Discretization in ILP  .   Abstract: We present and evaluate two methods improving ILP systems One them discretization numerical attributes based Fayyad and Irani text [9 adapted and extended cope some aspects only occur relational learning problemswhen indeterminate literals occur The second technique lookahead It ILP a learner always assess a refinement without enabled afterwards without looking ahead the refinemen>
Paper 10:  <Title: Theory-Guided Induction of Logic Programs by Inference of Regular Languages recursive clauses. merlin on the.   Abstract: resent allowed sequences resolution steps for the initial theory There, many characterizations allowed sequences resolution steps expressed a set resolvents One approach presented, the system mer-lin, an earlier technique learning finite-state automata that represent allowed sequences resolution steps merlin extends the previous technique in i negative examples considered in addition a new strategy performing generalization techn>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Rule Learning

Probabilistic Methods
Prediction:  Probabilistic Methods
Is prediction correct?  False

Prediction: 0
Processing index 452...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Principal Curve Clustering With Noise  
Abstract: Abstract: Technical Report 317 Department of Statistics University of Washington. 1 Derek Stanford is Graduate Research Assistant and Adrian E. Raftery is Professor of Statistics and Sociology, both at the Department of Statistics, University of Washington, Box 354322, Seattle, WA 98195-4322, USA. E-mail: stanford@stat.washington.edu and raftery@stat.washington.edu. Web: http://www.stat.washington.edu/raftery. This research was supported by ONR grants N00014-96-1-0192 and N00014-96-1-0330. The authors are grateful to Simon Byers, Gilles Celeux and Christian Posse for helpful discussions. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 3:  <Title: Model Selection and Accounting for Model Uncertainty in Linear Regression Models  .   Abstract: 1 Adrian E. Raftery is Professor Statistics David Madigan is Jennifer Hoeting, the Department GN-22 Washington The research of Raftery Hoeting was supported ONR Contract N-00014-91-J-1074 Madigan's research was partially supported NSF no DMS 92111627. The authors are grateful Danika Lew for research assistance>
Label: Probabilistic Methods
Paper 5:  <Title: A Note on the Dirichlet Process Prior in Bayesian Nonparametric Inference with Partial Exchangeability 1  .   Abstract: Technical Report no 297 Department Statistics University Washington 1 Sonia Petrone is Universita di Pavia Dipartimento Economia Politica e Metodi Quantitativi I-27100 Pavia and Adrian E. Raftery is Professor Statistics Department Washington Box 354322 This research ONR grant no N-00014-91-J-1074 and by grants MURST, Rome.>
Label: Probabilistic Methods
Paper 2:  <Title: Detecting Features in Spatial Point Processes with Clutter via Model-Based Clustering  .   Abstract: Technical Report No. 295 Department Statistics University Washington October, 1995 1 Abhijit Dasgupta is a graduate student Biostatistics Box 357232 981957232 and his e-mail address Adrian E. Raftery is Professor Statistics Sociology, Department Statistics Washington Box 354322 981954322 and his e-mail address This research Office Naval Research Grant no N-00014-91-J-1074. The authors ar>
Paper 4:  <Title: Covariate Selection in Hierarchical Models of Hospital Admission Counts: A Bayes Factor Approach 1  .   Abstract: TECHNICAL REPORT No. 268 Department Statistics GN-22 University Washington Seattle 98195 USA 1 Susan L. Rosenkranz is Pew Health Policy Postdoctoral Fellow Box 0936 University 94143 and Adrian E. Raftery is Professor Statistics Sociology, Department GN-22 Washington Rosenkranz's research was supported the National Research Service Award 5T32CA 09168-17 from The authors are grateful>
Paper 6:  <Title: Change Point and Change Curve Modeling in Stochastic Processes and Spatial Statistics  .   Abstract: 1 This article will appear Volume no 4 (1994 of Journal Applied Statistical Science Adrian E. Raftery Professor Statistics GN-22 Washington This research ONR contract no N-00014-91-J-1074, by NIH Grant no 5R01HD26330-02, by by the Universite de Paris VI and INRIA Rocquencourt France Raftery thanks latter two institutions Paul Deheuvels Gilles Celeux hearty hospitality his Paris sabbatical wh>
Paper 7:  <Title: Estimating Dependency Structure as a Hidden Variable  .   Abstract: This paper introduces a probability model the mixture trees account sparse, dynamically changing dependence relationships We present a family efficient algorithms EM and the Minimum Spanning Tree find the ML and MAP mixture trees priors the MDL priors This report research done the Dept of the Dept of Brain and the Center the Ma>
Paper 8:  <Title: FLEXIBLE PARAMETRIC MEASUREMENT ERROR MODELS  .   Abstract: Inferences in measurement error models sensitive modeling assumptions Specifically the model incorrect the estimates can inconsistent To reduce sensitivity modeling assumptions yet the efficiency parametric inference we propose which accommodate departures We use mixtures normals We study two cases detail linear errors a change-point Berkson model fl Raymond J. Carroll Professor Statistics Nutrition Toxi>
Paper 9:  <Title: The Weighted Majority Algorithm  .   Abstract: fl This research primarily while this author Calif. at Santa Cruz with support ONR grant N0001486-K-0454 and at Harvard University supported DARPA AFOSR-890506 Current address NEC Research Institute 4 Independence Way Princeton E-mail address nickl@research.nj.nec.com y Supported ONR grants N0001486-K-045491-J-1162 Part this research while this author Aiken Computation Laboratory Harvard, with partial support the ONR grants N>
Paper 10:  <Title: Sequential Importance Sampling for Nonparametric Bayes Models: The Next Generation Running Title: SIS for Nonparametric Bayes  .   Abstract: There two generations Gibbs sampling semi-parametric models involving The first generation suffered a severe drawback; namely the locations the clusters, groups parameters essentially fixed moving rarely Two strategies create the second generation Gibbs samplers integration appending a second stage wherein the cluster locations moved We show these same strategies easily the sequential importance sampler th>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Probabilistic Methods

Probabilistic Methods
Prediction:  Probabilistic Methods
Is prediction correct?  True

Prediction: 1
Processing index 1516...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Solving 3-SAT by GAs Adapting Constraint Weights  
Abstract: Abstract: Handling NP complete problems with GAs is a great challenge. In particular the presence of constraints makes finding solutions hard for a GA. In this paper we present a problem independent constraint handling mechanism, Stepwise Adaptation of Weights (SAW), and apply it for solving the 3-SAT problem. Our experiments prove that the SAW mechanism substantially increases GA performance. Furthermore, we compare our SAW-ing GA with the best heuristic technique we could trace, WGSAT, and conclude that the GA is superior to the heuristic method. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 3:  <Title: An Evolutionary Approach to Time Constrained Routing Problems  .   Abstract: Routing problems an important class planning problems Usually there many different constraints optimization criteria involved find general methods solving routing problems We propose an evolutionary solver such planning problems An instance this solver has tested a specific routing problem time constraints The performance this evolutionary solver compared a biased random solver Results show the evolutionary solver performs significantly>
Label: Genetic Algorithms
Paper 4:  <Title: A GENETIC ALGORITHM FOR FRAGMENT ALLOCATION IN A DISTRIBUTED DATABASE SYSTEM  .   Abstract: In explore the distributed database allocation problem intractable. We also discuss genetic algorithms have successfully Our experimental results the GA to far the greedy heuristic obtaining optimal and near optimal fragment placements the allocation problem with various data sets>
Label: Genetic Algorithms
Paper 5:  <Title: High-Performance Job-Shop Scheduling With A Time-Delay TD() Network  .   Abstract: Job-shop scheduling manufacturing industries We interested the particular task scheduling payload processing This paper summarizes formulating this task solution by the reinforcement algorithm T D(). A shortcoming this previous work hand-engineered input features This paper extendTDNN) architecture apply it irregular-length schedules Experimental tests this TDNN-T D() network match the perform>
Label: Reinforcement Learning
Paper 6:  <Title: Bibliography "SMART: Support Management Automated Reasoning Technology for COMPAQ Customer Service," "Instance-Based Learning Algorithms," Machine.   Abstract: Satisfiability (SAT refers the task finding a truth assignment makes an arbitrary boolean expression This paper compares GSATSelman 1992 solving GSAT can solve problem instances extremely traditional satisfiability algorithms Results suggest SASAT scales up better as solving at least as many hard SAT problems with The paper then presents an ablation study helps the relati>
Label: Theory
Paper 2:  <Title: Graph Coloring with Adaptive Evolutionary Algorithms  .   Abstract: This paper solving graph coloringEA After testing different algorithm variants we conclude an asexual EA order-based representation an adaptation mechanism periodically during This adaptive EA general using no domain specific knowledge except, from (fitness function We compare this adaptive EA a powerful traditional graph coloring technique DSatur and the Grouping GA on>
Paper 7:  <Title: How good are genetic algorithms at finding large cliques: an experimental study  .   Abstract: This paper genetic algorithms at solving the MAX-CLIQUE problem We measure a standard genetic algorithm an elementary set problem instances consisting embedded cliques random graphs We indicate improvement introduce the multi-phase annealed GA exhibits As scale the problem size test on "hard" benchmark instances notice a degraded performance the algorithm caused premature convergence local minima To alleviat>
Paper 8:  <Title: A Genetic Local Search Approach to the Quadratic Assignment Problem  .   Abstract: Augmenting genetic algorithms local search heuristics the solution In a genetic local search approach the quadratic assignment problemQAP New genetic operators realizing the approach described, its performance tested various QAP instances containing between 30 and 256 facilities/locations The results indicate the proposed algorithm able arrive high quality solutions: for the largest publicly known prob lem instance>
Paper 9:  <Title: A Genetic Algorithm for File and Task Placement in a Distributed System  .   Abstract: In explore the distributed file and task placement problem We also discuss genetic algorithms have successfully Our experimental results the GA to far the greedy heuristic obtaining optimal and near optimal file and task placements the problem with various data sets>
Paper 10:  <Title: Simulated Annealing for Hard Satisfiability Problems  .   Abstract: Satisfiability (SAT refers the task finding a truth assignment makes an arbitrary boolean expression This paper compares GSATSelman 1992 solving GSAT can solve problem instances extremely traditional satisfiability algorithms Results suggest SASAT scales up better as solving at least as many hard SAT problems with The paper then presents an ablation study helps the relati>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Genetic Algorithms

Genetic Algorithms
Prediction:  Genetic Algorithms
Is prediction correct?  True

Prediction: 1
Processing index 2298...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Convergence Analysis of Canonical Genetic Algorithms  
Abstract: Abstract: This paper analyzes the convergence properties of the canonical genetic algorithm (CGA) with mutation, crossover and proportional reproduction applied to static optimization problems. It is proved by means of homogeneous finite Markov chain analysis that a CGA will never converge to the global optimum regardless of the initialization, crossover operator and objective function. But variants of CGAs that always maintain the best solution in the population, either before or after selection, are shown to converge to the global optimum due to the irreducibility property of the underlying original nonconvergent CGA. These results are discussed with respect to the schema theorem.
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 3:  <Title: An Analysis of the MAX Problem in Genetic Programming hold only in some cases, in.   Abstract: We present genetic programmingGP populations the problem finding a program returns and function set a depth limitknown the MAX problem We confirm the basic message [ Gathercole and Ross 1996 crossover together program size restrictions responsible premature convergence to We show this can retains variety show in evolution from t>
Label: Genetic Algorithms
Paper 5:  <Title: Effects of Occam's Razor in Evolving Sigma-Pi Neural Nets  .   Abstract: Several evolutionary algorithms make hierarchical representations variable size rather linear strings Variable complexity the structures provides an additional representational power which widen evolutionary algorithms The price, however that the search space open- solutions grow arbitrarily large size In study structural complexity the solutions their generalization performance analyzing the fitness landscape sigma-pi neural networks The analys>
Label: Genetic Algorithms
Paper 2:  <Title: Graph Coloring with Adaptive Evolutionary Algorithms  .   Abstract: This paper solving graph coloringEA After testing different algorithm variants we conclude an asexual EA order-based representation an adaptation mechanism periodically during This adaptive EA general using no domain specific knowledge except, from (fitness function We compare this adaptive EA a powerful traditional graph coloring technique DSatur and the Grouping GA on>
Paper 4:  <Title: The Power of Self-Directed Learning  .   Abstract: A lower-bound result on the power Abstract This paper a genetic algorithm in We describe a new genetic algorithm the merged genetic algorithm prove for the class monotonic functions finds does an exponential convergence rate The analysis pertains the ideal behavior the algorithm where the main task reduces showing convergence probability distributions combinatorial structures to t>
Paper 6:  <Title: MLC Tutorial A Machine Learning library of C classes.  .   Abstract: Several evolutionary algorithms make hierarchical representations variable size rather linear strings Variable complexity the structures provides an additional representational power which widen evolutionary algorithms The price, however that the search space open- solutions grow arbitrarily large size In study structural complexity the solutions their generalization performance analyzing the fitness landscape sigma-pi neural networks The analys>
Paper 7:  <Title: Vector Quantizer Design Using Genetic Algorithms  .   Abstract: A Genetic ( approach vector quantizer design combines the conventional Generalized Lloyd Algorithm [6 is presented We refer this hybrid the Genetic Generalized Lloyd Algorithm It works briefly A finite number codebooks called chromosomes selected Each codebook undergoes iterative cycles reproduction We perform experiments various alternative design choices using Gaussian-Markov processes speech image as source data signal the performance measure In most cases GGLA showe>
Paper 8:  <Title: Reformulation: Nonsmooth, Piecewise Smooth, Semismooth and Smoothing Methods, A Globally Convergent Inexact Newton Method for.   Abstract: We propose systems combines Newton, proximal point projection methodologies An important property the algorithm iterates globally a solution without any additional regularity assumptions Moreover under standard assumptions the local su-perlinear rate convergence achieved As opposed classical globalization strategies Newton methods for computing the stepsize we do line-search aimed decreasing some merit function Ins>
Paper 9:  <Title: Genetic Programming and Redundancy  .   Abstract: The Genetic Programming optimization methodGP elaborated John Koza [ Koza 1992 The search space the problem domain consists computer programs represented parse trees the crossover operator realized an exchange subtrees Empirical analyses show large parts those trees never used or evaluated which these parts irrelevant the solution or redundant This paper concerned redundancy occuring GP. It starts a mathematical description>
Paper 10:  <Title: An Evolutionary Approach to Combinatorial Optimization Problems  .   Abstract:  paper reports the application genetic algorithms based the model organic evolution NP-complete combinatorial optimization problems In particular the subset sum, maximum cut minimum tardy task problems considered Except the fitness function no problem-specific changes of the genetic algorithm results of high quality even the problem instances size 100 used For constrained problems the subset sum and the minimum tardy task the constraints taken>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Genetic Algorithms

Genetic Algorithms
Prediction:  Genetic Algorithms
Is prediction correct?  True

Prediction: 1
Processing index 2459...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Control of Selective Visual Attention: Modeling the "Where" Pathway  
Abstract: Abstract: Intermediate and higher vision processes require selection of a subset of the available sensory information before further processing. Usually, this selection is implemented in the form of a spatially circumscribed region of the visual field, the so-called "focus of attention" which scans the visual scene dependent on the input and on the attentional state of the subject. We here present a model for the control of the focus of attention in primates, based on a saliency map. This mechanism is not only expected to model the functionality of biological vision but also to be essential for the understanding of complex scenes in machine vision.
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 3:  <Title: Expectation-Based Selective Attention for Visual Monitoring and Control of a Robot Vehicle  .   Abstract: Reliable vision-based control requires focus an input scene Previous work with an autonomous lane following system, ALVINN [Pomerleau 1993 yielded uncluttered conditions This paper based learning approach handling difficult scenes which will confuse the ALVINN system This work presents a mechanism achieving task-specific focus exploiting A saliency map a computed expectation t>
Label: Neural Networks
Paper 4:  <Title: Optimising Local Hebbian Learning: use the ffi-rule  .   Abstract: Many the lower-level areas the mammalian visual system organized retinotopically that as maps which preserve to A unit that a part such a retinotopic map normally responds selectively stimulation a well-delimited part referred its receptive fieldRF Receptive fields probably the most prominent and ubiquitous computational mechanism employed This paper surveys some the possible computational reasons RFs by>
Label: Neural Networks
Paper 5:  <Title: Hidden Markov Modeling of simultaneously recorded cells in the Associative cortex of behaving monkeys  .   Abstract: A widely held idea regarding information processing the cell-assembly hypothesis suggested Hebb in 1949 According this hypothesis the basic unit information processing an assembly cells briefly in This work presents this supposed activity using a Hidden Markov Model This model able reveal some the underlying cortical network activity behavioral processes In our study the process in hand the simultaneous activity severa>
Label: Neural Networks
Paper 10:  <Title: Combining Neural Network Forecasts on Wavelet-Transformed Time Series  .   Abstract: Many the lower-level areas the mammalian visual system organized retinotopically that as maps which preserve to A unit that a part such a retinotopic map normally responds selectively stimulation a well-delimited part referred its receptive fieldRF Receptive fields probably the most prominent and ubiquitous computational mechanism employed This paper surveys some the possible computational reasons RFs by>
Label: Neural Networks
Paper 2:  <Title: Computational Models of Sensorimotor Integration  Computational Maps and Motor Control.  .   Abstract: The sensorimotor integration system can viewed an observer attempting estimate its own state and by integrating multiple sources We describe a computational framework capturing this notion some specific models integration adaptation result Psychophysical results two sensorimotor systems subserving the integration adaptation visuo-auditory maps estimation the state the hand arm movements and within this framework These results: Spatia>
Paper 6:  <Title: CNN: a Neural Architecture that Learns Multiple Transformations of Spatial Representations  .   Abstract: Many the lower-level areas the mammalian visual system organized retinotopically that as maps which preserve to A unit that a part such a retinotopic map normally responds selectively stimulation a well-delimited part referred its receptive fieldRF Receptive fields probably the most prominent and ubiquitous computational mechanism employed This paper surveys some the possible computational reasons RFs by>
Paper 7:  <Title: Residual Q-Learning Applied to Visual Attention  .   Abstract: Foveal vision features imagers graded acuity coupled context sensitive sensor gaze control analogous prevalent throughout Foveal vision operates more uniform acuity vision resolution treated a dynamically allocatable resource but requires a more refined visual attention mechanism We demonstrate reinforcementRL significantly foveal visual attention of the overall vision system model based target recognition A simulated foveal vision system>
Paper 8:  <Title: Cortical Mechanisms of Visual Recognition and Learning: A Hierarchical Kalman Filter Model  .   Abstract: We describe dynamic recognition based the statistical theory Kalman filtering from optimal control theory The model utilizes a hierarchical network whose successive levels implement Kalman filters operating over successively spatial Each hierarchical level predicts the current visual recognition state adapts using the residual error between>
Paper 9:  <Title: A Neural Network Model of Visual Tilt Aftereffects  .   Abstract: RF-LISSOM, a self-organizing model laterally connected orientation maps in the psychological phenomenon known the tilt aftereffect The same self-organizing processes are the map its lateral connections shown result tilt aftereffects over the adult. The model allows observing large numbers neurons connections simultaneously making relate higher-level phenomena which difficult experimentally The re>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Neural Networks

Neural Networks
Prediction:  Neural Networks
Is prediction correct?  True

Prediction: 1
Processing index 1243...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: BLIND SEPARATION OF REAL WORLD AUDIO SIGNALS USING OVERDETERMINED MIXTURES  
Abstract: Abstract: We discuss the advantages of using overdetermined mixtures to improve upon blind source separation algorithms that are designed to extract sound sources from acoustic mixtures. A study of the nature of room impulse responses helps us choose an adaptive filter architecture. We use ideal inverses of acquired room impulse responses to compare the effectiveness of different-sized separating filter configurations of various filter lengths. Using a multi-channel blind least-mean-square algorithm (MBLMS), we show that, by adding additional sensors, we can improve upon the separation of signals mixed with real world filters. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 5:  <Title: Recognizing Handwritten Digits Using Mixtures of Linear Models  .   Abstract: We construct a mixture locally linear generative models a collection pixel-based images digits use recognition Different models a given digit capture different styles new images classified evaluating under We use an EM-based algorithm in the M-step computationally straightforward principal components analysis tangent-plane information [12 about expected local deformations only requires adding into for the PCA>
Label: Neural Networks
Paper 9:  <Title: Simple Neuron Models for Independent Component Analysis  .   Abstract: Recently several neural algorithms introduced Independent Component Analysis Here approach a single neuron First simple Hebbian-like learning rules introduced estimating one of the independent components from sphered data Some the learning rules can used estimate an independent component which has the others estimate Next a two-unit system estimate an independent component any kurtosis The results then generalized estimat>
Label: Neural Networks
Paper 2:  <Title: Blind separation of delayed and convolved sources.  .   Abstract: We address separating multiple speakers We combine Torkkola Amari Cichocki Yang Natural Gradient information maximisation rules recurrent (IIR) networks blindly adjusting delays separating mixed signals While they work well simulated data these rules fail real rooms which usually non-minimum phase transfer functions not- using stable IIR filters An approach perform infomax a feedforward architecture>
Paper 3:  <Title: BLIND SEPARATION OF DELAYED SOURCES BASED ON INFORMATION MAXIMIZATION  .   Abstract: Recently Bell and Sejnowski presented blind source separation the information maximization principle We extend this approach into the sources may delayed with each We present a network architecture capable coping such sources derive the adaptation equations the delays the weights maximizing transferred Examples using wideband sources speech are presented the algorithm>
Paper 4:  <Title: Analyzing Hyperspectral Data with Independent Component Analysis  .   Abstract: Hyperspectral image sensors provide images contiguous spectral channels per enable information different materials within obtained The problem spectrally unmixing materials may viewed a specific case the blind source separation problem where data consists mixed signals (in minerals the goal the contribution each mineral without prior knowledge The technique Independent Component AnalysisICA assumes the spectral components close t>
Paper 6:  <Title: A Context-Sensitive Generalization of ICA  .   Abstract: Source separation arises a surprising number signal processing applications EEG analysis In the square linear blind source separation problem without time delays one find an unmixing matrix detangle mixing n unknown independent sources through an unknown n fi n mixing matrix The recently introduced ICA blind source separation algorithm (Baram and Roth 1994 Bell solving ICA all the performing despite mak>
Paper 7:  <Title: Independent Component Analysis of Electroencephalographic Data  .   Abstract: Because the distance the skull brain their different resistivities collected any point includes activity generated within This spatial smearing EEG data volume conduction does involve significant time delays however suggesting Independent ComponentICA algorithm of Bell1 suitable performing blind source separation on The ICA algorithm separates the problem source identification source localization First r>
Paper 8:  <Title: A Blind Identification and Separation Technique via Multi-layer Neural Networks  .   Abstract: This paper deals blind identification source separation which consists estimation the mixing matrix stochastically independent sources without on. The method we propose estimates the mixture matrix by recurrent InputIO Identification using as inputs the estimated sources Herein, the nonlinear transformationdistortion consists the inputs the IO-Identification device contrast t>
Paper 10:  <Title: Finding Overlapping Distributions with MML  .   Abstract: This paper considers an aspect mixture modelling. Significantly overlapping distributions require their parameters accurately than well separated distributions For example two Gaussian distributions considered significantly overlap their means within If insufficient data only a single component distribution estimated although originates We consider how much data distinguish two component distributions one dis>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Neural Networks

Neural Networks
Prediction:  Neural Networks
Is prediction correct?  True

Prediction: 1
Processing index 1152...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Fish and Shrink. A next step towards e-cient case retrieval in large scaled case bases  
Abstract: Abstract: Keywords: Case-Based Reasoning, case retrieval, case representation This paper deals with the retrieval of useful cases in case-based reasoning. It focuses on the questions of what "useful" could mean and how the search for useful cases can be organized. We present the new search algorithm Fish and Shrink that is able to search quickly through the case base, even if the aspects that deflne usefulness are spontaneously combined at query time. We compare Fish and Shrink to other algorithms and show that most of them make an implicit closed world assumption. We flnally refer to a realization of the presented idea in the context of the prototype of the FABEL-Project 1 . The scenery is as follows. Previously collected cases are stored in a large scaled case base. An expert describes his problem and gives the aspects in which the requested case should be similar. The similarity measure thus given spontaneously shall now be used to explore the case base within a short time, shall present a required number of cases and make sure that none of the other cases is more similar. The question is now how to prepare the previously collected cases and how to deflne a retrieval algorithm which is able to deal with sponta neously user-deflned similarity measures.
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 2:  <Title: Case Retrieval Nets: Basic Ideas and Extensions  .   Abstract: An efficient retrieval relevant cases a huge case base a crucial subtask Case-Based Reasoning In present Case Retrieval Nets a memory model has recently this task The main idea apply a spreading activation process a net-like case memory in retrieve cases being similar a posed query case We summarize the basic ideas CRNs suggest some useful extensions present some initial experimental results which successfully handle case bases larger>
Label: Case Based
Paper 3:  <Title: Context-Based Similarity Applied to Retrieval of Relevant Cases  .   Abstract: Retrieving relevant cases a crucial component case-based reasoning systems The task use user-defined query to useful information which close query-defined request according certain measures The difficulty stems it may ( it specify query requests precisely completely resulting a situation known a fuzzy-querying It usually small domains a large repositories which store various information (multifunctional informati>
Label: Case Based
Paper 5:  <Title: On the Usefulness of Re-using Diagnostic Solutions  .   Abstract: Recent studies on planning comparing plan reuse plan generation both the above tasks may computational complexity even we deal very similar problems The aim the same kind results apply also diagnosis. We propose a theoretical complexity analysis coupled some experimental tests intended evaluate adaptation strategies which reuse the solutions past diagnostic problems build a solution to Results such analysis even>
Label: Case Based
Paper 6:  <Title: ADAPtER: an Integrated Diagnostic System Combining Case-Based and Abductive Reasoning  .   Abstract: The aim the ADAPtER system a diagnostic architecture combining case-based reasoning abductive reasoning exploiting the adaptation the solution old episodes focus the reasoning process Domain knowledge represented via a logical model basic mechanisms based abductive reasoning with consistency constraints have defined solving complex diagnostic problems involving multiple faults The model-based component supplemented a case memory adaptation mechanisms have mak>
Label: Case Based
Paper 4:  <Title: Growing a Hypercubical Output Space in a Self-Organizing Feature Map  .   Abstract: Recent studies on planning comparing plan reuse plan generation both the above tasks may computational complexity even we deal very similar problems The aim the same kind results apply also diagnosis. We propose a theoretical complexity analysis coupled some experimental tests intended evaluate adaptation strategies which reuse the solutions past diagnostic problems build a solution to Results such analysis even>
Paper 7:  <Title: Lazy Induction Triggered by CBR  .   Abstract: In case-based reasoning demonstrated problem complex domains Also mixed paradigm approaches emerged combining CBR and induction techniques aiming verifying the knowledge building an efficient case memory However in complex domains induction over the whole problem space or too time In this paper an approach which (owing a close interaction the CBR part attempts induce rules only a particular context a problem just beingor>
Paper 8:  <Title: REPRO: Supporting Flowsheet Design by Case-Base Retrieval  .   Abstract:  very close the designer behavior during, seems a fruitable computer aided-design approach if a library The goal presents: REPRO, that supports chemical process design The crucial problems like the case representation structural similarity measure widely described The presented experimental results the expert evaluation shows usefulness the described system The papers>
Paper 9:  <Title: A Preprocessing Model for Integrating CBR and Prototype-Based Neural Networks  .   Abstract: Some important factors play the performances a CBR (Case-Based Reasoning) system the complexity the accuracy the retrieval phase Both flat memory inductive approaches suffer serious drawbacks In the first approach search time increases when dealing large scale memory base the modification of the case memory becomes complex because its sophisticated architecture In this paper construct a simple efficient indexing system structure The idea construct a case hierar>
Paper 10:  <Title: Adaptive Tuning of Numerical Weather Prediction Models: Simultaneous Estimation of Weighting, Smoothing and Physical Parameters 1  .   Abstract: In case-based reasoning demonstrated problem complex domains Also mixed paradigm approaches emerged combining CBR and induction techniques aiming verifying the knowledge building an efficient case memory However in complex domains induction over the whole problem space or too time In this paper an approach which (owing a close interaction the CBR part attempts induce rules only a particular context a problem just beingor>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Case Based

Category: Case Based
Prediction:  Category: Case Based
Is prediction correct?  False

Prediction: 0
Processing index 1020...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Error-Based and Entropy-Based Discretization of Continuous Features  
Abstract: Abstract: We present a comparison of error-based and entropy-based methods for discretization of continuous features. Our study includes both an extensive empirical comparison as well as an analysis of scenarios where error minimization may be an inappropriate discretization criterion. We present a discretization method based on the C4.5 decision tree algorithm and compare it to an existing entropy-based discretization algorithm, which employs the Minimum Description Length Principle, and a recently proposed error-based technique. We evaluate these discretization methods with respect to C4.5 and Naive-Bayesian classifiers on datasets from the UCI repository and analyze the computational complexity of each method. Our results indicate that the entropy-based MDL heuristic outperforms error minimization on average. We then analyze the shortcomings of error-based approaches in comparison to entropy-based methods. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 2:  <Title: Supervised and Unsupervised Discretization of Continuous Features  .   Abstract: Many supervised machine learning algorithms require a discrete feature space In review continuous feature discretization identify defining characteristics the methods conduct several methods We compare binning, entropy-based and purity-based methods supervised algorithms We found the performance the Naive-Bayes algorithm significantly features discretized using In fact over the 16 tested datasets discreti>
Label: Theory
Paper 4:  <Title: Efficient Learning of Selective Bayesian Network Classifiers  .   Abstract: In a computation-ally efficient method inducing selective Bayesian network classifiers Our approach information-theoretic metrics efficiently select attributes learn the classifier We explore three conditional, information-theoretic met-rics extensions metrics extensively decision tree learning namely Quin-lan's gain gain ratio metrics Man-taras's distance metric We experimentally show the algorithms based gain ratio distance metric learn selective Bayesian networks>
Label: Probabilistic Methods
Paper 6:  <Title: Cost-Sensitive Classification: Empirical Evaluation of a Hybrid Genetic Decision Tree Induction Algorithm  .   Abstract: This paper introduces ICET algorithm costsensitive classification ICET uses evolve biases a decision tree induction algorithm The fitness function the genetic algorithm the average cost classification when using including testsfeatures measurements classification errors ICET is compared here three other algorithms costsensitive classification EG2, CS-ID3 IDX and also C4.5 classifies without regard The five algorithms evalua>
Label: Genetic Algorithms
Paper 3:  <Title: Evolutionary Design of Neural Architectures A Preliminary Taxonomy and Guide to Literature  .   Abstract: In a computation-ally efficient method inducing selective Bayesian network classifiers Our approach information-theoretic metrics efficiently select attributes learn the classifier We explore three conditional, information-theoretic met-rics extensions metrics extensively decision tree learning namely Quin-lan's gain gain ratio metrics Man-taras's distance metric We experimentally show the algorithms based gain ratio distance metric learn selective Bayesian networks>
Paper 5:  <Title: Search-based Class Discretization  .   Abstract: We present a methodology enables classification algorithms regression tasks We implement system RECLA that transforms a regression problem classification one and an existent classification system The transformation consists mapping a continuous variable grouping intervals We use misclassification costs reflect the implicit ordering among the new variable We describe alternative discretization>
Paper 7:  <Title: Pruning Decision Trees with Misclassification Costs  .   Abstract: We describe pruning methods decision tree classifiers when minimizing loss rather error In two common methods error minimization CART's cost-complexity pruning study the extension loss and one pruning variant based We perform these methods evaluate loss. We found applying the Laplace correction estimate at the leaves beneficial all pru>
Paper 8:  <Title: Understanding Musical Sound with Forward Models and Physical Models  .   Abstract: This paper introduces ICET algorithm costsensitive classification ICET uses evolve biases a decision tree induction algorithm The fitness function the genetic algorithm the average cost classification when using including testsfeatures measurements classification errors ICET is compared here three other algorithms costsensitive classification EG2, CS-ID3 IDX and also C4.5 classifies without regard The five algorithms evalua>
Paper 9:  <Title: Towards a Better Understanding of Memory-Based Reasoning Systems  .   Abstract: We quantify both experimentally memory-based reasoning To start gaining insight the capabilities MBR algorithms compare a value difference metric These two approaches similar make certain independence assumptions However whereas MBR uses specific cases perform classification summarize We demonstrate a particular MBR system called Pebls works comparatively a wide r>
Paper 10:  <Title: Prototype and Feature Selection by Sampling and Random Mutation Hill Climbing Algorithms  .   Abstract: With computational costs without accuracy describe two algorithms find sets prototypes nearest neighbor classification Here the term prototypes the reference instances a nearest neighbor computation the instances with respect which similarity assessed assign a new data item Both algorithms rely search sets prototypes are simple first is a Monte Carlo sampling algorithm; applies random mutation hill climbing On f>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Probabilistic Methods

Probabilistic Methods
Prediction:  Probabilistic Methods
Is prediction correct?  True

Prediction: 1
Processing index 975...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: State Reconstruction for Determining Predictability in Driven Nonlinear Acoustical Systems  
Abstract: Abstract: Genetic programming is distinguished from other evolutionary algorithms in that it uses tree representations of variable size instead of linear strings of fixed length. The flexible representation scheme is very important because it allows the underlying structure of the data to be discovered automatically. One primary difficulty, however, is that the solutions may grow too big without any improvement of their generalization ability. In this paper we investigate the fundamental relationship between the performance and complexity of the evolved structures. The essence of the parsimony problem is demonstrated empirically by analyzing error landscapes of programs evolved for neural network synthesis. We consider genetic programming as a statistical inference problem and apply the Bayesian model-comparison framework to introduce a class of fitness functions with error and complexity terms. An adaptive learning method is then presented that automatically balances the model-complexity factor to evolve parsimonious programs without losing the diversity of the population needed for achieving the desired training accuracy. The effectiveness of this approach is empirically shown on the induction of sigma-pi neural networks for solving a real-world medical diagnosis problem as well as benchmark tasks. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 4:  <Title: Effects of Occam's Razor in Evolving Sigma-Pi Neural Nets  .   Abstract: Several evolutionary algorithms make hierarchical representations variable size rather linear strings Variable complexity the structures provides an additional representational power which widen evolutionary algorithms The price, however that the search space open- solutions grow arbitrarily large size In study structural complexity the solutions their generalization performance analyzing the fitness landscape sigma-pi neural networks The analys>
Label: Genetic Algorithms
Paper 10:  <Title: Hierarchical Self-Organization in Genetic Programming  .   Abstract: This paper automatic discovery functions Genetic Programming The approach discovery useful building blocks analyzing the evolution trace generalizing blocks define finally adapting the problem representation on- Adaptating the representation determines a hierarchical organization extended function set which enables a restructuring so solutions Measures complexity solution trees for an adaptive representation framework The minimum>
Label: Genetic Algorithms
Paper 2:  <Title: Balancing Accuracy and Parsimony in Genetic Programming 1  .   Abstract: Genetic programming distinguished other evolutionary algorithms tree representations variable size instead linear strings The flexible representation scheme very the underlying structure discovered automatically One primary difficulty, the solutions may grow too without any improvement of their generalization ability In this paper investigate the fundamental relationship the performance complexity the evolved structures. The essence the parsimony>
Paper 3:  <Title: MLC Tutorial A Machine Learning library of C classes.  .   Abstract: Several evolutionary algorithms make hierarchical representations variable size rather linear strings Variable complexity the structures provides an additional representational power which widen evolutionary algorithms The price, however that the search space open- solutions grow arbitrarily large size In study structural complexity the solutions their generalization performance analyzing the fitness landscape sigma-pi neural networks The analys>
Paper 5:  <Title: Spline Smoothing For Bivariate Data With Applications To Association Between Hormones  .   Abstract: Standard methods inducing both the structure weight values recurrent neural networks fit an assumed class architectures to every task This simplification necessary the interactions network structure function Evolutionary computation includes genetic algorithms evolutionary programming a population-based search method shown such complex tasks This paper argues genetic algorithms inappropriate network acquisition describes an evolutionary program, GNARL simul>
Paper 6:  <Title: Evolving Optimal Neural Networks Using Genetic Algorithms with Occam's Razor  .   Abstract: Genetic algorithms neural networks two main ways optimize the network architecture train the weights a fixed architecture While most previous work focuses only of investigates an alternative evolutionary approach called Breeder Genetic Programming the architecture the weights optimized simultaneously The genotype each network represented whose depth and dynamically adapted the particular application by specifically defined genetic operators The weights t>
Paper 7:  <Title: An Evolutionary Algorithm that Constructs Recurrent Neural Networks  .   Abstract: Standard methods inducing both the structure weight values recurrent neural networks fit an assumed class architectures to every task This simplification necessary the interactions network structure function Evolutionary computation includes genetic algorithms evolutionary programming a population-based search method shown such complex tasks This paper argues genetic algorithms inappropriate network acquisition describes an evolutionary program, GNARL simul>
Paper 8:  <Title: A Sampling-Based Heuristic for Tree Search Applied to Grammar Induction  .   Abstract: In the field Operation Research and Artificial Intelligence several stochastic search algorithms designed based global random searchZhigljavsky 1991 Basically those techniques iteratively sample the search space with respect a probability distribution which updated according previous samples some predefined strategy Genetic Algorithms ( (Goldberg 1989 or Greedy Randomized Adaptive Search Procedures (Feo Resende two particular instances this paradigm In SAGE al>
Paper 9:  <Title: Genetic Programming of Minimal Neural Nets Using Occam's Razor  .   Abstract: A genetic programming method investigated optimizing both the architecture the connection weights The genotype each network whose depth dynamically adapted the particular application by specifically defined genetic operators The weights trained a next-ascent hillclimb-ing search A new fitness function proposed quantifies the principle It makes an optimal trade-off the error fitting ability the parsimony the network We discuss the result>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Neural Networks

Genetic Algorithms
Prediction:  Genetic Algorithms
Is prediction correct?  False

Prediction: 0
Processing index 488...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Prediction, Learning, Uniform Convergence, and Scale-sensitive Dimensions  
Abstract: Abstract: We present a new general-purpose algorithm for learning classes of [0; 1]-valued functions in a generalization of the prediction model, and prove a general upper bound on the expected absolute error of this algorithm in terms of a scale-sensitive generalization of the Vapnik dimension proposed by Alon, Ben-David, Cesa-Bianchi and Haussler. We give lower bounds implying that our upper bounds cannot be improved by more than a constant factor in general. We apply this result, together with techniques due to Haussler and to Benedek and Itai, to obtain new upper bounds on packing numbers in terms of this scale-sensitive notion of dimension. Using a different technique, we obtain new bounds on packing numbers in terms of Kearns and Schapire's fat-shattering function. We show how to apply both packing bounds to obtain improved general bounds on the sample complexity of agnostic learning. For each * &gt; 0, we establish weaker sufficient and stronger necessary conditions for a class of [0; 1]-valued functions to be agnostically learnable to within *, and to be an *-uniform Glivenko-Cantelli class. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 4:  <Title: Boosting a weak learning algorithm by majority To be published in Information and Computation  .   Abstract: We present an algorithm improving algorithms learning binary concepts The improvement achieved combining hypotheses each generated training given learning examples Our algorithm ideas presented Schapire inThe strength weak learnability represents an improvement his results The analysis our algorithm provides general upper bounds the resources required learning in Valiant's polynomial PAC learning framework are the best genera>
Label: Theory
Paper 7:  <Title: Noise-Tolerant Parallel Learning of Geometric Concepts  .   Abstract: We present several efficient parallel algorithms PAC-learning geometric concepts that robust even malicious misclassification noise of any rate less 1=2 In particular we consider the class geometric concepts defined (d 1)-dimensional hyperplanes against where from a set known slopes (of unrestricted slopes a product distribution Next we>
Label: Theory
Paper 9:  <Title: On the Sample Complexity of Noise-Tolerant Learning  .   Abstract: In further characterize the PAC model Specifically show a general lower log(1=ffi examples required PAC learning classification noise Combined a result Simon, effectively show the sample complexity PAC learning classification noise VC(F "(12 2 : Furthermore the optimality general lower by providing symmetric Boolean functions which uses a sample s>
Label: Theory
Paper 2:  <Title: Simulating Access to Hidden Information while Learning  .   Abstract: We introduce which enables a learner without access hidden information learn nearly as We apply our technique solve an open problem of Maass and Turan [18 showing for any concept class F, the least number queries sufficient learning by an algorithm which access only arbitrary equivalence queries at most a factor 1= log4=3 more membership que>
Paper 3:  <Title: Statistical Queries and Faulty PAC Oracles  .   Abstract: In study learning the PAC model Valiant [ the example oracle used may faulty one examples We first consider models examples misclassified Kearns [12 recently showed efficient learning a new model using statistical queries a sufficient condition PAC learning with classification noise We show efficient learning with statistical queries sufficient learning the PAC model malicious error rate proportio>
Paper 5:  <Title: General Bounds on Statistical Query Learning and PAC Learning with Noise via Hypothesis Boosting  .   Abstract: We derive general bounds learning the Statistical Query model the PAC model with classification noise We do so considering boosting weak learning algorithms which fall the Statistical Query model This new model Kearns [12 efficient PAC learning the presence classification noise We first show a general scheme boosting weak SQ learning algorithms proving equivalent The boosting is effi>
Paper 6:  <Title: Predicting a binary sequence almost as well as the optimal biased coin  .   Abstract: We apply the exponential weight algorithm introduced and Littlestone and Warmuth [ by Vovk [24 predicting a binary sequence almost the best biased coin We first show for the case the derived algorithm Jeffrey's prior that was studied Xie Barron under26 We derive a uniform bound the regret which holds any sequence We show the empirical distribution the sequence bounded away 0>
Paper 8:  <Title: Theory and Applications of Agnostic PAC-Learning with Small Decision Trees  .   Abstract: We exhibit a theoretically founded algorithm T2 agnostic PAC-learning of decision trees of at most 2 levels whose computation time almost linear the size We evaluate this learning algorithm T2 on 15 common real-world datasets show for most provides simple decision trees predictive power ( C4.5 In fact for datasets continuous attributes its error rate tends C4.5 To the PAC-learning>
Paper 10:  <Title: A General Lower Bound on the Number of Examples Needed for Learning  .   Abstract: We prove a lower ( 1 * ln 1 ffi + VCdim(C ) on random examples required distribution-free learning a concept class C the Vapnik-Chervonenkis dimension * and the accuracy and confidence parameters This improves previous best lower ( 1 * ln 1 ffi VCdim(C comes O * ) consistent algorithms We show for many interesting concept classes including kCNF our bound is actually tight within a constant factor>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Theory

Theory
Prediction:  Theory
Is prediction correct?  True

Prediction: 1
Processing index 1139...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Optimal Mutation Rates in Genetic Search  
Abstract: Abstract: The optimization of a single bit string by means of iterated mutation and selection of the best (a (1+1)-Genetic Algorithm) is discussed with respect to three simple fitness functions: The counting ones problem, a standard binary encoded integer, and a Gray coded integer optimization problem. A mutation rate schedule that is optimal with respect to the success probability of mutation is presented for each of the objective functions, and it turns out that the standard binary code can hamper the search process even in case of unimodal objective functions. While normally a mutation rate of 1=l (where l denotes the bit string length) is recommendable, our results indicate that a variation of the mutation rate is useful in cases where the fitness function is a multimodal pseudo-boolean function, where multimodality may be caused by the objective function as well as the encoding mechanism.
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 3:  <Title: An Analysis of the MAX Problem in Genetic Programming hold only in some cases, in.   Abstract: We present genetic programmingGP populations the problem finding a program returns and function set a depth limitknown the MAX problem We confirm the basic message [ Gathercole and Ross 1996 crossover together program size restrictions responsible premature convergence to We show this can retains variety show in evolution from t>
Label: Genetic Algorithms
Paper 2:  <Title: An Evolutionary Approach to Combinatorial Optimization Problems  .   Abstract:  paper reports the application genetic algorithms based the model organic evolution NP-complete combinatorial optimization problems In particular the subset sum, maximum cut minimum tardy task problems considered Except the fitness function no problem-specific changes of the genetic algorithm results of high quality even the problem instances size 100 used For constrained problems the subset sum and the minimum tardy task the constraints taken>
Paper 4:  <Title: Genetic algorithms with multi-parent recombination  .   Abstract: In genetic algorithms where the recombination operation In particular introduce gene scanning a reproduction mechanism generalizes classical crossovers n-point crossover uniform crossover applicable an arbitrary number (two parents We performed optimizing numerical functions the TSP graph coloring to observe different numbers parents The experiments 2-parent recombination outperformed using more parents on the cl>
Paper 5:  <Title: Putting the Genetics back into Genetic Algorithms  .   Abstract: In genetic algorithms where the recombination operation In particular introduce gene scanning a reproduction mechanism generalizes classical crossovers n-point crossover uniform crossover applicable an arbitrary number (two parents We performed optimizing numerical functions the TSP graph coloring to observe different numbers parents The experiments 2-parent recombination outperformed using more parents on the cl>
Paper 6:  <Title: MLC Tutorial A Machine Learning library of C classes.  .   Abstract: Several evolutionary algorithms make hierarchical representations variable size rather linear strings Variable complexity the structures provides an additional representational power which widen evolutionary algorithms The price, however that the search space open- solutions grow arbitrarily large size In study structural complexity the solutions their generalization performance analyzing the fitness landscape sigma-pi neural networks The analys>
Paper 7:  <Title: Balancing Accuracy and Parsimony in Genetic Programming 1  .   Abstract: Genetic programming distinguished other evolutionary algorithms tree representations variable size instead linear strings The flexible representation scheme very the underlying structure discovered automatically One primary difficulty, the solutions may grow too without any improvement of their generalization ability In this paper investigate the fundamental relationship the performance complexity the evolved structures. The essence the parsimony>
Paper 8:  <Title: Graph Coloring with Adaptive Evolutionary Algorithms  .   Abstract: This paper solving graph coloringEA After testing different algorithm variants we conclude an asexual EA order-based representation an adaptation mechanism periodically during This adaptive EA general using no domain specific knowledge except, from (fitness function We compare this adaptive EA a powerful traditional graph coloring technique DSatur and the Grouping GA on>
Paper 9:  <Title: Price's Theorem and the MAX Problem  .   Abstract: We present GP populations the problem finding a program returns and function set a depth limitknown the MAX problem We confirm the basic message [ Gathercole and Ross 1996 crossover together program size restrictions responsible premature convergence to We show this can retains variety show in evolution from the sub-optimal soluti>
Paper 10:  <Title: State Reconstruction for Determining Predictability in Driven Nonlinear Acoustical Systems  .   Abstract: Genetic programming distinguished other evolutionary algorithms tree representations variable size instead linear strings The flexible representation scheme very the underlying structure discovered automatically One primary difficulty, the solutions may grow too without any improvement of their generalization ability In this paper investigate the fundamental relationship the performance complexity the evolved structures. The essence the parsimony>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Genetic Algorithms

Category: Genetic Algorithms
Prediction:  Category: Genetic Algorithms
Is prediction correct?  False

Prediction: 0
Processing index 954...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Unsupervised learning of distributions on binary vectors using two layer networks  
Abstract: Abstract: We present a distribution model for binary vectors, called the influence combination model and show how this model can be used as the basis for unsupervised learning algorithms for feature selection. The model is closely related to the Harmonium model defined by Smolensky [RM86][Ch.6]. In the first part of the paper we analyze properties of this distribution representation scheme. We show that arbitrary distributions of binary vectors can be approximated by the combination model. We show how the weight vectors in the model can be interpreted as high order correlation patterns among the input bits. We compare the combination model with the mixture model and with principle component analysis. In the second part of the paper we present two algorithms for learning the combination model from examples. The first algorithm is based on gradient ascent. Here we give a closed form for this gradient that is significantly easier to compute than the corresponding gradient for the general Boltzmann machine. The second learning algorithm is a greedy method that creates the hidden units and computes their weights one at a time. This method is a variant of projection pursuit density estimation. In the third part of the paper we give experimental results for these learning methods on synthetic data and on natural data of handwritten digit images. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 4:  <Title: Learning a set of primitive actions with an Induction of decision trees. Machine Learning, 1(1):81-106,.   Abstract: Although probabilistic inference in a general Bayesian belief network inference computation time reduced most practical cases exploiting domain knowledge by making the knowledge representation In the property similarity states a new method approximate knowledge representation which We define two or more states a node to similar when the likelihood ratio their probabilities does depend the instantiations netwo>
Label: Theory
Paper 7:  <Title: Static Data Association with a Terrain-Based Prior Density  .   Abstract: In works learning probabilistic belief networks Current state have shown successful two learning scenarios learning both network structure parameters from complete data parameters a fixed network from incomplete datathat is, the presence missing values or hidden variables However no method demonstrated effectively learn network structure incomplete data In this paper learning network structure incomplete data This method>
Label: Probabilistic Methods
Paper 10:  <Title: Learning Markov chains with variable memory length from noisy output  .   Abstract: The problem modeling complicated data sequences speech often practice Most the algorithms select a hypothesis within a model class assuming the observed sequence the direct output In when the output passes a memoryless noisy channel before observation In particular show the class Markov chains variable memory length learning affected factors, which, despite super still small some practical cases Markov model>
Label: Theory
Paper 2:  <Title: Separating Formal Bounds from Practical Performance in Learning Systems  .   Abstract: We present a distribution model binary vectors called the influence combination model and show as feature selection The model closely the Harmonium model defined Smolensky [RM86][Ch.6 In the paper analyze properties this distribution representation scheme We show arbitrary distributions binary vectors the combination model. We show the weight vectors interpreted high order correlation patterns among t>
Paper 3:  <Title: On the Learnability and Usage of Acyclic Probabilistic Finite Automata  .   Abstract: We propose and analyze a distribution learning a subclass This subclass characterized a certain distinguishability property the automata's states Though hardness results are known learning distributions generated general APFAs prove our algorithm efficiently the subclass we consider In particular show the KL-divergence the distribution generated the target source our hypothesis made arbitra>
Paper 5:  <Title: Training Algorithms for Hidden Markov Models Using Entropy Based Distance Functions  .   Abstract: We present new algorithms parameter estimation HMMs By adapting supervised learning construct maximize the observations while attempting stay close the current estimated parameters We use a bound on the relative entropy between the two HMMs as between The result new iterative training algorithms similar the EM (Baum-Welch training HMMs composed similar the expectation step a new upd>
Paper 6:  <Title: EXPERIMENTING WITH THE CHEESEMAN-STUTZ EVIDENCE APPROXIMATION FOR PREDICTIVE MODELING AND DATA MINING  .   Abstract: The work discussed motivated of building decision support systems Our goal use these systems supporting Bayes optimal decision making where the action maximizing the expected utility, with respect predicted probabilities should selected For this reason the models used need probabilistic | the output has numbers For the model family we chosen the set simple discrete finite mixture mo>
Paper 8:  <Title: Distribution Category:  Users Guide to the PGAPack Parallel Genetic Algorithm Library  .   Abstract: The problem modeling complicated data sequences speech often practice Most the algorithms select a hypothesis within a model class assuming the observed sequence the direct output In when the output passes a memoryless noisy channel before observation In particular show the class Markov chains variable memory length learning affected factors, which, despite super still small some practical cases Markov model>
Paper 9:  <Title: Support Vector Machines: Training and Applications  .   Abstract: The Support Vector Machine a new and very promising classification technique Vapnik his group [3, 6 24 This new learning algorithm can seen an alternative training technique Polynomial, Radial Basis Function and Multi-Layer Perceptron classifiers The main idea the technique separate the classes with a surface maximizes the margin An interesting property this approach an approximate implementation Structural Risk Minimization induction principle [23 The>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Probabilistic Methods

Neural Networks
Prediction:  Neural Networks
Is prediction correct?  False

Prediction: 0
Processing index 1520...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Equivariant adaptive source separation  
Abstract: Abstract: Source separation consists in recovering a set of independent signals when only mixtures with unknown coefficients are observed. This paper introduces a class of adaptive algorithms for source separation which implements an adaptive version of equivariant estimation and is henceforth called EASI (Equivariant Adaptive Separation via Independence). The EASI algorithms are based on the idea of serial updating: this specific form of matrix updates systematically yields algorithms with a simple, parallelizable structure, for both real and complex mixtures. Most importantly, the performance of an EASI algorithm does not depend on the mixing matrix. In particular, convergence rates, stability conditions and interference rejection levels depend only on the (normalized) distributions of the source signals. Close form expressions of these quantities are given via an asymptotic performance analysis. This is completed by some numerical experiments illustrating the effectiveness of the proposed approach. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 10:  <Title: Simple Neuron Models for Independent Component Analysis  .   Abstract: Recently several neural algorithms introduced Independent Component Analysis Here approach a single neuron First simple Hebbian-like learning rules introduced estimating one of the independent components from sphered data Some the learning rules can used estimate an independent component which has the others estimate Next a two-unit system estimate an independent component any kurtosis The results then generalized estimat>
Label: Neural Networks
Paper 2:  <Title: A Blind Identification and Separation Technique via Multi-layer Neural Networks  .   Abstract: This paper deals blind identification source separation which consists estimation the mixing matrix stochastically independent sources without on. The method we propose estimates the mixture matrix by recurrent InputIO Identification using as inputs the estimated sources Herein, the nonlinear transformationdistortion consists the inputs the IO-Identification device contrast t>
Paper 3:  <Title: A New Learning Algorithm for Blind Signal Separation  .   Abstract:  on learning algorithm which a statistical dependency among outputs derived blind separation mixed signals The dependency measured the average mutual information The source signals and the mixing matrix unknown except the sources The Gram-Charlier expansion instead used evaluating the MI. The natural gradient approach minimize the MI. A novel activation function proposed on learning algorithm which an equivariant property easily imple>
Paper 4:  <Title: The Central Classifier Bound ANew Error Bound for the Classifier Chosen by Early Stopping Key.   Abstract:  on learning algorithm which a statistical dependency among outputs derived blind separation mixed signals The dependency measured the average mutual information The source signals and the mixing matrix unknown except the sources The Gram-Charlier expansion instead used evaluating the MI. The natural gradient approach minimize the MI. A novel activation function proposed on learning algorithm which an equivariant property easily imple>
Paper 5:  <Title: A FAMILY OF FIXED-POINT ALGORITHMS FOR INDEPENDENT COMPONENT ANALYSIS  .   Abstract: Independent Component AnalysisICA a statistical signal processing technique whose main applications blind source separation feature Estimation ICA is usually performed optimizing a 'contrast' function based higher-order cumulants In it almost any error function construct a contrast function perform the ICA estimation In particular this means one contrast functions that robust As a practical method tnding the relevant extrema such contrast fu>
Paper 6:  <Title: On the performance of orthogonal source separation algorithms  .   Abstract: Source separation consists recovering n independent signals m n observed instantaneous mixtures possibly corrupted additive noise Many source separation algorithms second order information a whitening operation which the non trivial part determining a unitary matrix Most further show a kind invariance property exploited predict some general results their performance Our first contribution exhibit lower bound the performance in accuracy the separation T>
Paper 7:  <Title: SELF-ADAPTIVE NEURAL NETWORKS FOR BLIND SEPARATION OF SOURCES  .   Abstract: Novel on- learning algorithms with self adaptive learning ratesparameters blind separation signals proposed The main motivation development new learning rules convergence speed crosstalking non-stationary signals Furthermore discovered under the proposed neural network models with associated learning algorithms exhibit a random switch attention ability of chaotic or random switching or cross output signals such way a specified separated signal a>
Paper 8:  <Title: BLIND SEPARATION OF DELAYED SOURCES BASED ON INFORMATION MAXIMIZATION  .   Abstract: Recently Bell and Sejnowski presented blind source separation the information maximization principle We extend this approach into the sources may delayed with each We present a network architecture capable coping such sources derive the adaptation equations the delays the weights maximizing transferred Examples using wideband sources speech are presented the algorithm>
Paper 9:  <Title: NEURAL NETWORK APPROACH TO BLIND SEPARATION AND ENHANCEMENT OF IMAGES  .   Abstract: In this contribution propose blind separation sources (for one dimensional signals images the case that not only the waveform sources unknown their number For this purpose multi-layer neural networks with associated adaptive learning algorithms developed The primary source signals can have any non-Gaussian distribution sub super Computer experiments presented which demonstrate the validity high performance the proposed approach>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Neural Networks

Neural Networks
Prediction:  Neural Networks
Is prediction correct?  True

Prediction: 1
Processing index 161...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: On Bayesian analysis of mixtures with an unknown number of components  Summary  
Abstract: Abstract: New methodology for fully Bayesian mixture analysis is developed, making use of reversible jump Markov chain Monte Carlo methods, that are capable of jumping between the parameter subspaces corresponding to different numbers of components in the mixture. A sample from the full joint distribution of all unknown variables is thereby generated, and this can be used as a basis for a thorough presentation of many aspects of the posterior distribution. The methodology is applied here to the analysis of univariate normal mixtures, using a hierarchical prior model that offers an approach to dealing with weak prior information while avoiding the mathematical pitfalls of using improper priors in the mixture context.
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 2:  <Title: Hyperparameter estimation in Dirichlet process mixture models  .   Abstract: In Bayesian density estimation prediction standard, exponential family distributions the precision or total mass parameter a critical hyperparame-ter that strongly resulting inferences numbers mixture components This note shows, respect a flexible class prior distributions this parameter posterior may represented easily simulated As inference this key quantity developed tandem the existing, rou>
Label: Probabilistic Methods
Paper 8:  <Title: Parallel Markov chain Monte Carlo sampling.  .   Abstract: Markov chain Monte Carlo samplers proved popular tools Bayesian computation However problems can their application the density interest dimensional strongly In these circumstances the sampler slow traverse the state space mixing poor In offer The state space of the Markov chain augmented to multiple chains Updates individual chains based around a genetic style crossover operator acting `parent' states draw>
Label: Probabilistic Methods
Paper 3:  <Title: Strategies for the Parallel Training of Simple Recurrent Neural Networks  .   Abstract: In Bayesian density estimation prediction standard, exponential family distributions the precision or total mass parameter a critical hyperparame-ter that strongly resulting inferences numbers mixture components This note shows, respect a flexible class prior distributions this parameter posterior may represented easily simulated As inference this key quantity developed tandem the existing, rou>
Paper 4:  <Title: Bayesian Mixture Modeling by Monte Carlo Simulation  .   Abstract: It shown Bayesian inference from modeled a mixture distribution feasibly via This method exhibits the true Bayesian predictive distribution implicitly integrating over An infinite number mixture components without difficulty a prior distribution mixing proportions selects a reasonable subset explain any finite training set The need decide components thereby avoided The feasibility the method sho>
Paper 5:  <Title: In Advances in Neural Information Processing Systems 8  Gaussian Processes for Regression  .   Abstract: The Bayesian analysis difficult a simple prior over weights implies a complex prior distribution functions In investigate Gaussian process priors over functions permit the predictive Bayesian analysis for fixed values to exactly using matrix operations Two methods using optimization averaging (via Hybrid Monte Carlo over have challenging problems>
Paper 6:  <Title: Probabilistic Principal Component Analysis  .   Abstract: Principal component analysis a ubiquitous technique data analysis processing one based upon a probability model In this paper demonstrate the principal axes a set observed data vectors may determined through maximum-likelihood estimation parameters closely factor analysis We consider the properties the associated likelihood function giving an EM algorithm the principal subspace iteratively discuss conveyed the definition a probability density func>
Paper 7:  <Title: Bayesian Finite Mixtures for Nonlinear Modeling of Educational data  .   Abstract: In this paper finding latent classes In our approach finite mixture models in demonstrate the possibility to use full joint probability models raises interesting new prospects The concepts discussed illustrated with using a data a recent educational study The Bayesian classification approach described has implemented, presents an appealing addition the standard toolbox exploratory anal>
Paper 9:  <Title: Bayesian MARS  .   Abstract: A Bayesian adaptive regression spline ( fitting 1991 proposed This takes a probability distribution over possible MARS models which explored using reversible jump Markov chain Monte Carlo methods (Green 1995 The generated sample MARS models produced is to averaged and allows easy interpretation to the overall fit>
Paper 10:  <Title: Signal Processing and Communications Reversible Jump Sampler for Autoregressive Time Series, Employing Full Conditionals to.   Abstract: Technical Report CUED/F-INFENG/TR. 304 We use reversible jump Markov Monte methodsGreen 1995 address model order uncertainty au-toregressive (AR) time series within Efficient model jumping achieved proposing model space moves the full conditional density for the AR parameters obtained This compared an alternative method for the moves cheaper proposals only the new parameters in Results are presented both synthetic>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Probabilistic Methods

Probabilistic Methods
Prediction:  Probabilistic Methods
Is prediction correct?  True

Prediction: 1
Processing index 1853...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: 99-113. Construction of Phylogenetic Trees, Science, Fitting the Gene Lineage Into Its Species Lineage. A
Abstract: Abstract: 6] Farach, M. and Thorup, M. 1993. Fast Comparison of Evolutionary Trees, Technical Report 93-46, DIMACS, Rutgers University, Piscataway, NJ. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 4:  <Title: A Neuro-Dynamic Programming Approach to Retailer Inventory Management 1  .   Abstract: Miller (1956 The magical number seven plus or: Some limits our capacity processing The Psychological Review 63(2):8197 Schmidhuber (1990b compositional learning with dynamic neural networks Technical Report FKI-12990, Technische Universitat Munchen Institut fu Informatik. Servan-Schreiber Cleermans (1988 Encoding sequential structure simple recurrent networks Technical Report CMUCS-88183, Carnegie Mellon University>
Label: Reinforcement Learning
Paper 7:  <Title: Causal inference, path analysis, and recursive struc-tural equations models. In C. Clogg, editor, Sociological Methodology,.   Abstract: Lipid Research Clinic Program 84. The Lipid Research Clinics Coronary Primary Prevention Trial results parts I Journal 251(3):351374 1984 [Pearl 93 Judea Pearl. Aspects graphical models connected causality Technical Report R-195-LL Cognitive Systems Laboratory UCLA June 1993 Submitted to BiometrikaJune 1993 Short version in Proceedings Invited papers Flo rence Italy August 1993 Tome LV Book 391401>
Label: Probabilistic Methods
Paper 2:  <Title: MML mixture modelling of multi-state, Poisson, von Mises circular and Gaussian distributions  .   Abstract: 11] M.H. Overmars. A random approach motion planning. Technical Report RUU-CS-92-32, Department October 1992>
Paper 3:  <Title: Mingers, 1989 J. Mingers. An empirical comparison of pruning methods for decision tree induction. Machine.   Abstract: Ourston Mooney 1990b ] Ourston Mooney. Improving shared rules multiple category domain theories Technical Report AI90150 Artificial Intelligence Labora tory University Texas December 1990>
Paper 5:  <Title: References elements that can solve difficult learning control problems. on Simulation of Adaptive Behavior, pages.   Abstract: Miller (1956 The magical number seven plus or: Some limits our capacity processing The Psychological Review 63(2):8197 Schmidhuber (1990b compositional learning with dynamic neural networks Technical Report FKI-12990, Technische Universitat Munchen Institut fu Informatik. Servan-Schreiber Cleermans (1988 Encoding sequential structure simple recurrent networks Technical Report CMUCS-88183, Carnegie Mellon University>
Paper 6:  <Title: Chunking in soar: The anatomy of a general learn ing mechanism. Machine Learning, 1(1). Learning.   Abstract: gers University Also appears tech. report ML- TR-7 Minton (1988 Quantitative results concerning explanation-based learning. Proceedings National Conference Artificial Intelli gence pages 564>
Paper 8:  <Title: "Linear Dependencies Represented by Chain Graphs," "Graphical Modelling With MIM," Manual. "Identifying Independence in Bayesian.   Abstract: 8] Dori and Tarsi "A Simple Algorithm Construct a Consistent Extension a Partially Oriented Graph. Also Technical Report R-185 UCLA Cognitive Systems Laboratory October 1992 [14] Pearl and Wermuth N. "When Can Association Graphs Admit" UCLA, Cognitive Systems Laboratory Technical Report R-183-L 1992 [17 Verma Pearl "Deciding Morality of Graphs is NP," Technical Report R-188, UCLA, Cognitive Systems Laboratory>
Paper 9:  <Title: CABeN: A Collection of Algorithms for Belief Networks  Correspond with:  .   Abstract: Portions this report have the Fifteenth Annual Symposium Computer Applications in Medical CareNovember, 1991>
Paper 10:  <Title: Local quartet splits of a binary tree infer all quartet splits via one dyadic inference.   Abstract: DIMACS Technical Report 96-43 DIMACS is a partnership Rutgers University Bellcore Bell Laboratories DIMACS is an NSF Science and Technology Center funded under contract STC-9119999; also receives the New Jersey Commission>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Theory

Probabilistic Methods
Prediction:  Probabilistic Methods
Is prediction correct?  False

Prediction: 0
Processing index 2591...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Lookahead and Discretization in ILP  
Abstract: Abstract: We present and evaluate two methods for improving the performance of ILP systems. One of them is discretization of numerical attributes, based on Fayyad and Irani's text [9], but adapted and extended in such a way that it can cope with some aspects of discretization that only occur in relational learning problems (when indeterminate literals occur). The second technique is lookahead. It is a well-known problem in ILP that a learner cannot always assess the quality of a refinement without knowing which refinements will be enabled afterwards, i.e. without looking ahead in the refinement lattice. We present a simple method for specifying when lookahead is to be used, and what kind of lookahead is interesting. Both the discretization and lookahead techniques are evaluated experimentally. The results show that both techniques improve the quality of the induced theory, while computational costs are acceptable.
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 4:  <Title: Structural Regression Trees  .   Abstract: In many real domains the task machine learning algorithms a theory predicting numerical values In particular several standard test domains used Inductive Logic Programming concerned predicting numerical values examples relational and mostly non-determinate background knowledge However so far no ILP algorithm except can predict numbers cope non-determinate background knowledge (The only exception a covering algorithm FORS In present Structural Regression Trees>
Label: Rule Learning
Paper 2:  <Title: Using Qualitative Models to Guide Inductive Learning  .   Abstract: This paper using qualitative models guide inductive learning Our objectives induce rules which not accurate explainable with respect the qualitative model reduce learning time exploiting domain knowledge Such explainability essential both practical application inductive technology integrating the results learning back We apply two process control problems a water tank network an ore grinding process used in>
Paper 3:  <Title: Induction of decision trees using RELIEFF  .   Abstract:  machine from examples this deals estimating attributes with and dependencies Greedy search prevents current inductive machine learning algorithms to significant dependencies the attributes Recently Kira Rendell developed the RELIEF algorithm estimating attributes is able detect dependencies show strong relation RELIEF's estimates impurity functions that usually heuristic guidance inductive learnin>
Paper 5:  <Title: Learning by Refining Algorithm Sketches  .   Abstract: In this paper suggest improves significantly a top-down inductive logic programming (ILP) learning system This improvement achieved at giving to extra information difficult formulate This information appears an algorithm sketch: an incomplete and somewhat vague representation the computation related a particular example We describe which sketches admissible give details the learning algorithm exploits The experiments carried>
Paper 6:  <Title: Naive Bayesian classifier within ILP-R  .   Abstract: When dealing the classification problems current ILP systems lag state attributional learners Part the blame can a much larger hypothesis space which therefore as thoroughly However sometimes due ILP systems take hypotheses when classifying unseen examples This paper proposes just that We developed within our ILP-R first order learner. The learner itself uses a clever RELIEF based heuristic able detec>
Paper 7:  <Title: Multi-class problems and discretization in ICL Extended abstract  .   Abstract: Handling multi-class problems real numbers practical applications KDD problems While attributevalue learners address as a rule very few ILP systems The few ILP systems handle real numbers mostly trying out all real values applicable thus running efficiency or overfitting This paper some recent extensions ICL that address ICL stands an ILP system learns first order logic formulae positive>
Paper 8:  <Title: Linear Space Induction in First Order Logic with RELIEFF  .   Abstract: Current ILP algorithms typically variants extensions the greedy search This prevents to detect significant relationships the training objects Instead myopic impurity functions propose based RELIEF guidance ILP algorithms At each step in our ILP-R system this heuristic a beam candidate literals The beam then an exhaustive search a potentially good conjunction literals From the efficiency point we introduce interesting declarative bias which keep>
Paper 9:  <Title: Constructing Intermediate Concepts by Decomposition of Real Functions  .   Abstract: In learning from examples it expand an attribute-vector representation intermediate concepts The usual advantage such structuring of the learning problem easier improves induced descriptions In develop discovering useful intermediate concepts when both the class the attributes real The technique a decomposition method originally the design switching circuits recently extended handle incompletely specified multivalued>
Paper 10:  <Title: Growing a Hypercubical Output Space in a Self-Organizing Feature Map  .   Abstract: Recent studies on planning comparing plan reuse plan generation both the above tasks may computational complexity even we deal very similar problems The aim the same kind results apply also diagnosis. We propose a theoretical complexity analysis coupled some experimental tests intended evaluate adaptation strategies which reuse the solutions past diagnostic problems build a solution to Results such analysis even>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Rule Learning

Rule Learning
Prediction:  Rule Learning
Is prediction correct?  True

Prediction: 1
Processing index 2507...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: The Observer-Observation Dilemma in Neuro-Forecasting: Reliable Models From Unreliable Data Through CLEARNING  
Abstract: Abstract: This paper introduces the idea of clearning, of simultaneously cleaning data and learning the underlying structure. The cleaning step can be viewed as top-down processing (the model modifies the data), and the learning step can be viewed as bottom-up processing (where the data modifies the model). After discussing the statistical foundation of the proposed method from a maximum likelihood perspective, we apply clearning to a notoriously hard problem where benchmark performances are very well known: the prediction of foreign exchange rates. On the difficult 1993-1994 test period, clearning in conjunction with pruning yields an annualized return between 35 and 40% (out-of-sample), significantly better than an otherwise identical network trained without cleaning. The network was started with 69 inputs and 15 hidden units and ended up with only 39 non-zero weights between inputs and hidden units. The resulting ultra-sparse final architectures obtained with clearning and pruning are immune against overfitting, even on very noisy problems since the cleaned data allow for a simpler model. Apart from the very competitive performance, clearning gives insight into the data: we show how to estimate the overall signal-to-noise ratio of each input variable, and we show that error estimates for each pattern can be used to detect and remove outliers, and to replace missing or corrupted data by cleaned values. Clearning can be used in any nonlinear regression or classification problem.
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 7:  <Title: TO IMPROVE FORECASTING  .   Abstract: Working Paper IS-97-007, Leonard N. Stern School In: Journal Computational Intelligence in Finance 61998 14 (Special Issue onImproving Generalization Nonlinear Financial Forecasting Models". Abstract. Predictive models for financial data plausible inputs potentially nonlinearly combined yield the conditional expectation a target a daily return This paper introduces this ta>
Label: Neural Networks
Paper 2:  <Title: WRAPPERS FOR PERFORMANCE ENHANCEMENT AND OBLIVIOUS DECISION GRAPHS  .   Abstract: This paper introduces clearning of simultaneously cleaning data learning The cleaning step can viewed top-down processing (the model modifieswhere After discussing the statistical foundation the proposed method from a maximum likelihood perspective apply clearning a notoriously hard problem where benchmark performances very well: foreign exchange rates On the difficult 1993-1994 test period clea>
Paper 3:  <Title: LEARNING MORE FROM LESS DATA: EXPERIMENTS WITH LIFELONG ROBOT LEARNING  .   Abstract: Most connectionist modeling assumes noise-free inputs This assumption often violated. This paper introduces clearning of simultaneously cleaning the data learning The cleaning step can viewed top-down processing (where the model modifies the learning step Clearning is used conjunction standard pruning This paper discusses the statistical foundation clearning gives an interpretation in a mechanical model describes>
Paper 4:  <Title: On-Line Adaptation of a Signal Predistorter through Dual Reinforcement Learning  .   Abstract: Most connectionist modeling assumes noise-free inputs This assumption often violated. This paper introduces clearning of simultaneously cleaning the data learning The cleaning step can viewed top-down processing (where the model modifies the learning step Clearning is used conjunction standard pruning This paper discusses the statistical foundation clearning gives an interpretation in a mechanical model describes>
Paper 5:  <Title: On-Line Adaptation of a Signal Predistorter through Dual Reinforcement Learning  .   Abstract: Most connectionist modeling assumes noise-free inputs This assumption often violated. This paper introduces clearning of simultaneously cleaning the data learning The cleaning step can viewed top-down processing (where the model modifies the learning step Clearning is used conjunction standard pruning This paper discusses the statistical foundation clearning gives an interpretation in a mechanical model describes>
Paper 6:  <Title: Avoiding overfitting by locally matching the noise level of the data gating network discovers the.   Abstract: When trying forecast two of nonstationarity of the process, regime switching overfittingparticularly serious for noisy processes This articles shows gated experts point solutions The architecture, also society of experts mixture experts consists a (nonlinear) gating network Each expert learns a conditional meanas usual its own adaptive width The gating network learns a>
Paper 8:  <Title: Packet Routing and Reinforcement Learning: Estimating Shortest Paths in Dynamic Graphs  .   Abstract: This article exposes problems the commonly used technique splitting the available data training, that held fixed warns drawing such static splits shows ignoring variability across splits Using a bootstrap or resampling compare the uncertainty the solution stemming the data splitting neural network specific uncertaintiesparameter initialization choice number hidden units We present two results data First>
Paper 9:  <Title: A Bootstrap Evaluation of the Effect of Data Splitting on Financial Time Series  .   Abstract: This article exposes problems the commonly used technique splitting the available data training, that held fixed warns drawing such static splits shows ignoring variability across splits Using a bootstrap or resampling compare the uncertainty the solution stemming the data splitting neural network specific uncertaintiesparameter initialization choice number hidden units We present two results data First>
Paper 10:  <Title: Evaluating Neural Network Predictors by Bootstrapping  .   Abstract: We present, inspired the bootstrap, whose goal it determine the quality a neural network predictor Our method leads more robust forecasting along statistical information forecast performance exploit We exhibit the method in multi-variate time series prediction on financial data It turns the variation due different resamplings training, crossvalidation test sets significantly differ>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Neural Networks

Neural Networks
Prediction:  Neural Networks
Is prediction correct?  True

Prediction: 1
Processing index 38...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: HOW TO EVOLVE AUTONOMOUS ROBOTS: DIFFERENT APPROACHES IN EVOLUTIONARY ROBOTICS  
Abstract: Abstract: In most applications of neuro-evolution, each individual in the population represents a complete neural network. Recent work on the SANE system, however, has demonstrated that evolving individual neurons often produces a more efficient genetic search. This paper demonstrates that while SANE can solve easy tasks very quickly, it often stalls in larger problems. A hierarchical approach to neuro-evolution is presented that overcomes SANE's difficulties by integrating both a neuron-level exploratory search and a network-level exploitive search. In a robot arm manipulation task, the hierarchical approach outperforms both a neuron-based search and a network-based search. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 5:  <Title: A Cooperative Coevolutionary Approach to Function Optimization  .   Abstract: A general model the coevolution cooperating species is presented This model instantiated tested the domain function optimization compared a traditional GA-based function optimizer The results encouraging in two respects They suggest ways the performance GA and other EA-based optimizers they evolving complex structures such rule sets>
Label: Genetic Algorithms
Paper 8:  <Title: A Coevolutionary Approach to Learning Sequential Decision Rules  .   Abstract: We present learning sequential decision rules which appears The coevolutionary approach encourages stable niches representing simpler sub The evolutionary direction each subbehavior controlled independently providing evolving complex behavior using intermediate training steps Results are presented showing a significant learning rate speedup a simulated robot domain In sugg>
Label: Genetic Algorithms
Paper 10:  <Title: Evolutionary Neural Networks for Value Ordering in Constraint Satisfaction Problems  .   Abstract: Technical Report AI94218 May 1994 developing good value-ordering strategies constraint satisfaction search Using an evolutionary technique SANE in individual neurons evolve cooperate problem-specific knowledge discovered results better value-ordering decisions A neural network was evolved a chronological backtrack search decide the ordering cars a resource-limited assembly line The network required 1/30 the backtracks o>
Label: Genetic Algorithms
Paper 2:  <Title: Hierarchical Evolution of Neural Networks  .   Abstract:  most applications of neuro-evolution each individual in represents Recent work the SANE system however evolving individual neurons often produces a more efficient genetic search This paper while SANE solve easy tasks very often stalls larger problems A hierarchical approach presented that overcomes SANE's difficulties both a neuron-level exploratory search a robot arm manipulation task hiera>
Paper 3:  <Title: Machine Learning,  Efficient Reinforcement Learning through Symbiotic Evolution  .   Abstract:  presents a new reinforcement learning method SANESymbiotic, evolves a population through genetic algorithms form capable Symbiotic evolution promotes both cooperation specialization results a fast, efficient genetic search discourages convergence to In the inverted pendulum problem SANE formed effective networks 9 the Adaptive Heuristic Critic and 2 Q-learning and GENITOR neuro appr>
Paper 4:  <Title: TD Learning of Game Evaluation Functions with Hierarchical Neural Architectures  .   Abstract: Genetic algorithms solve hard optimization problems ranging the Travelling Salesman problem the Quadratic Assignment problem We show the Simple Genetic Algorithm solve derived the 3-Conjunctive Normal Form problem By separating the populations small sub parallel genetic algorithms exploits the inherent parallelism prevents premature convergence Genetic algorithms using hill-climbing conduct genetic search in the space local optima can less>
Paper 6:  <Title: Evolving Optimal Neural Networks Using Genetic Algorithms with Occam's Razor  .   Abstract: Genetic algorithms neural networks two main ways optimize the network architecture train the weights a fixed architecture While most previous work focuses only of investigates an alternative evolutionary approach called Breeder Genetic Programming the architecture the weights optimized simultaneously The genotype each network represented whose depth and dynamically adapted the particular application by specifically defined genetic operators The weights t>
Paper 7:  <Title: Toward a unified theory of spatiotemporal processing in the retina  .   Abstract: Traditional evolutionary optimization algorithms assume a static evaluation function according solutions evolved Incremental evolution an approach through a dynamic evaluation function scaled over improve evolutionary optimization In this paper empirical results this approach genetic programming Using two domains a two-agent pursuit-evasion game the Tracker [6] trail-following task demonstrate incremental evolution most successful applied near t>
Paper 9:  <Title: Hierarchical priors and mixture models, with application in regression and density estimation  .   Abstract: We integrated the distributed search genetic programmingGP collective memory form a collective adaptation search method Such a system significantly search as problem complexity Since the pure GP approach does scale problem complexity a natural question actually contributing We investigate a collective memory search which utilizes find significantly We examine the solution space s>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Genetic Algorithms

Genetic Algorithms
Prediction:  Genetic Algorithms
Is prediction correct?  True

Prediction: 1
Processing index 2652...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Knowledge-Based Re-engineering of Legacy Programs for Robustness in Automated Design  
Abstract: Abstract: Systems for automated design optimization of complex real-world objects can, in principle, be constructed by combining domain-independent numerical routines with existing domain-specific analysis and simulation programs. Unfortunately, such legacy analysis codes are frequently unsuitable for use in automated design. They may crash for large classes of input, be numerically unstable or locally non-smooth, or be highly sensitive to control parameters. To be useful, analysis programs must be modified to reduce or eliminate only the undesired behaviors, without altering the desired computation. To do this by direct modification of the programs is labor-intensive, and necessitates costly revalidation. We have implemented a high-level language and run-time environment that allow failure-handling strategies to be incorporated into existing Fortran and C analysis programs while preserving their computational integrity. Our approach relies on globally managing the execution of these programs at the level of discretely callable functions so that the computation is only affected when problems are detected. Problem handling procedures are constructed from a knowledge base of generic problem management strategies. We show that our approach is effective in improving analysis program robustness and design optimization performance in the domain of conceptual design of jet engine nozzles. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 5:  <Title: Using Modeling Knowledge to Guide Design Space Search  .   Abstract: Automated search a space of candidate designs seems an attractive way the traditional engineering design process To however the automated design system include both knowledge the modeling limitations evaluate candidate designs also an effective way use influence We suggest a productive approach include this knowledge by implementing model constraint functions measure each modeling assumptions is violated influence the search>
Label: Genetic Algorithms
Paper 6:  <Title: A Transformation System for Interactive Reformulation of Design Optimization Strategies  .   Abstract: Numerical design optimization algorithms highly sensitive the particular formulation the optimization problems they given The formulation the search space, the objective function will generally have the duration the optimization process as the resulting design Furthermore the best formulation will vary one application domain one problem within Unfortunately a design engineer may the best formulation in advance attempting>
Label: Genetic Algorithms
Paper 10:  <Title: Using Case-Based Reasoning as a Reinforcement Learning Framework for Optimization with Changing Criteria  .   Abstract: Practical optimization problems such job-shop scheduling often optimization criteria change Repair-based frameworks have identified flexible computational paradigms difficult combinatorial optimization problems Since the control problem of repair-based optimization severe ReinforcementRL techniques potentially However some the fundamental assumptions made traditional RL algorithms repair-based optimization Case-Based Reasoning ( compensates some traditional>
Label: Case Based
Paper 2:  <Title: Program Synthesis and Transformation Techniques for Simpuation, Optimization and Constraint Satisfaction Deductive Synthesis of Numerical.   Abstract: Scientists face recurring problems of constructing, modifying numerical simulation programs The process coding revising such simulators extremely because almost conventional programming languages Scientists can therefore benefit software facilitates construction programs simulating Our research adapts the methodology deductive program synthesis the problem constructing numerical simulation codes We focused simulators represent>
Paper 3:  <Title: A Transformation System for Interactive Reformulation of Design Optimization Strategies  .   Abstract: Automatic design optimization highly sensitive problem formulation The choice objective function, constraints design parameters dramatically optimization best formulation varies A design engineer will usually not know the best formulation in advance In order address supports interactive formulation testing reformulation design optimization strategies Our system includes an executable, data-flow lang>
Paper 4:  <Title: Intelligent Gradient-Based Search of Incompletely Defined Design Spaces  .   Abstract: Gradient-based numerical optimization complex engineering designs offers rapidly producing However such methods generally assume the objective function and constraint functions continuous smooth defined everywhere Unfortunately realistic simulators tend violate these assumptions We present a rule-based technique intelligently computing gradients the presence such pathologies the simulators this gradient computation method part We tested>
Paper 7:  <Title: Knowledge Compilation and Speedup Learning in Continuous Task Domains  .   Abstract: Many techniques speedup learning knowledge compilation focus the learning optimization macrooperators or control rules task domains characterized a problem-space search paradigm However such a characterization fit well the class task domains the problem solver required in a continuous manner For example many robotic domains the problem solver required monitor real-valued perceptual inputs vary its motor control parameters continuous on successfully accomplish In>
Paper 8:  <Title: Data Exploration with Reflective Adaptive Models  .   Abstract: Case-Based Planning provides scaling domain-independent planning solve It replaces the detailed and lengthy search a solution the retrieval adaptation previous planning experiences In general CBP demonstrated over generative (from- planning However the performance improvements it dependent adequate judgements as problem similarity In particular although CBP substantially planning effort overall subject a mis-retrieval problem T>
Paper 9:  <Title: State Reconstruction for Determining Predictability in Driven Nonlinear Acoustical Systems  .   Abstract: Genetic programming distinguished other evolutionary algorithms tree representations variable size instead linear strings The flexible representation scheme very the underlying structure discovered automatically One primary difficulty, the solutions may grow too without any improvement of their generalization ability In this paper investigate the fundamental relationship the performance complexity the evolved structures. The essence the parsimony>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Case Based

Genetic Algorithms
Prediction:  Genetic Algorithms
Is prediction correct?  False

Prediction: 0
Processing index 959...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Numerical techniques for efficient sonar bearing and range searching in the near field using genetic algorithms  
Abstract: Abstract: This article describes a numerical method that may be used to efficiently locate and track underwater sonar targets in the near-field, with both bearing and range estimation, for the case of very large passive arrays. The approach used has no requirement for a priori knowledge about the source and uses only limited information about the receiver array shape. The role of sensor position uncertainty and the consequence of targets always being in the near-field are analysed and the problems associated with the manipulation of large matrices inherent in conventional eigenvalue type algorithms noted. A simpler numerical approach is then presented which reduces the problem to that of search optimization. When using this method the location of a target corresponds to finding the position of the maximum weighted sum of the output from all sensors. Since this search procedure can be dealt with using modern stochastic optimization methods, such as the genetic algorithm, the operational requirement that an acceptable accuracy be achieved in real time can usually be met. The array studied here consists of 225 elements positioned along a flexible cable towed behind a ship with 3.4m between sensors, giving an effective aperture of 761.6m. For such a long array, the far field assumption used in most beam-forming algorithms is no longer appropriate. The waves emitted by the targets then have to be considered as curved rather than plane. It is shown that, for simulated data, if no significant noise 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 3:  <Title: Space-Frequency Localized Basis Function Networks for Nonlinear System Estimation and Control  .   Abstract: Stable neural network control estimation may viewed formally a merging concepts tools multivariate approximation theory This paper extends earlier results adaptive control estimation gaussian radial basis functions on generation irregularly sampled networks tools multiresolution analysis wavelet theory yields much more compact and efficient system representations while global closed-loop stability Approximation models employing basis functio>
Label: Neural Networks
Paper 2:  <Title: A Sequential Niche Technique for Multimodal Function Optimization  .   Abstract: c fl UWCC COMMA Technical Report No 93001 February 1993 x No part for Abstract A technique is unimodal function optimization methods extended efficiently locate all optima multimodal problems We describe a traditional genetic algorithmGA This involves iterating the GA, but uses knowledge avoid re, on regions problem space solutions This achieved applying a fitness>
Paper 4:  <Title: A comparison of neural net and conventional techniques for lighting control  .   Abstract: We compare two techniques lighting control an actual room equipped seven banks photoresistors detect four sensing points Each bank lights independently set The task the device intensity levels achieve sensor readings One technique explored uses approximate the mapping sensor readings device intensity levels The other technique examined uses a conventional feedback control loop The neural network a>
Paper 5:  <Title: Data Reconciliation and Gross Error Detection for Dynamic Systems  .   Abstract: Gross error detection plays parameter estimation data reconciliation for In particular recent advances process optimization now data reconciliation dynamic systems appropriate problem formulations need them Data errors due either miscalibrated or faulty sensors just random events nonrepresentative the underlying statistical distribution induce heavy biases in the reconciled data In concentrate robust estimators explorator>
Paper 6:  <Title: Bayesian Training of Backpropagation Networks by the Hybrid Monte Carlo Method  .   Abstract: It shown Bayesian training feasibly the "Hybrid Monte Carlo" method This approach allows the true predictive distribution for a test case given training cases to approximated arbitrarily closely contrast approximate In this work the Hybrid Monte Carlo method implemented conjunction simulated annealing speed relaxation to a good region parameter space The method has applied a test problem dem>
Paper 7:  <Title: A GENERAL METHOD FOR INCREMENTAL SELF-IMPROVEMENT AND MULTI-AGENT LEARNING  .   Abstract: Process simulation has emerged process design analysis operation In this work extend iterated linear programmingLP dealing problems encountered dynamic nonsmooth process simulation A previously developed LP method refined a new descent strategy line search a trust region approach This adds more stability efficiency the method The LP method naturally dealing profile bounds as This demonstrated to avoid the computational difficulti>
Paper 8:  <Title: An Investigation of Marker-Passing Algorithms for Analogue Retrieval  .   Abstract: If analogy and case-based reasoning systems scale very large case bases analyze retrieving analogues to identify the features for appropriate This paper reports one such analysis comparison retrieval by marker passing or spreading activation a semantic network with Knowledge-Directed Spreading Activation be well retrieving semantically distant analogues a large knowledge base The analysis has two complementary components theoretical m>
Paper 9:  <Title: A Fast Fixed-Point Algorithm for Independent Component Analysis  .   Abstract: This paper will appear Neural Computation 9:1483 1997 Abstract We introduce a novel fast algorithm Independent Component Analysis can blind source separation feature It shown how a neural network learning rule transformed a txed-point iteration provides an algorithm very simple, does depend any user-detned parameters fast converge the most accurate solution allowed The algorithm tnds, one at all non-Gaussian independent components regardless their probability>
Paper 10:  <Title: An Overview of Genetic Algorithms Part 1, Fundamentals  .   Abstract:  approaches three fundamental problems will described: feature selection robust representation The feature selection problem considered discriminating while recognizing irrelevant and redundant features suppressing creates a lean model often generalizes better new unseen data real data confirm improved generalization leaner models Clustering exemplified patterns clusters may exist is a usefu>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Genetic Algorithms

Genetic Algorithms
Prediction:  Genetic Algorithms
Is prediction correct?  True

Prediction: 1
Processing index 616...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: A Decision Tree System for Finding Genes in DNA  
Abstract: Abstract: MORGAN is an integrated system for finding genes in vertebrate DNA sequences. MORGAN uses a variety of techniques to accomplish this task, the most distinctive of which is a decision tree classifier. The decision tree system is combined with new methods for identifying start codons, donor sites, and acceptor sites, and these are brought together in a frame-sensitive dynamic programming algorithm that finds the optimal segmentation of a DNA sequence into coding and noncoding regions (exons and introns). The optimal segmentation is dependent on a separate scoring function that takes a subsequence and assigns to it a score reflecting the probability that the sequence is an exon. The scoring functions in MORGAN are sets of decision trees that are combined to give a probability estimate. Experimental results on a database of 570 vertebrate DNA sequences show that MORGAN has excellent performance by many different measures. On a separate test set, it achieves an overall accuracy of 95%, with a correlation coefficient of 0.78 and a sensitivity and specificity for coding bases of 83% and 79%. In addition, MORGAN identifies 58% of coding exons exactly; i.e., both the beginning and end of the coding regions are predicted correctly. This paper describes the MORGAN system, including its decision tree routines and the algorithms for site recognition, and its performance on a benchmark database of vertebrate DNA. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 4:  <Title: Hidden Markov Models in Computational Biology: Applications to Protein Modeling UCSC-CRL-93-32 Keywords: Hidden Markov Models,.   Abstract: Hidden Markov ModelsHMMs applied the problems statistical modeling database searching multiple sequence alignment protein families protein domains These methods demonstrated on the globin family the protein the EF-hand calcium binding motif In each case the parameters an HMM estimated a training set unaligned sequences After the HMM is built obtain a multiple alignment all the training sequences It used search the SWISS-PROT 22 database other sequences members th>
Label: Neural Networks
Paper 8:  <Title: The megaprior heuristic for discovering protein sequence patterns  .   Abstract: Several computer algorithms for discovering patterns groups protein sequences in that fitting the parameters related sequences These include hidden Markov model (HMM) algorithms multiple sequence alignment the MEME and Gibbs sampler algorithms discovering motifs These algorithms sometimes prone producing models incorrect two or more patterns combined The statistical model produced this situation a convex combination (weighted>
Label: Neural Networks
Paper 10:  <Title: Maximum A Posteriori Classification of DNA Structure from Sequence Information  .   Abstract: We introduce lllama simple pattern recognizers into a general method estimating the entropy Each pattern recognizer exploits a partial match subsequences build Since the primary features interest biological sequence domains subsequences small variations exact composition lllama particularly We describe two methods, lllama-lengthalone this entropy estimate perform maximum a posteriori classification We apply seve>
Label: Neural Networks
Paper 2:  <Title: A Generalized Hidden Markov Model for the Recognition of Human Genes in DNA  .   Abstract: We present genes DNA. A Generalized Hidden Markov Model provides describing the grammar a legal parseStormo Haussler Probabilities assigned transitions states the GHMM to the generation each nucleotide base given Machine learning techniques applied optimize these probabilities a standardized training set Given a new candidate sequence the best parse deduced the model using a dynamic programming algorithm to th>
Paper 3:  <Title: Finding Genes in DNA with a Hidden Markov Model  .   Abstract: This study describes a new Hidden Markov Model (HMM) system segmenting uncharacterized genomic DNA sequences exons introns Separate HMM modules were designed trained specific regions DNA: exons introns intergenic regions The models then tied a biologically feasible topology The integrated HMM was trained further a set eukaryotic DNA sequences tested it segment a separate set sequences The resulting HMM system called VEILViterbi Exon-Intron Locator obtains>
Paper 5:  <Title: Dirichlet Mixtures: A Method for Improving Detection of Weak but Significant Protein Sequence Homology  .   Abstract: UCSC Technical Report-CRL-9609 Abstract the mathematical foundations Dirichlet mixtures improve database search results homologous sequences when a variable number from a protein family or domain known We present condensing a protein database a mixture Dirichlet densities These mixtures designed combined observed amino acid frequencies to form estimates expected amino acid probabilities at a profile hidden Markov model other st>
Paper 6:  <Title: Face Recognition: A Hybrid Neural Network Approach  .   Abstract: Faces represent complex, multidimensional, meaningful visual stimuli developing difficult (Turk and Pentland 1991 We present a hybrid neural network solution compares favorably The system combines local image sampling The self-organizing map provides a quantization the image samples into inputs that nearby in also nearby thereby dimensionality red>
Paper 7:  <Title: Learning to Predict Reading Frames in E. coli DNA Sequences  .   Abstract: Two fundamental problems analyzing DNA sequences ( locating the regions encode the reading frame We investigate using find coding regions, determine reading frames detect frameshift errors E. coli DNA sequences We describe our adaptation the approach Uberbacher Mural to identify coding regions human DNA compare ANNs several conventional methods predicting reading frames Our experiments demonstrate ANNs c>
Paper 9:  <Title: Searching for dependencies in Bayesian classifiers j A n V n j If the attributes.   Abstract: Naive Bayesian classifiers which make independence assumptions perform remarkably but poorly We explore ways searching dependencies attributes We propose and evaluate two algorithms dependencies attributes show the backward sequential elimination and joining provides The domains on the most improvement those domains on the naive Bayesian classifier significantly>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Neural Networks

Probabilistic Methods
Prediction:  Probabilistic Methods
Is prediction correct?  False

Prediction: 0
Processing index 2049...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Learning Feature-based Semantics with Simple Recurrent Networks  
Abstract: Abstract: The paper investigates the possibilities for using simple recurrent networks as transducers which map sequential natural language input into non-sequential feature-based semantics. The networks perform well on sentences containing a single main predicate (encoded by transitive verbs or prepositions) applied to multiple-feature objects (encoded as noun-phrases with adjectival modifiers), and shows robustness against ungrammatical inputs. A second set of experiments deals with sentences containing embedded structures. Here the network is able to process multiple levels of sentence-final embeddings but only one level of center-embedding. This turns out to be a consequence of the network's inability to retain information that is not reflected in the outputs over intermediate phases of processing. Two extensions to Elman's [9] original recurrent network architecture are introduced. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 2:  <Title: TABLE DES MATI ERES 1 Apprentissage et approximation les techniques de regularisation 3 1.1 Introduction.   Abstract: The paper investigates using simple recurrent networks transducers which map sequential natural language input The networks perform sentences containing a single main predicateencoded applied multiple-feature objects with adjectival modifiers shows against A second set experiments deals sentences containing embedded structures Here the network able process multiple levels sentence-fina>
Paper 3:  <Title: Subsymbolic Case-Role Analysis of Sentences with Embedded Clauses  .   Abstract: A distributed neural network model called SPEC for processing sentences recursive relative clauses The model separating the tasks segmenting the input word sequence clauses forming the case-role representations keeping into different modules The system needs trained only the basic sentence constructs generalizes not new instances familiar relative clause structures novel structures SPEC exhibits plausible memory degradation the depth the center embeddin>
Paper 4:  <Title: Simple Synchrony Networks: Learning Generalisations across Syntactic Constituents  .   Abstract: This paper a training algorithm Simple Synchrony Networks and reports experiments in language learning a recursive grammar The SSN a new connectionist architecture combining a technique learning patterns across time Simple Recurrent Networks Temporal Synchrony Variable Binding The use TSVB means the SSN can learn entities the training set generalise entities test In the experiments the network is trained sentences up to one embedded clause with some words>
Paper 5:  <Title: Natural Language Grammatical Inference with Recurrent Neural Networks  .   Abstract: This paper the inductive inference a complex grammar specifically, the task considered is training thereby exhibiting discriminatory power provided the Principles and Parameters linguistic framework Government-and-Binding theory Neural networks trained, without the division into learned innate components assumed Chomsky produce the same judgments native speakers on sharply grammatical/ungrammatical data Ho>
Paper 6:  <Title: Data-defined Problems and Multiversion Neural-net Systems  .   Abstract: We inv estigate an adaptive neural network problems time-dependent input by demonstrating a deterministic parser natural language inputs of recurrent connectionist architectures The traditional stacking mechanism to necessary proper treatment context-free languages symbolic systems absent the design, having subsumed recurrency the network>
Paper 7:  <Title: On the Applicability of Neural Network and Machine Learning Methodologies to Natural Language Processing  .   Abstract: We examine the inductive inference a complex grammar specifically, we training classify thereby exhibiting discriminatory power provided the Principles and Parameters linguistic framework or Government-and-Binding theory We investigate the following models Fransconi-Gori-Soda Back-Tsoi locally recurrent networks Elman, Narendra & Williams Zipser recurrent networks edit-distance s>
Paper 8:  <Title: Simple Synchrony Networks Learning to Parse Natural Language with Temporal Synchrony Variable Binding  .   Abstract: The Simple Synchrony Network a new connectionist architecture incorporating the insights Temporal Synchrony Variable Binding Simple Recurrent Networks The use TSVB means SSNs output representations structures can learn generalisations over the constituentsas required systematicity This paper the SSN an associated training algorithm SSNs' generalisation abilities through results training SSNs parse real natural language sentences>
Paper 9:  <Title: Scaling-up RAAMs  .   Abstract: Modifications Recursive Auto-Associative Memory are presented allow it store reported These modifications include extra layers the compressor and reconstructor networks employing integer rather real-valued representations pre the weights presetting the representations compatible them The resulting system tested data syntactic trees extracted the Penn Treebank>
Paper 10:  <Title: AN ADAPTIVE NEURAL NETWORK PARSER  .   Abstract: We inv estigate an adaptive neural network problems time-dependent input by demonstrating a deterministic parser natural language inputs of recurrent connectionist architectures The traditional stacking mechanism to necessary proper treatment context-free languages symbolic systems absent the design, having subsumed recurrency the network>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Neural Networks

Neural Networks
Prediction:  Neural Networks
Is prediction correct?  True

Prediction: 1
Processing index 1390...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Learning Finite Automata Using Local Distinguishing Experiments  
Abstract: Abstract: One of the open problems listed in [ Rivest and Schapire, 1989 ] is whether and how that the copies of L fl in their algorithm can be combined into one for better performance. This paper describes an algorithm called D fl that does that combination. The idea is to represent the states of the learned model using observable symbols as well as hidden symbols that are constructed during learning. These hidden symbols are created to reflect the distinct behaviors of the model states. The distinct behaviors are represented as local distinguishing experiments (LDEs) (not to be confused with global distinguishing sequences), and these LDEs are created when the learner's prediction mismatches the actual observation from the unknown machine. To synchronize the model with the environment, these LDEs can also be concatenated to form a homing sequence. It can be shown that D fl can learn, with probability 1 , a model that is an *-approximation of the unknown machine, in a number of actions polynomial in the size of the environment and 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 5:  <Title: Using Errors to Create Piecewise Learnable Partitions  .   Abstract: In this paper which exploits the error distribution generated a learning algorithm break up the domain which being approximated into piecewise learnable partitions the error distribution neglected favor a lump error measure such RMS By doing however lose The error distribution tells where the algorithm doing badly if exists a "ridge also tells partition the space so one part not interfere the learnin>
Label: Theory
Paper 6:  <Title: Learning Markov chains with variable memory length from noisy output  .   Abstract: The problem modeling complicated data sequences speech often practice Most the algorithms select a hypothesis within a model class assuming the observed sequence the direct output In when the output passes a memoryless noisy channel before observation In particular show the class Markov chains variable memory length learning affected factors, which, despite super still small some practical cases Markov model>
Label: Theory
Paper 9:  <Title: Using Mixtures of Factor Analyzers for Segmentation and Pose Estimation  Category: Visual Processing Preference: Oral  .   Abstract: To a hand-written digit string it segment separate digits Bottom-up segmentation heuristics often neighboring digits overlap substantially We describe has each digit class we the only knowledge required segmentation The system uses Gibbs sampling construct a perceptual interpretation a digit string segmentation arises naturally "explaining away By using conditional mixtures of factor analyzers>
Label: Neural Networks
Paper 2:  <Title: Online Learning with Random Representations  .   Abstract: We consider the requirements online learning|learning which must done incrementally realtime the results learning available soon each new example acquired Despite the abundance methods learning from examples online learning as components reinforcement learning systems Most few, including radial basis functions CMACs Ko-honen's self-organizing maps those developed share All expand the original input representation a higher dimensiona>
Paper 3:  <Title: Efficient Learning of Typical Finite Automata from Random Walks (Extended Abstract)  .   Abstract: This paper new and efficient algorithms learning deterministic finite automata Our approach primarily distinguished two features an average-case setting to model the "typical" labeling a finite automaton while retaining a worst-case model the underlying graph of along a learning model in not provided the means experiment must solely observing on a random input sequence The main contribution in pr>
Paper 4:  <Title: Distribution Category:  Users Guide to the PGAPack Parallel Genetic Algorithm Library  .   Abstract: The problem modeling complicated data sequences speech often practice Most the algorithms select a hypothesis within a model class assuming the observed sequence the direct output In when the output passes a memoryless noisy channel before observation In particular show the class Markov chains variable memory length learning affected factors, which, despite super still small some practical cases Markov model>
Paper 7:  <Title: Two Methods for Hierarchy Learning in Reinforcement Environments  .   Abstract: This paper two methods hierarchically organizing temporal behaviors first is more intuitive grouping common sequences events single units so may treated individual behaviors This system immediately encounters however the units binary the behaviors must execute completely or hinders the construction good training algorithms The system also runs into difficulty ( active The second system a hierarchy transition val>
Paper 8:  <Title: Improving Generalization with Active Learning  .   Abstract: Active learning differs " from examples the learning algorithm assumes what part receives In some situations active learning provably that learning examples alone giving better generalization for a fixed number training examples In consider learning a binary concept noiseValiant 1984 We describe a formalism active concept learning called selective sampling show may approximately implemente>
Paper 10:  <Title: Connectionist Modeling of the Fast Mapping Phenomenon  .   Abstract: The problem making optimal decisions uncertain conditions central Artificial Intelligence If the state the world known at all times can modeled a Markov Decision Process MDPs studied many methods known determining optimal courses or policies The more realistic case where state information only partially observable, Partially Observable Markov Decision Processes have received The best exact algorithms these problems very inefficient in both space ti>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Theory

Theory
Prediction:  Theory
Is prediction correct?  True

Prediction: 1
Processing index 1519...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: What online Machine Learning can do for Knowledge Acquisition A Case Study  
Abstract: Abstract: This paper reports on the development of a realistic knowledge-based application using the MOBAL system. Some problems and requirements resulting from industrial-caliber tasks are formulated. A step-by-step account of the construction of a knowledge base for such a task demonstrates how the interleaved use of several learning algorithms in concert with an inference engine and a graphical interface can fulfill those requirements. Design, analysis, revision, refinement and extension of a working model are combined in one incremental process. This illustrates the balanced cooperative modeling approach. The case study is taken from the telecommunications domain and more precisely deals with security management in telecommunications networks. MOBAL would be used as part of a security management tool for acquiring, validating and refining a security policy. The modeling approach is compared with other approaches, such as KADS and stand-alone machine learning. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 4:  <Title: Diplomarbeit A Genetic Algorithm for the Topological Optimization of Neural Networks  .   Abstract: We describe an integrated problem architecture named INBANCA Bayesian networks case-based reasoning ( work multiagent planning tasks This includes two-team dynamic tasks this paper concentrates simulated soccer as Bayesian networks characterize action selection whereas determine implement actions This paper two contributions First survey integrations case Bayesian from a popular CBR task decomposition framework thus e>
Label: Genetic Algorithms
Paper 6:  <Title: Structural Similarity as Guidance in Case-Based Design  .   Abstract: This paper determine structural similarity as guidance adaptationCbr We advance structural similarity assessment which the most specific structure two cases have inclusive the modification rules needed Our approach treats retrieval matching adaptation This guarantees the retrieval matching not only similar but adaptable cases Both together enlarge the overall problem solving>
Label: Case Based
Paper 8:  <Title: An Interactive Planning Architecture The Forest Fire Fighting case  .   Abstract: This paper an interactive planning system was inside aimed supporting an operator when the initial attack forest fires The planning architecture rests the integration case-based reasoning techniques constraint reasoning techniques exploited, mainly performing temporal reasoning temporal metric information Temporal reasoning plays supporting interactive functions provided two basic steps plan adaptation>
Label: Case Based
Paper 2:  <Title: Adaptive Tuning of Numerical Weather Prediction Models: Simultaneous Estimation of Weighting, Smoothing and Physical Parameters 1  .   Abstract: In case-based reasoning demonstrated problem complex domains Also mixed paradigm approaches emerged combining CBR and induction techniques aiming verifying the knowledge building an efficient case memory However in complex domains induction over the whole problem space or too time In this paper an approach which (owing a close interaction the CBR part attempts induce rules only a particular context a problem just beingor>
Paper 3:  <Title: Computation and Psychophysics of Sensorimotor Integration  .   Abstract: In learning classification rules data We sketch two modules our architecture namely LINNEO + and GAR LINNEO +, which a knowledge acquisition tool ill-structured domains automatically generating classes examples incrementally works LINNEO + 's output, a representation the conceptual structure classes the input GAR that classification rules GAR can generate both conjunctive and disjunctive r>
Paper 5:  <Title: Rule Generation and Compaction in the wwtp  .   Abstract: In learning classification rules data We sketch two modules our architecture namely LINNEO + and GAR LINNEO +, which a knowledge acquisition tool ill-structured domains automatically generating classes examples incrementally works LINNEO + 's output, a representation the conceptual structure classes the input GAR that classification rules GAR can generate both conjunctive and disjunctive r>
Paper 7:  <Title: Lazy Induction Triggered by CBR  .   Abstract: In case-based reasoning demonstrated problem complex domains Also mixed paradigm approaches emerged combining CBR and induction techniques aiming verifying the knowledge building an efficient case memory However in complex domains induction over the whole problem space or too time In this paper an approach which (owing a close interaction the CBR part attempts induce rules only a particular context a problem just beingor>
Paper 9:  <Title: TECHNIQUES FOR REDUCING THE DISRUPTION OF SUPERIOR BUILDING BLOCKS IN GENETIC ALGORITHMS  .   Abstract: We describe an integrated problem architecture named INBANCA Bayesian networks case-based reasoning ( work multiagent planning tasks This includes two-team dynamic tasks this paper concentrates simulated soccer as Bayesian networks characterize action selection whereas determine implement actions This paper two contributions First survey integrations case Bayesian from a popular CBR task decomposition framework thus e>
Paper 10:  <Title: A Similarity-Based Retrieval Tool for Software Repositories  .   Abstract: In a prototype a flexible similarity-based retrieval system Its flexibility supported allowing an imprecisely specified query Moreover our algorithm allows assessing if the retrieved items the initial context specified The presented system can a supporting tool a software repository We also discuss system evaluation with concerns on usefulness comparability the T A3 system on three domains gives encouraging and an integration TA3>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Rule Learning

Case Based
Prediction:  Case Based
Is prediction correct?  False

Prediction: 0
Processing index 1485...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Maximizing the Robustness of a Linear Threshold Classifier with Discrete Weights  
Abstract: Abstract: Quantization of the parameters of a Perceptron is a central problem in hardware implementation of neural networks using a numerical technology. An interesting property of neural networks used as classifiers is their ability to provide some robustness on input noise. This paper presents efficient learning algorithms for the maximization of the robustness of a Perceptron and especially designed to tackle the combinatorial problem arising from the discrete weights. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 2:  <Title: A Provably Convergent Dynamic Training Method for Multilayer Perceptron Networks  .   Abstract: This paper training multilayer perceptron networks called DMP1 1 The method based builds networks binary trees dynamically allocating layers as The individual nodes trained a gentetic algorithm The method capable handling real-valued inputs a proof is concerning its convergence properties of the basic model Simulation results DMP1 performs favorably comparison other learning algorithms>
Label: Neural Networks
Paper 3:  <Title: Interpretable Neural Networks with BP-SOM  .   Abstract:  models induced often In focus a relatively novel neural network architecture learning bpsom offers possibilities overcome It shown networks trained with bp-som show interesting regularities in that hidden-unit activations become restricted discrete values the som part can exploited automatic rule extraction>
Label: Neural Networks
Paper 6:  <Title: Improving RBF Networks by the Feature Selection Approach EUBAFES  .   Abstract: The curse dimensionality concerning the application RBF networks The number RBF nodes and therefore training examples needed grows the intrinsic dimensionality the input space One way address the application feature selection as a data preprocessing step In this paper an optimal feature subset First reduced those best discrimination properties by the application fast and robust f>
Label: Neural Networks
Paper 10:  <Title: Effects of Occam's Razor in Evolving Sigma-Pi Neural Nets  .   Abstract: Several evolutionary algorithms make hierarchical representations variable size rather linear strings Variable complexity the structures provides an additional representational power which widen evolutionary algorithms The price, however that the search space open- solutions grow arbitrarily large size In study structural complexity the solutions their generalization performance analyzing the fitness landscape sigma-pi neural networks The analys>
Label: Genetic Algorithms
Paper 4:  <Title: Feature Selection by Means of a Feature Weighting Approach  .   Abstract: Selecting features which optimal a given classification task We address using the flexible and robust filter technique EUBAFES EUBAFES based a feature weighting approach binary feature weights therefore a solution the feature selection sense and gives feature relevance by continuous weights Moreover the user gets not one several potentially optimal feature subsets filter-based feature selection algorithms since i>
Paper 5:  <Title: MLC Tutorial A Machine Learning library of C classes.  .   Abstract: Several evolutionary algorithms make hierarchical representations variable size rather linear strings Variable complexity the structures provides an additional representational power which widen evolutionary algorithms The price, however that the search space open- solutions grow arbitrarily large size In study structural complexity the solutions their generalization performance analyzing the fitness landscape sigma-pi neural networks The analys>
Paper 7:  <Title: Analysis of Decision Boundaries Generated by Constructive Neural Network Learning Algorithms  .   Abstract: Constructive learning algorithms offer incremental construction near-minimal artificial neural networks for pattern classification Examples such algorithms Tower Pyramid Upstart Tiling algorithms which construct threshold logic unitsor, These algorithms differ terms construct which in biases a decision boundary correctly classifies the training set This paper such algorithms from>
Paper 8:  <Title: Adaptive Noise Injection for Input Variables Relevance Determination  .   Abstract: In consider training with noise to input variables relevance determination Noise injection modified penalize irrelevant features The proposed algorithm attractive requires the tuning This parameter controls the penalization the inputs together the complexity After the presentation the method experimental evidences given simulated data sets>
Paper 9:  <Title: Sample Complexity for Learning Recurrent Perceptron Mappings  .   Abstract: Recurrent perceptron classifiers They take into those correlations dependences input coordinates which arise linear digital filtering This paper tight bounds sample complexity associated to the fitting>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Neural Networks

Neural Networks
Prediction:  Neural Networks
Is prediction correct?  True

Prediction: 1
Processing index 1643...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Learning to coordinate without sharing information  
Abstract: Abstract: Researchers in the field of Distributed Artificial Intelligence (DAI) have been developing efficient mechanisms to coordinate the activities of multiple autonomous agents. The need for coordination arises because agents have to share resources and expertise required to achieve their goals. Previous work in the area includes using sophisticated information exchange protocols, investigating heuristics for negotiation, and developing formal models of possibilities of conflict and cooperation among agent interests. In order to handle the changing requirements of continuous and dynamic environments, we propose learning as a means to provide additional possibilities for effective coordination. We use reinforcement learning techniques on a block pushing problem to show that agents can learn complimentary policies to follow a desired path without any knowledge about each other. We theoretically analyze and experimentally verify the effects of learning rate on system convergence, and demonstrate benefits of using learned coordination knowledge on similar problems. Reinforcement learning based coordination can be achieved in both cooperative and non-cooperative domains, and in domains with noisy communication channels and other stochastic characteristics that present a formidable challenge to using other coordination schemes. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 4:  <Title: Scaling Reinforcement Learning Algorithms by Learning Variable Temporal Resolution Models  .   Abstract: The close connection reinforcement dynamic programming algorithms fueled research RL within Yet increased theoretical understanding RL algorithms applicable simple tasks only In the abstract framework afforded the connection dynamic programming the scaling issues faced RL researchers I focus learning agents to solve multiple structured RL tasks I propose learning abstract environment models where abst>
Label: Reinforcement Learning
Paper 2:  <Title: Reinforcement Learning with Hierarchies of Machines  .   Abstract: We present reinforcement learning the policies considered constrained hierarchies partially specified machines This allows prior knowledge reduce the search space provides in transferred problems in component solutions recombined Our approach can providing a link reinforcement learning behavior- or teleo-reactive approaches control. We present provably alg>
Paper 3:  <Title: Reinforcement Learning with Imitation in Heterogeneous Multi-Agent Systems  .   Abstract: The application decision making learning algorithms presents many interestingresearch challenges Among these the ability agents learn act by or We describe, the IQ-algorithm integrates imitation, a Q-learner uses the observations it an expert agent bias its exploration in promising directions This algorithm goes previous work relaxing the oft-made assumptions (observer the expe>
Paper 5:  <Title: Modeling the Student with Reinforcement Learning  .   Abstract: We describe a methodology enabling an intelligent teaching system make high level strategy decisions on low level student modeling information This framework less costly construct superior hand coding teaching strategies as responsive In order accomplish reinforcement learning learn associate superior teaching actions certain states the student's knowledge Reinforcement learning (RL flexible handling noisy data does need expert domain knowledge A dr>
Paper 6:  <Title: Evolving Cooperation Strategies  .   Abstract: The identification, design implementation strategies cooperation is a central research issue Distributed Artificial Intelligence We propose the construction cooperation strategies a group problem solvers based Genetic Programming (GP GPs a class adaptive algorithms used evolve solution structures optimize a given evaluation criterion Our approach designing a representation cooperation strategies manipulated GPs present results experiments the p>
Paper 7:  <Title: Learning in Multi-Robot Systems  .   Abstract: This paper 1 discusses why traditional reinforcement learning methods algorithms applied those models result dynamic situated multi-agent domains characterized multiple goals noisy perception action inconsistent reinforcement We propose designing the representation and the forcement functions take advantage implicit domain knowledge accelerate demonstrate it two different mobile robot domains>
Paper 8:  <Title: Learning Hierarchical Control Structures for Multiple Tasks and Changing Environments  .   Abstract: While the need hierarchies within control systems apparent many researchers should learned Learning both the structure the component behaviors is The benefit learning the hierarchical structures behaviors the decomposition the control structure smaller transportable chunks previously knowledge new but related tasks Presented this paper improvements Nested Q-learningNQL that allow more realistic learning control hierarchies reinforcement>
Paper 9:  <Title: CABINS A Framework of Knowledge Acquisition and Iterative Revision for Schedule Improvement and Reactive Repair  .   Abstract: Mixed-initiative systems present the challenge finding an effective level interaction humans computers Machine learning presents in systems automatically adapt accommodate different users In learning user models an adaptive assistant crisis scheduling We describe the problem domain the scheduling assistant then present an initial formulation the adaptive assistant's learning task a baseline study After this re>
Paper 10:  <Title: REINFORCEMENT LEARNING FOR COORDINATED REACTIVE CONTROL  .   Abstract: The demands rapid response the complexity many environments decompose, tune coordinate reactive behaviors while ensuring consistency Reinforcement learning networks address the tuning problem do decomposition coordination We hypothesize interacting reactions can often decomposed separate control tasks resident separate networks coordinated the tuning mechanism a higher level controller To explore implemented a reinforcement le>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Reinforcement Learning

Reinforcement Learning
Prediction:  Reinforcement Learning
Is prediction correct?  True

Prediction: 1
Processing index 469...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Interpolation Models with Multiple  
Abstract: Abstract: A traditional interpolation model is characterized by the choice of reg-ularizer applied to the interpolant, and the choice of noise model. Typically, the regularizer has a single regularization constant ff, and the noise model has a single parameter fi. The ratio ff=fi alone is responsible for determining globally all these attributes of the interpolant: its `complexity', `flexibility', `smoothness', `characteristic scale length', and `characteristic amplitude'. We suggest that interpolation models should be able to capture more than just one flavour of simplicity and complexity. We describe Bayesian models in which the interpolant has a smoothness that varies spatially. We emphasize the importance, in practical implementation, of the concept of `conditional convexity' when designing models with many hyperparameters. We apply the new models to the interpolation of neuronal spike data and demonstrate a substantial improvement in generalization error. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 6:  <Title: Wavelet Thresholding via a Bayesian Approach  .   Abstract: We discuss a Bayesian formalism gives wavelet threshold estimation A prior distribution imposed the wavelet coefficients the unknown response function designed capture common most applications For prior specified, the posterior median yields Our prior model the underlying function can adjusted give functions falling any specific Besov space We establish a relation the prior model tho>
Label: Probabilistic Methods
Paper 8:  <Title: Choice of Thresholds for Wavelet Shrinkage Estimate of the Spectrum fff j g are level-dependent.   Abstract: We study the problem estimating the log spectrum a stationary Gaussian time series thresholding the empirical wavelet coefficients We propose thresholds t j;n depending n wavelet basis resolution level At fine resolution levelsj =; 2 :: propose The purpose this thresholding level the reconstructed log-spectrum as nearly noise In pleasant from leads attractive theoretical properties over smoothness assumptions Pre>
Label: Probabilistic Methods
Paper 2:  <Title: A Practical Bayesian Framework for Backprop Networks  .   Abstract: A quantitative and practical Bayesian framework is described learning of mappings feedforward networks The framework makes possible: (1) objective comparisons solutions using alternative network architectures objective stopping rules network pruning or growing procedures magnitude weight decay terms or additive regularisers penalising large weights etc well-determined parameters quantified estimates on network parameters on network outp>
Paper 3:  <Title: Tempering Backpropagation Networks: Not All Weights are Created Equal approach yields hitherto unparalleled performance on.   Abstract: Backpropagation learning algorithms typically collapse the network's structure weight parameters to optimized We suggest their performance utilizing the structural information instead introduce a framework tempering each weight accordingly In the tempering model activation error signals treated approximately independent random variables The characteristic scale weight changes then matched that allowing structural properties a node's fan-in affe>
Paper 4:  <Title: Bayesian Non-linear Modelling for the Prediction Competition  .   Abstract: The 1993 energy prediction competition involved the prediction a series building energy loads from environmental input variables Non-linear regression using `neural networks such modeling tasks Since it obvious large a time-window of inputs appropriate or preprocessing of best can viewed a regression problem in actually irrelevant Because a finite data will show random correlations b>
Paper 5:  <Title: From Isolation to Cooperation: An Alternative View of a System of Experts  .   Abstract: We introduce a constructive, incremental learning system regression problems models data by means locally linear experts contrast other approaches the experts trained independently compete data during learning Only when a prediction a query required the experts cooperate by blen ding their individual predictions Each expert minimizing a penalized local cross validation error using second order methods an expert adjust its predictions valid>
Paper 7:  <Title: Function Approximation with Neural Networks and Local Methods: Bias, Variance and Smoothness  .   Abstract: We review global and local methods a function mapping R m ) R n from samples the function containing noise The relationship the methods is examined an empirical comparison a linear local approximation the following commonly used datasets the Mackey-Glass chaotic time series British English Vowel data TIMIT speech phonemes building energy prediction data We fin>
Paper 9:  <Title: Local Feedforward Networks  .   Abstract: Although feedforward neural networks well function approximation in some applications networks experience a desired function One problem interference which learning in the input space causes unlearning Networks less susceptible interference referred spatially local networks To these properties a theoretical framework, consisting a measure interference network localization developed that not only the network weights architect>
Paper 10:  <Title: State Reconstruction for Determining Predictability in Driven Nonlinear Acoustical Systems  .   Abstract: Genetic programming distinguished other evolutionary algorithms tree representations variable size instead linear strings The flexible representation scheme very the underlying structure discovered automatically One primary difficulty, the solutions may grow too without any improvement of their generalization ability In this paper investigate the fundamental relationship the performance complexity the evolved structures. The essence the parsimony>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Neural Networks

Probabilistic Methods
Prediction:  Probabilistic Methods
Is prediction correct?  False

Prediction: 0
Processing index 2686...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Penalisation multiple adaptative un nouvel algorithme de regression, la penalisation multiple adapta-tive. Cet algorithme represente
Abstract: Abstract: Chaque parametre du modele est penalise individuellement. Le reglage de ces penalisations se fait automatiquement a partir de la definition d'un hyperparametre de regularisation globale. Cet hyperparametre, qui controle la complexite du regresseur, peut ^etre estime par des techniques de reechantillonnage. Nous montrons experimentalement les performances et la stabilite de la penalisation multiple adaptative dans le cadre de la regression lineaire. Nous avons choisi des problemes pour lesquels le probleme du controle de la complexite est particulierement crucial, comme dans le cadre plus general de l'estimation fonctionnelle. Les comparaisons avec les moindres carres regularises et la selection de variables nous permettent de deduire les conditions d'application de chaque algorithme de penalisation. Lors des simulations, nous testons egalement plusieurs techniques de reechantillonnage. Ces techniques sont utilisees pour selectionner la complexite optimale des estimateurs de la fonction de regression. Nous comparons les pertes occasionnees par chacune d'entre elles lors de la selection de modeles sous-optimaux. Nous regardons egalement si elles permettent de determiner l'estimateur de la fonction de regression minimisant l'erreur en generalisation parmi les differentes methodes de penalisation en competition. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 3:  <Title: Model Selection for Generalized Linear Models via GLIB, with Application to Epidemiology 1  .   Abstract: 1 This the first draft for Bayesian Biostatistics edited Donald A. Berry and Darlene K. Strangl. Adrian E. Raftery Professor Statistics Department GN-22 Washington Sylvia Richardson Directeur de Recherche, INSERM Unite 170 16 avenue Paul Vaillant Couturier 94807 Villejuif CEDEX France Raftery's research was supported ONR contract no N-00014-91-J-1074, by by the Universite de Paris VI and INRIA Rocquencourt France Rafter>
Label: Probabilistic Methods
Paper 7:  <Title: Adaptive Boosting of Neural Networks for Character Recognition  .   Abstract: Technical Report1072 D epartement erationnelle Universit e Montr eal Abstract Boosting any learning algorithm consistently classifiers which need only slightly random guessing A recently proposed and very promising boosting algorithm AdaBoost [5 It applied with several benchmark machine learning problems using rather simple learning algorithms [ in1 6 In AdaBoost impro>
Label: Neural Networks
Paper 2:  <Title: ALECSYS and the AutonoMouse: Learning to Control a Real Robot by Distributed Classifier Systems  .   Abstract: Chaque parametre du modele est penalise individuellement Le reglage de ces penalisations se a definition d'un hyperparametre de regularisation globale Cet hyperparametre qui controle la complexite du regresseur ^etre estime par des techniques de reechantillonnage Nous montrons experimentalement les performances et la stabilite de la penalisation multiple adaptative dans regression lineaire Nous avons choisi le probleme du controle de la complexite est particulierement>
Paper 4:  <Title: A DISCUSSION ON SOME DESIGN PRINCIPLES FOR EFFICIENT CROSSOVER OPERATORS FOR GRAPH COLORING PROBLEMS  .   Abstract: A year a new metaheuristic for graph coloring problems Costa Hertz Dubuis They shown, computer experiments some clear indication the benefits this approach Graph coloring has many applications specially the areas scheduling assignments timetabling The metaheuristic can classified a memetic algorithm since a population search in periods local optimization phases new configurations from earlier well-developed configurations or local minima>
Paper 5:  <Title: GAL: Networks that grow when they learn and shrink when they forget  .   Abstract: Learning when limited modification some parameters has a limited scope; the capability the system structure also needed get the learnable. In artificial neural networks learning by iterative adjustment only succeed the network designer predefines hidden layers units the size their receptive and projective fields This paper advocates the network structure, usually determined trial but shou>
Paper 6:  <Title: GREQE a Diplome des Etudes Approfondies en Economie Mathematique et Econometrie A Genetic Algorithm for.   Abstract: The purpose propose a refinement innateness If we merely identify innateness bias obtain a poor characterisation this notion any learning device relies makes choose a given hypothesis instead another We show our intuition innateness better captured a characteristic bias related isotropy Generalist models learning shown rely an isotropic bias whereas the bias specialised models, include what to necessa>
Paper 8:  <Title: Stochastic Hillclimbing as a Baseline Method for Evaluating Genetic Algorithms  .   Abstract: We investigate as a baseline evaluating genetic algorithms (GAs as combinatorial function optimizers In particular address four problems to GAs applied the maximum cut problem, Koza's 11-multiplexer problem MDAP the jobshop problem We demonstrate simple stochastic hillclimbing methods results comparable or obtained the GAs designed address these four problems We further il>
Paper 9:  <Title: Wavelet Shrinkage: Asymptopia?  .   Abstract: Considerable effort directed recently minimax methods in problems recovering infinite-dimensional objectscurves densities spectral densities images noisy data A rich and complex body evolved, nearly- or minimax estimators being obtained interesting problems Unfortunately the results often not translated practice for sometimes similarity known methods sometimes, computational intractability spatial adaptivity We discuss a met>
Paper 10:  <Title: TABLE DES MATI ERES 1 Apprentissage et approximation les techniques de regularisation 3 1.1 Introduction.   Abstract: The paper investigates using simple recurrent networks transducers which map sequential natural language input The networks perform sentences containing a single main predicateencoded applied multiple-feature objects with adjectival modifiers shows against A second set experiments deals sentences containing embedded structures Here the network able process multiple levels sentence-fina>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Neural Networks

Probabilistic Methods
Prediction:  Probabilistic Methods
Is prediction correct?  False

Prediction: 0
Processing index 417...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Constructing Intermediate Concepts by Decomposition of Real Functions  
Abstract: Abstract: In learning from examples it is often useful to expand an attribute-vector representation by intermediate concepts. The usual advantage of such structuring of the learning problem is that it makes the learning easier and improves the comprehensibility of induced descriptions. In this paper, we develop a technique for discovering useful intermediate concepts when both the class and the attributes are real-valued. The technique is based on a decomposition method originally developed for the design of switching circuits and recently extended to handle incompletely specified multi-valued functions. It was also applied to machine learning tasks. In this paper, we introduce modifications, needed to decompose real functions and to present them in symbolic form. The method is evaluated on a number of test functions. The results show that the method correctly decomposes fairly complex functions. The decomposition hierarchy does not depend on a given repertoir of basic functions (background knowledge). 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 4:  <Title: Evolving Compact Solutions in Genetic Programming: A Case Study  .   Abstract: Genetic programmingGP where handled trees This makes GP especially evolving functional relationships or computer programs both represented trees Symbolic regression the determination a function dependence y = g(x that i ; In the feasibility symbolic regression with GP is on taken different domains Furthermore several suggested methods from literature compared that intended GP perfor>
Label: Genetic Algorithms
Paper 5:  <Title: Hidden Markov Model Analysis of Motifs in Steroid Dehydrogenases and their Homologs  .   Abstract: Methods to build function approximators example data have gained Especially methodologies build models allow an interpretation have attracted Most existing algorithms however either complicated high-dimensional problems presents efficient algorithm construct fuzzy graphs example data The resulting fuzzy graphs based locally independent fuzzy rules operate solely selected, important attributes This enables these fuzzy>
Label: Neural Networks
Paper 9:  <Title: Structural Regression Trees  .   Abstract: In many real domains the task machine learning algorithms a theory predicting numerical values In particular several standard test domains used Inductive Logic Programming concerned predicting numerical values examples relational and mostly non-determinate background knowledge However so far no ILP algorithm except can predict numbers cope non-determinate background knowledge (The only exception a covering algorithm FORS In present Structural Regression Trees>
Label: Rule Learning
Paper 2:  <Title: FONN: Combining First Order Logic with Connectionist Learning  .   Abstract: This paper manage structured data refine knowledge bases expressed a first order logic language The presented framework well classification problems concept de scriptions depend numerical features In fact the main goal the neural architecture that refining the numerical part without changing In particular discuss a method translate a set classification rules neural computation units Here, focus the translat>
Paper 3:  <Title: Machine Learning by Function Decomposition  .   Abstract: We present, given training examples induces a definition a hierarchy intermediate concepts their definitions This effectively decomposes The method inspired the Boolean function decomposition approach to the design digital circuits To cope high time complexity finding an optimal decomposition propose The method, implemented program HINTHIerarchy Induction Tool experimentally evaluated>
Paper 6:  <Title: Using Qualitative Models to Guide Inductive Learning  .   Abstract: This paper using qualitative models guide inductive learning Our objectives induce rules which not accurate explainable with respect the qualitative model reduce learning time exploiting domain knowledge Such explainability essential both practical application inductive technology integrating the results learning back We apply two process control problems a water tank network an ore grinding process used in>
Paper 7:  <Title: First Order Regression: Applications in Real-World Domains  .   Abstract: A first order regression algorithm capable handling introduced and some Regressional learning assumes real-valued class The algorithm combines regressional learning standard ILP concepts first order concept description background knowledge A clause is generated successively refining literals A = v for the discrete attributes and for background knowledge literals to>
Paper 8:  <Title: Growing a Hypercubical Output Space in a Self-Organizing Feature Map  .   Abstract: Recent studies on planning comparing plan reuse plan generation both the above tasks may computational complexity even we deal very similar problems The aim the same kind results apply also diagnosis. We propose a theoretical complexity analysis coupled some experimental tests intended evaluate adaptation strategies which reuse the solutions past diagnostic problems build a solution to Results such analysis even>
Paper 10:  <Title: Constructing Fuzzy Graphs from Examples  .   Abstract: Methods to build function approximators example data have gained Especially methodologies build models allow an interpretation have attracted Most existing algorithms however either complicated high-dimensional problems presents efficient algorithm construct fuzzy graphs example data The resulting fuzzy graphs based locally independent fuzzy rules operate solely selected, important attributes This enables these fuzzy>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Rule Learning

Genetic Algorithms
Prediction:  Genetic Algorithms
Is prediction correct?  False

Prediction: 0
Processing index 2489...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: BECOMING AN EXPERT CASE-BASED REASONER: LEARNING TO ADAPT PRIOR CASES  
Abstract: Abstract: Experience plays an important role in the development of human expertise. One computational model of how experience affects expertise is provided by research on case-based reasoning, which examines how stored cases encapsulating traces of specific prior problem-solving episodes can be retrieved and re-applied to facilitate new problem-solving. Much progress has been made in methods for accessing relevant cases, and case-based reasoning is receiving wide acceptance both as a technology for developing intelligent systems and as a cognitive model of a human reasoning process. However, one important aspect of case-based reasoning remains poorly understood: the process by which retrieved cases are adapted to fit new situations. The difficulty of encoding effective adaptation rules by hand is widely recognized as a serious impediment to the development of fully autonomous case-based reasoning systems. Consequently, an important question is how case-based reasoning systems might learn to improve their expertise at case adaptation. We present a framework for acquiring this expertise by using a combination of general adaptation rules, introspective reasoning, and case-based reasoning about the case adaptation task itself. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 2:  <Title: Computer Evolution of Buildable Objects for Evolutionary Design by Computers  .   Abstract: Experience plays human expertise One computational model how experience expertise provided research case-based reasoning examines how stored cases encapsulating traces specific prior problem-solving episodes retrieved re facilitate new problem Much progress methods accessing relevant cases case-based reasoning receiving wide acceptance both as a technology developing intelligent systems a cognitive model a human reasoning process However o>
Label: Genetic Algorithms
Paper 3:  <Title: Combining Rules and Cases to Learn Case Adaptation  .   Abstract: Computer models case-based reasoning generally guide case adaptation using adaptation rules A difficult practical problem identify the knowledge guide adaptation particular tasks Likewise an open issue CBR as a cognitive model case adaptation knowledge We describe acquiring case adaptation knowledge In this approach adaptation problems initially reasoning scratch abstract rules structural transformations general memory search heuristics Traces the pr>
Label: Case Based
Paper 7:  <Title: NESTED NETWORKS FOR ROBOT CONTROL  .   Abstract: Case-based reasoning depends multiple knowledge sources beyond the case library knowledge case adaptation criteria similarity assessment Because hand coding this knowledge accounts the knowledge acquisition burden developing CBR systems appealing acquire CBR apply This observation developing case-based CBR systems whose components themselves use. However despite early interest case-based approaches CBR this method compara>
Label: Neural Networks
Paper 8:  <Title: A Case Study of Case-Based CBR  .   Abstract: Case-based reasoning depends multiple knowledge sources beyond the case library knowledge case adaptation criteria similarity assessment Because hand coding this knowledge accounts the knowledge acquisition burden developing CBR systems appealing acquire CBR apply This observation developing case-based CBR systems whose components themselves use. However despite early interest case-based approaches CBR this method compara>
Label: Case Based
Paper 9:  <Title: Case-Based Similarity Assessment: Estimating Adaptability from Experience  .   Abstract: Case-based problem-solving systems rely similarity assessment select stored cases whose solutions easily fit current problems However widely-used similarity assessment strategies evaluation semantic similarity adaptability As systems may select cases difficult adapt even easily adaptable cases in memory This paper a new similarity assessment approach which couples similarity judgments directly a case library containing the system's adaptation>
Label: Case Based
Paper 10:  <Title: Meta-Cases: Explaining Case-Based Reasoning  .   Abstract: AI research on case-based reasoning led many laboratory case-based systems As introducing these systems work environments explaining the processes case-based reasoning In this paper the notion a meta-case illustrating, explaining justifying case-based reasoning A meta-case contains a trace the processing in a problem-solving episode provides a (partial) justification The language for repr>
Label: Case Based
Paper 4:  <Title: Learning to Improve Case Adaptation by Introspective Reasoning and CBR  .   Abstract: In current CBR systems case adaptation usually rule-based methods hand The ability define those rules depends knowledge the task domain may not a presenting endowing CBR systems the needed adaptation knowledge This paper ongoing research a method address acquiring adaptation knowledge experience The method uses reasoning from scratch based introspective reasoning the requirements successfu>
Paper 5:  <Title: Acquiring Case Adaptation Knowledge: A Hybrid Approach  .   Abstract: The ability case-based reasoning (CBR) systems apply cases novel situations depends their case adaptation knowledge However endowing CBR systems adequate adaptation knowledge has This paper a hybrid method performing case adaptation, It shows this approach provides acquiring flexible adaptation knowledge experiences with autonomous adaptation suggests its potential acquisition adaptation knowledge interact>
Paper 6:  <Title: Learning Adaptation Strategies by Introspective Reasoning about Memory Search  .   Abstract: In case-based reasoning systems the case adaptation process traditionally controlled static libraries hand-coded adaptation rules This paper learning adaptation knowledge adaptation strategies of developed and hand Kass [90 Adaptation strategies differ standard adaptation rules encode general memory search procedures finding during case adaptation; this paper the issues learning to form>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Case Based

Case Based
Prediction:  Case Based
Is prediction correct?  True

Prediction: 1
Processing index 2331...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Improved Hoeffding-Style Performance Guarantees for Accurate Classifiers  
Abstract: Abstract: We extend Hoeffding bounds to develop superior probabilistic performance guarantees for accurate classifiers. The original Hoeffding bounds on classifier accuracy depend on the accuracy itself as a parameter. Since the accuracy is not known a priori, the parameter value that gives the weakest bounds is used. We present a method that loosely bounds the accuracy using the old method and uses the loose bound as an improved parameter value for tighter bounds. We show how to use the bounds in practice, and we generalize the bounds for individual classifiers to form uniform bounds over multiple classifiers. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 2:  <Title: Improved Uniform Test Error Bounds  .   Abstract: We derive distribution-free uniform test error bounds improve VC-type bounds validation We show knowledge test inputs the bounds. The bounds sharp require intense computation We introduce trade sharpness speed computation Also compute the bounds several test cases>
Label: Theory
Paper 3:  <Title: Validation of Voting Committees  .   Abstract: This paper contains bound the test errors voting committees members chosen trained classifiers There so many prospective committees validating them directly does achieve useful error bounds Because fewer classifiers prospective committees validate individually linear programming infer committee error bounds We test the method credit card data Also extend the method infer bounds classifiers general>
Label: Theory
Paper 4:  <Title: Self bounding learning algorithms  .   Abstract: Most which attempts give bounds the generalization error the hypothesis generated a learning algorithm methods from uniform convergence These bounds a-priori bounds hold any distribution examples calculated before observed In this paper bounding the generalization error after the data has observed A self-bounding learning algorithm, addition the hypothesis outputs, outputs reliable upper the generalization err>
Label: Theory
Paper 5:  <Title: Boosting a weak learning algorithm by majority To be published in Information and Computation  .   Abstract: We present an algorithm improving algorithms learning binary concepts The improvement achieved combining hypotheses each generated training given learning examples Our algorithm ideas presented Schapire inThe strength weak learnability represents an improvement his results The analysis our algorithm provides general upper bounds the resources required learning in Valiant's polynomial PAC learning framework are the best genera>
Label: Theory
Paper 6:  <Title: Using Qualitative Relationships for Bounding Probability Distributions  .   Abstract: We exploit qualitative probabilistic relationships variables computing bounds interest Using the signs qualitative relationships implement abstraction operations guaranteed bound the distributions interest By evaluating incrementally improved approximate networks our algorithm tightening bounds converge exact distributions For supermodular utility functions the tightening bounds monotonically admissible deci>
Label: Probabilistic Methods
Paper 7:  <Title: Anytime Influence Diagrams  .   Abstract: In for feedforward neural networks consistent. This paper extends earlier results universal approximation properties The proof consistency embeds a density estimation problem then uses bounds the bracketing entropy show posterior is Hellinger neighborhoods It then relates this result back the regression setting We show consistency both the setting the number hidden nodes growing>
Label: Probabilistic Methods
Paper 9:  <Title: UNIVERSAL FORMULAS FOR TREATMENT EFFECTS FROM NONCOMPLIANCE DATA  .   Abstract: This paper establishes formulas bound the actual treatment effect any experimental study treatment assignment random but subject compliance imperfect These formulas provide the tightest bounds the average treatment effect inferred given assignments treatments responses Our results reveal even high rates noncompliance experimental data can yield significant and sometimes accurate information a treatment the population>
Label: Probabilistic Methods
Paper 8:  <Title: An Efficient Extension to Mixture Techniques for Prediction and Decision Trees  .   Abstract: We present maintaining mixtures prunings a prediction or decision tree that extends the "node-based" prunings [Bun90, WST95 HS97 to the larger class The method includes an efficient online weight allocation prediction compression classification Although the set edge-based prunings of a given tree much our algorithm has similar space previous mixture algorithms for trees Using general on framework Freund and Sch>
Paper 10:  <Title: A Fast, Bottom-Up Decision Tree Pruning Algorithm with Near-Optimal Generalization  .   Abstract: In this work present a new bottom-up algorithm decision tree pruning very (requiring prove a strong performance guarantee the generalization error We work the typical setting in the given tree T may derived the given training sample S may badly overfit In this setting give bounds additional generalization error our pruning suffers compared of T. More generally our results if there>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Theory

Theory
Prediction:  Theory
Is prediction correct?  True

Prediction: 1
Processing index 1827...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Efficient Algorithms for Inverting Evolution  
Abstract: Abstract: Evolution is a stochastic process which operates on the DNA of species. The evolutionary process leaves tell-tale signs in the DNA which can be used to construct phylogenies, or evolutionary trees, for a set of species. Maximum Likelihood Estimations (MLE) methods seek the evolutionary tree which is most likely to have produced the DNA under consideration. While these methods are widely accepted and intellectually satisfying, they have been computationally intractable. In this paper, we address the intractability of MLE methods as follows. We introduce a metric on stochastic process models of evolution. We show that this metric is meaningful by proving that in order for any algorithm to distinguish between two stochatic models that are close according to this metric, it needs to be given a lot of observations. We complement this result with a simple and efficient algorithm for inverting the stochastic process of evolution, that is, for building the tree from observations on the DNA of the species. Our result can be viewed as a result on the PAC-learnability of the class of distributions produced by tree-like processes. Though there have been many heuristics suggested for this problem, our algorithm is the first one with a guaranteed convergence rate, and further, this rate is within a polynomial of the lower-bound rate we establish. Ours is also the the first polynomial-time algorithm which is guaranteed to converge at all to the correct tree. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 3:  <Title: On the Sample Complexity of Learning Bayesian Networks  .   Abstract: In there learning Bayesian networks data One learning such networks based the minimum description length principle Previous work this learning procedure successful: with probability one, it will converge the target distribution given samples However the rate this convergence hitherto In this work examine the sample complexity MDL based learning procedures We show nu>
Label: Probabilistic Methods
Paper 5:  <Title: Learning Bayesian Prototype Trees by Simulated Annealing  .   Abstract: Given a set samples an unknown probability distribution study the problem constructing a good approximative Bayesian network model in question This task can viewed a search problem the goal a maximal probability network model given In this work do make learn arbitrarily complex multi-connected Bayesian network structures since such resulting models unsuitable practical purposes due the exponential amount time the reasoning task Instead restrict ou>
Label: Probabilistic Methods
Paper 9:  <Title: Self bounding learning algorithms  .   Abstract: Most which attempts give bounds the generalization error the hypothesis generated a learning algorithm methods from uniform convergence These bounds a-priori bounds hold any distribution examples calculated before observed In this paper bounding the generalization error after the data has observed A self-bounding learning algorithm, addition the hypothesis outputs, outputs reliable upper the generalization err>
Label: Theory
Paper 10:  <Title: Learning a set of primitive actions with an Induction of decision trees. Machine Learning, 1(1):81-106,.   Abstract: Although probabilistic inference in a general Bayesian belief network inference computation time reduced most practical cases exploiting domain knowledge by making the knowledge representation In the property similarity states a new method approximate knowledge representation which We define two or more states a node to similar when the likelihood ratio their probabilities does depend the instantiations netwo>
Label: Theory
Paper 2:  <Title: A Fast Algorithm for the Computation and Enumeration of Perfect Phylogenies  .   Abstract: The Perfect Phylogeny Problem a classical problem computational evolutionary biology species/taxa described qualitative characters In the problem shown NP-Complete in general while the different fixed parameter versions each solved In particular Agarwala Fernandez-Baca developed an O(2 3r (nk 3 +k algorithm the perfect phylogeny problem n species defined k r-state characters Since commonly the character data drawn alignments molecular sequences k>
Paper 4:  <Title: Theory and Applications of Agnostic PAC-Learning with Small Decision Trees  .   Abstract: We exhibit a theoretically founded algorithm T2 agnostic PAC-learning of decision trees of at most 2 levels whose computation time almost linear the size We evaluate this learning algorithm T2 on 15 common real-world datasets show for most provides simple decision trees predictive power ( C4.5 In fact for datasets continuous attributes its error rate tends C4.5 To the PAC-learning>
Paper 6:  <Title: On the Learnability and Usage of Acyclic Probabilistic Finite Automata  .   Abstract: We propose and analyze a distribution learning a subclass This subclass characterized a certain distinguishability property the automata's states Though hardness results are known learning distributions generated general APFAs prove our algorithm efficiently the subclass we consider In particular show the KL-divergence the distribution generated the target source our hypothesis made arbitra>
Paper 7:  <Title: Constructing Computationally Efficient Bayesian Models via Unsupervised Clustering  Probabilistic Reasoning and Bayesian Belief Networks,  .   Abstract: Given a set samples an unknown probability distribution study the problem constructing a good approximative Bayesian network model in question This task can viewed a search problem the goal a maximal probability network model given In this work do make learn arbitrarily complex multi-connected Bayesian network structures since such resulting models unsuitable practical purposes due the exponential amount time the reasoning task Instead restrict ou>
Paper 8:  <Title: A Fast, Bottom-Up Decision Tree Pruning Algorithm with Near-Optimal Generalization  .   Abstract: In this work present a new bottom-up algorithm decision tree pruning very (requiring prove a strong performance guarantee the generalization error We work the typical setting in the given tree T may derived the given training sample S may badly overfit In this setting give bounds additional generalization error our pruning suffers compared of T. More generally our results if there>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Theory

Probabilistic Methods
Prediction:  Probabilistic Methods
Is prediction correct?  False

Prediction: 0
Processing index 1534...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: The Use of Explicit Goals for Knowledge to Guide Inference and Learning  
Abstract: Abstract: Combinatorial explosion of inferences has always been a central problem in artificial intelligence. Although the inferences that can be drawn from a reasoner's knowledge and from available inputs is very large (potentially infinite), the inferential resources available to any reasoning system are limited. With limited inferential capacity and very many potential inferences, reasoners must somehow control the process of inference. Not all inferences are equally useful to a given reasoning system. Any reasoning system that has goals (or any form of a utility function) and acts based on its beliefs indirectly assigns utility to its beliefs. Given limits on the process of inference, and variation in the utility of inferences, it is clear that a reasoner ought to draw the inferences that will be most valuable to it. This paper presents an approach to this problem that makes the utility of a (potential) belief an explicit part of the inference process. The method is to generate explicit desires for knowledge. The question of focus of attention is thereby transformed into two related problems: How can explicit desires for knowledge be used to control inference and facilitate resource-constrained goal pursuit in general? and, Where do these desires for knowledge come from? We present a theory of knowledge goals, or desires for knowledge, and their use in the processes of understanding and learning. The theory is illustrated using two case studies, a natural language understanding program that learns by reading novel or unusual newspaper stories, and a differential diagnosis program that improves its accuracy with experience. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 4:  <Title: Introduction to the Special Section on Knowledge-Based Construction of Probabilistic and Decision Models (IEEE Transactions.   Abstract: Modeling techniques developed recently the AI and uncertain reasoning communities permit significantly more flexible specifications probabilistic knowledge Specifically, graphical decision-modeling formalisms|belief networks influence diagrams their variants|provide compact representation probabilistic relationships support inference algorithms automatically exploit the dependence structure [ These advances brought on a resurgence computational decision systems based normative theories belief preference>
Label: Probabilistic Methods
Paper 5:  <Title: Blocking Gibbs Sampling for Linkage Analysis in Large Pedigrees with Many Loops  .   Abstract: Learning is a fundamental component intelligence a key consideration designing cognitive architectures such Soar [ Laird 1986 This chapter considers the question what an appropriate general-purpose learning mechanism We interested mechanisms and reproduce the rich variety learning capabilities humans ranging learning perceptual-motor skills how ride highly cognitive tasks how play Research on learning in fields artificial intel>
Label: Probabilistic Methods
Paper 6:  <Title: Representing Self-knowledge for Introspection about Memory Search  .   Abstract: This position paper sketches modeling introspective reasoning discusses that framework modeling about memory search It argues effective and flexible memory processing in rich memories should built five types explicitly represented self-knowledge knowledge information needs relationships different types expectations the actual behavior the information search process desires its ideal behavior representations how those expectations it>
Label: Case Based
Paper 7:  <Title: Learning Analytically and Inductively  .   Abstract: Learning is a fundamental component intelligence a key consideration designing cognitive architectures such Soar [ Laird 1986 This chapter considers the question what an appropriate general-purpose learning mechanism We interested mechanisms and reproduce the rich variety learning capabilities humans ranging learning perceptual-motor skills how ride highly cognitive tasks how play Research on learning in fields artificial intel>
Label: Reinforcement Learning
Paper 8:  <Title: On Decision-Theoretic Foundations for Defaults  .   Abstract: In considerable effort understanding default reasoning Most this effort concentrated the question entailment what conclusions warranted a knowledge-base defaults Surprisingly few works formally examine the general role defaults We argue an examination this role defaults suggest a concrete role defaults: simplify allowing fast, approximately optimal decisions by ignoring certain possible states In order formalize this ap>
Label: Probabilistic Methods
Paper 9:  <Title: Between MDPs and Semi-MDPs: Learning, Planning, and Representing Knowledge at Multiple Temporal Scales  .   Abstract: Learning, planning representing knowledge at temporal abstraction key challenges AI In this paper develop these problems based the mathematical framework reinforcement learning Markov decision processes We extend the usual notion action options|whole courses behavior may temporally extended stochastic contingent events Examples options include picking an object going lunch traveling a distant city, primitive actions muscle twitches joint tor>
Label: Reinforcement Learning
Paper 2:  <Title: A Goal-Based Approach to Intelligent Information Retrieval  .   Abstract: Intelligent information retrievalIIR requires inference The number inferences by even a simple reasoner very large the inferential resources any practical computer system This problem one long faced AI researchers In this paper used two recent machine learning programs for control inference that is relevant the design IIR systems The key feature the approach explicit representations desired knowledge we knowledge goals Our theory addresses the represent>
Paper 3:  <Title: Correcting Imperfect Domain Theories: A Knowledge-Level Analysis  .   Abstract: Explanation-Based Learning [Mitchell 1986 DeJong Mooney has shown promise a powerful analytical learning technique However EBL is severely the requirement a complete and correct domain theory for successful learning to occur Clearly in non-trivial domains developing such a domain theory Therefore much research devoted an imperfect domain theory corrected extended during system performance In this paper a characterization this problem use analyze>
Paper 10:  <Title: Adaptive probabilistic networks  .   Abstract: Belief networks (or probabilistic networks and neural networks two forms network representations intelligent systems in Belief networks provide a concise representation general probability distributions over facilitate exact calculation the impact evidence propositions of Neural networks represent parameterized algebraic combinations nonlinear activation functions found widespread use as models real neural systems as funct>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Case Based

Probabilistic Methods
Prediction:  Probabilistic Methods
Is prediction correct?  False

Prediction: 0
Processing index 1782...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Least-Squares Temporal Difference Learning  
Abstract: Abstract: Submitted to NIPS-98 TD() is a popular family of algorithms for approximate policy evaluation in large MDPs. TD() works by incrementally updating the value function after each observed transition. It has two major drawbacks: it makes inefficient use of data, and it requires the user to manually tune a stepsize schedule for good performance. For the case of linear value function approximations and = 0, the Least-Squares TD (LSTD) algorithm of Bradtke and Barto [5] eliminates all stepsize parameters and improves data efficiency. This paper extends Bradtke and Barto's work in three significant ways. First, it presents a simpler derivation of the LSTD algorithm. Second, it generalizes from = 0 to arbitrary values of ; at the extreme of = 1, the resulting algorithm is shown to be a practical formulation of supervised linear regression. Third, it presents a novel, intuitive interpretation of LSTD as a model-based reinforcement learning technique.
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 4:  <Title: INCREMENTAL POLYNOMIAL CONTROLLER NETWORKS: two self-organising non-linear controllers  .   Abstract: Temporal differenceTD) methods constitute learning predictions parameterized a recency factor. Currently the most important application these methods temporal credit assignment reinforcement learning Well known reinforcement learning algorithms AHC may viewed instances TD learning This paper the efficient and general implementation TD() arbitrary, for use reinforcement learning algorithms optimizing the discounted sum rewards The tradi>
Label: Neural Networks
Paper 5:  <Title: Adapting Bias by Gradient Descent: An Incremental Version of Delta-Bar-Delta  .   Abstract: Appropriate bias widely viewed efficient learning I present a new algorithm the Incremental Delta-Bar-DeltaIDBD the learning appropriate biases based previous learning experience The IDBD algorithm developed the case simple linear learning system|the LMS or delta rule with a separate learning-rate parameter The IDBD algorithm adjusts the learning-rate parameters an important form bias this system Because bias this approach adapted based previous learning experience>
Label: Neural Networks
Paper 7:  <Title: Gas Identification System using Graded Temperature Sensor and Neural Net Interpretation  .   Abstract: We present three new algorithms setting ff and, temporal-difference learning methods such TD The overall task that learning predict an unknown Markov chain based repeated observations its state trajectories The new algorithms select step-size parameters online normally temporal-difference methods We compare our algorithms Monte Carlo methods a natural way setting the step size for each state s they>
Label: Neural Networks
Paper 8:  <Title: On Step-Size and Bias in Temporal-Difference Learning  .   Abstract: We present three new algorithms setting ff and, temporal-difference learning methods such TD The overall task that learning predict an unknown Markov chain based repeated observations its state trajectories The new algorithms select step-size parameters online normally temporal-difference methods We compare our algorithms Monte Carlo methods a natural way setting the step size for each state s they>
Label: Reinforcement Learning
Paper 9:  <Title: PUSH-PULL SHUNTING MODEL OF GANGLION CELLS Simulations of X and Y retinal ganglion cell behavior.   Abstract: We present three new algorithms setting ff and, temporal-difference learning methods such TD The overall task that learning predict an unknown Markov chain based repeated observations its state trajectories The new algorithms select step-size parameters online normally temporal-difference methods We compare our algorithms Monte Carlo methods a natural way setting the step size for each state s they>
Label: Neural Networks
Paper 10:  <Title: An Upper Bound on the Loss from Approximate Optimal-Value Functions  .   Abstract: Many reinforcement learning (RL) approaches can formulated from Markov decision processes the associated method dynamic programming The value this theoretical understanding, tempered many practical concerns One important question DP-based approaches that function approximation rather lookup tables can avoid catastrophic effects This note presents a result in Bertsekas1987 guarantees small errors a task's optimal value function produce arbitrarily bad perf>
Label: Reinforcement Learning
Paper 2:  <Title: Truncating Temporal Differences: On the Efficient Implementation of TD() for Reinforcement Learning  .   Abstract: Temporal differenceTD) methods constitute learning predictions parameterized a recency factor. Currently the most important application these methods temporal credit assignment reinforcement learning Well known reinforcement learning algorithms AHC may viewed instances TD learning This paper the efficient and general implementation TD() arbitrary, for use reinforcement learning algorithms optimizing the discounted sum rewards The tradi>
Paper 3:  <Title: Toward Learning Systems That Integrate Different Strategies and Representations TR93-22  .   Abstract: Temporal differenceTD) methods constitute learning predictions parameterized a recency factor. Currently the most important application these methods temporal credit assignment reinforcement learning Well known reinforcement learning algorithms AHC may viewed instances TD learning This paper the efficient and general implementation TD() arbitrary, for use reinforcement learning algorithms optimizing the discounted sum rewards The tradi>
Paper 6:  <Title: Update rules for parameter estimation in Bayesian networks  .   Abstract: This paper re parameter estimation missing values hidden variables from the perspective recent work on learning [12 We provide a unified framework parameter estimation that encompasses on learning where continuously adapted new data cases as they arrive more traditional batch samples in In the batch case our framework encompasses both the gradient projection algorithm [ the EM algorith>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Reinforcement Learning

Reinforcement Learning
Prediction:  Reinforcement Learning
Is prediction correct?  True

Prediction: 1
Processing index 487...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Language as a dynamical system  
Abstract: Abstract: Designers across a variety of domains engage in many of the same creative activities. Since much creativity stems from using old solutions in novel ways, we believe that case-based reasoning can be used to explain many creative design processes. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 4:  <Title: Belief Networks Revisited  .   Abstract: Experiment design execution a central activity The SeqER system a general architecture the integration automated planning techniques domain knowledge in plan scientific experiments These planning techniques include rule-based methods, derivational analogy Derivational analogy allows planning experience captured as cases reused Analogy also allows the system function strong domain knowledge Cases efficiently flexibly retrieved a large case>
Label: Probabilistic Methods
Paper 10:  <Title: Grounding Robotic Control with Genetic Neural Networks  .   Abstract: Technical Report AI94223 May 1994 Abstract An important but often neglected problem grounding systems their environment such the representations manipulate have inherent meaning for Since humans rely semantics the grounding is crucial truly intelligent behavior This study investigates simulated robotic agents with neural network processors as a method ensure grounding. Both the topology weights opt>
Label: Genetic Algorithms
Paper 2:  <Title: CASE-BASED CREATIVE DESIGN  .   Abstract: Designers across a variety domains engage many Since much creativity stems using old solutions in believe case-based reasoning explain many creative design processes>
Paper 3:  <Title: Explaining Serendipitous Recognition in Design  .   Abstract: Creative designers often see solutions pending design problems the everyday objects surrounding This can often innovation insight sometimes revealing new functions purposes common design pieces in We interested modeling serendipitous recognition solutions pending problems creative mechanical design This paper characterizes this ability analyzing observations we placing other forms recognition We propose capture explore>
Paper 5:  <Title: Towards More Creative Case-Based Design Systems  .   Abstract: Case-based reasoning ( supporting creative design particularly processes rely previous design experience framing the problem evaluating design alternatives However most existing CBR systems living They tend adapt and reuse old solutions in routine ways producing robust but uninspired results Little research effort directed the kinds situation assessment, evaluation assimilation processes facilitate the exploration ideas the elaboration redefini>
Paper 6:  <Title: Creative Design: Reasoning and Understanding  .   Abstract: This paper memory issues long- term creative problem design activity taking a case-based reasoning perspective Our exploration We abstract Bell's reasoning and understanding mechanisms appear time long-term creative design We identify that the understanding mechanism responsible analogical anticipation design constraints analogical evaluation beside case-based design But an already understood design can satisfy op>
Paper 7:  <Title: Understanding Creativity: A Case-Based Approach  .   Abstract: Dissatisfaction existing standard case-based reasoning (CBR) systems prompted us investigate make creative, broadly what would it them creative This paper discusses three research goals understanding creative processes better investigating cases CBR creative problem understanding the framework that supports this more interesting kind case-based reasoning In addition it discusses methodological issues creativity, use CBR a researc>
Paper 8:  <Title: Protein Sequencing Experiment Planning Using Analogy protein sequencing experiments. Planning is interleaved with experiment execution,.   Abstract: Experiment design execution a central activity The SeqER system a general architecture the integration automated planning techniques domain knowledge in plan scientific experiments These planning techniques include rule-based methods, derivational analogy Derivational analogy allows planning experience captured as cases reused Analogy also allows the system function strong domain knowledge Cases efficiently flexibly retrieved a large case>
Paper 9:  <Title: Generic Teleological Mechanisms and their Use in Case Adaptation  .   Abstract: In experience-based (or case-based) reasoning new problems retrieving adapting similar problems encountered An important issue experience-based reasoning to identify different types knowledge reasoning useful different classes case-adaptation tasks In this paper a class non-routine case-adaptation tasks patterned insertions new elements old solutions We describe solving this task the design physical devices The method uses knowledge o>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Neural Networks

Case Based
Prediction:  Case Based
Is prediction correct?  False

Prediction: 0
Processing index 928...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Learning to Refine Case Libraries:  
Abstract: Abstract: Initial Results Abstract. Conversational case-based reasoning (CBR) systems, which incrementally extract a query description through a user-directed conversation, are advertised for their ease of use. However, designing large case libraries that have good performance (i.e., precision and querying efficiency) is difficult. CBR vendors provide guidelines for designing these libraries manually, but the guidelines are difficult to apply. We describe an automated inductive approach that revises conversational case libraries to increase their conformance with design guidelines. Revision increased performance on three conversational case libraries.
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 4:  <Title: Belief Networks Revisited  .   Abstract: Experiment design execution a central activity The SeqER system a general architecture the integration automated planning techniques domain knowledge in plan scientific experiments These planning techniques include rule-based methods, derivational analogy Derivational analogy allows planning experience captured as cases reused Analogy also allows the system function strong domain knowledge Cases efficiently flexibly retrieved a large case>
Label: Probabilistic Methods
Paper 8:  <Title: NESTED NETWORKS FOR ROBOT CONTROL  .   Abstract: Case-based reasoning depends multiple knowledge sources beyond the case library knowledge case adaptation criteria similarity assessment Because hand coding this knowledge accounts the knowledge acquisition burden developing CBR systems appealing acquire CBR apply This observation developing case-based CBR systems whose components themselves use. However despite early interest case-based approaches CBR this method compara>
Label: Neural Networks
Paper 2:  <Title: Refining Conversational Case Libraries  .   Abstract: Conversational case-based reasoning (CBR) shells, Inference's CBR Express commercially successful tools supporting help desk In contrast rule-based expert systems they capture knowledge as cases rather more problematic rules incrementally extended However rather eliminate the knowledge engineering bottleneck refocus it case engineering task carefully authoring cases according library design guidelines to good performance Designing complex libraries according>
Paper 3:  <Title: Towards More Creative Case-Based Design Systems  .   Abstract: Case-based reasoning ( supporting creative design particularly processes rely previous design experience framing the problem evaluating design alternatives However most existing CBR systems living They tend adapt and reuse old solutions in routine ways producing robust but uninspired results Little research effort directed the kinds situation assessment, evaluation assimilation processes facilitate the exploration ideas the elaboration redefini>
Paper 5:  <Title: Learning to Improve Case Adaptation by Introspective Reasoning and CBR  .   Abstract: In current CBR systems case adaptation usually rule-based methods hand The ability define those rules depends knowledge the task domain may not a presenting endowing CBR systems the needed adaptation knowledge This paper ongoing research a method address acquiring adaptation knowledge experience The method uses reasoning from scratch based introspective reasoning the requirements successfu>
Paper 6:  <Title: Protein Sequencing Experiment Planning Using Analogy protein sequencing experiments. Planning is interleaved with experiment execution,.   Abstract: Experiment design execution a central activity The SeqER system a general architecture the integration automated planning techniques domain knowledge in plan scientific experiments These planning techniques include rule-based methods, derivational analogy Derivational analogy allows planning experience captured as cases reused Analogy also allows the system function strong domain knowledge Cases efficiently flexibly retrieved a large case>
Paper 7:  <Title: Data Exploration with Reflective Adaptive Models  .   Abstract: Case-Based Planning provides scaling domain-independent planning solve It replaces the detailed and lengthy search a solution the retrieval adaptation previous planning experiences In general CBP demonstrated over generative (from- planning However the performance improvements it dependent adequate judgements as problem similarity In particular although CBP substantially planning effort overall subject a mis-retrieval problem T>
Paper 9:  <Title: Modeling Case-based Planning for Repairing Reasoning Failures  .   Abstract: One application models reasoning behavior allow a reasoner introspectively detect repair failures We address the transferability such models versus the specificity the knowledge in them the kinds structured the evaluation introspective reasoning systems We present the ROBBIE system implements its planning processes improve the planner response reasoning failures We show ROBBIE's hierarchical model balances model>
Paper 10:  <Title: Supporting Conversational Case-Based Reasoning in an Integrated Reasoning Framework  Conversational Case-Based Reasoning  .   Abstract: Conversational case-based reasoningCCBR successfully assist case retrieval tasks However behavioral limitations CCBR motivate integrations other reasoning approaches This paper briefly towards enhancing the inferencing behaviors a conversational case-based reasoning development tool named NaCoDAE In particular focus integrating NaCoDAE generative planning modules This paper defines CCBR briefly summarizes the integrations>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Case Based

Case Based
Prediction:  Case Based
Is prediction correct?  True

Prediction: 1
Processing index 1238...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: On Pruning and Averaging Decision Trees  
Abstract: Abstract: Pruning a decision tree is considered by some researchers to be the most important part of tree building in noisy domains. While, there are many approaches to pruning, an alternative approach of averaging over decision trees has not received as much attention. We perform an empirical comparison of pruning with the approach of averaging over decision trees. For this comparison we use a computa-tionally efficient method of averaging, namely averaging over the extended fanned set of a tree. Since there are a wide range of approaches to pruning, we compare tree averaging with a traditional pruning approach, along with an optimal pruning approach.
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 3:  <Title: Building Classifiers using Bayesian Networks  .   Abstract: Recent work supervised learning a surprisingly simple Bayesian classifier strong assumptions of independence among features called naive Bayes competitive state of C4.5 This fact raises less restrictive assumptions perform even In this paper and approaches inducing classifiers from data, based recent results the theory learning Bayesian networks Bayesian networks factored representations>
Label: Probabilistic Methods
Paper 7:  <Title: Improving Bagging Performance by Increasing Decision Tree Diversity  .   Abstract: Ensembles decision trees often exhibit greater predictive accuracy alone Bagging boosting two standard ways generating and combining multiple trees Boosting has empirically determined more of recently proposed this produces more diverse trees bagging This paper strongly We enforce greater decision tree diversity bagging a simple modification the underlying decision tree learner that utilizes randomly-generat>
Label: Theory
Paper 10:  <Title: Simplifying Decision Trees: A Survey  .   Abstract: Induced decision trees an extensively-researched solution classification tasks For many practical tasks the trees produced tree-generation algorithms not comprehensible users due Although many tree induction algorithms shown produce simpler, more comprehensible treesor data structures derived good classification accuracy tree simplification usually of secondary concern relative accuracy no attempt survey from the perspective simplification We present>
Label: Theory
Paper 2:  <Title: Pruning Decision Trees with Misclassification Costs  .   Abstract: We describe pruning methods decision tree classifiers when minimizing loss rather error In two common methods error minimization CART's cost-complexity pruning study the extension loss and one pruning variant based We perform these methods evaluate loss. We found applying the Laplace correction estimate at the leaves beneficial all pru>
Paper 4:  <Title: More Efficient Windowing  .   Abstract: Windowing has proposed a procedure efficient memory use the ID3 decision tree learning However previous work windowing may often a decrease performance In this work try argue separate-and-conquer rule learning algorithms more appropriate windowing learn rules independently changes class distributions In particular we will present a new windowing algorithm achieves additional gains efficiency exploiting this property>
Paper 5:  <Title: Decision Tree Induction: How Effective is the Greedy Heuristic?  .   Abstract: Most existing decision tree systems a greedy approach induce trees | locally optimal splits are induced Although the greedy approach believed produce reasonably good trees In the current work attempt verify this belief We quantify the goodness greedy tree induction empirically the popular decision tree algorithms C4.5 CART We induce decision trees on thousands compare found a novel map coloring idea We measure>
Paper 6:  <Title: An Empirical Evaluation of Bagging and Boosting  .   Abstract: An ensemble consists independently trained classifierssuch decision trees whose predictions combined when classifying novel instances Previous research an ensemble as often accurate any of the single classifiers Bagging (Breiman 1996a Boosting & Schapire producing ensembles In evaluate these methods both neural networks decision trees our classification algorithms Our results clearly two impo>
Paper 8:  <Title: Multivariate Decision Trees  .   Abstract: COINS Technical Report 92-82 December 1992 Abstract Multivariate decision trees overcome a representational limitation restricted splits the instance space the feature's axis This paper discusses the following issues for constructing multivariate decision trees representing including symbolic and numeric features learning the coefficients selecting include pruning of We present some new and>
Paper 9:  <Title: Top-Down Pruning in Relational Learning  .   Abstract: Pruning an effective method dealing noise Machine Learning Recently pruning algorithms, in particular Reduced Error Pruning also attracted Inductive Logic Programming However shown these methods inefficient most is for generating clauses explain noisy examples subsequently pruning We introduce which searches good theories to get the pruning algorithm Experiments show this approach>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Theory

Probabilistic Methods
Prediction:  Probabilistic Methods
Is prediction correct?  False

Prediction: 0
Processing index 1081...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Specialization of Recursive Predicates  
Abstract: Abstract: When specializing a recursive predicate in order to exclude a set of negative examples without excluding a set of positive examples, it may not be possible to specialize or remove any of the clauses in a refutation of a negative example without excluding any positive exam ples. A previously proposed solution to this problem is to apply program transformation in order to obtain non-recursive target predicates from recursive ones. However, the application of this method prevents recursive specializations from being found. In this work, we present the algorithm spectre ii which is not limited to specializing non-recursive predicates. The key idea upon which the algorithm is based is that it is not enough to specialize or remove clauses in refutations of negative examples in order to obtain correct specializations, but it is sometimes necessary to specialize clauses that appear only in refutations of positive examples. In contrast to its predecessor spectre, the new algorithm is not limited to specializing clauses defining one predicate only, but may specialize clauses defining multiple predicates. Furthermore, the positive and negative examples are no longer required to be instances of the same predicate. It is proven that the algorithm produces a correct specialization when all positive examples are logical consequences of the original program, there is a finite number of derivations of positive and negative examples and when no positive and negative examples have the same sequence of input clauses in their refutations.
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 3:  <Title: Predicate Invention and Learning from Positive Examples Only  .   Abstract: Previous bias shift approaches predicate invention applicable learning positive examples only if a complete hypothesis found the given language negative examples required determine new predicates should One approach presented, MERLIN a successor a system in predicate invention guided sequences input clauses in SLD-refutations positive and negative examples w.r.t an overly general theory In contrast which searches the minimal finite-state>
Label: Rule Learning
Paper 6:  <Title: Some studies in machine learning using the game of checkers. IBM Journal, 3(3):211-229, 1959. Some.   Abstract: covering has used and compared the covering technique a logic programming framework Covering works repeatedly specializing an overly general hypothesis on each iteration focusing finding a clause a high coverage positive examples Divide- works specializing an overly general hypothesis once focusing discriminating positive from Experimental results demonstrating cases more accurate hypotheses>
Label: Genetic Algorithms
Paper 8:  <Title: The Challenge of Revising an Impure Theory  .   Abstract: A pure rule-based program will return answers; set even its rules re However, an impure program the Prolog cut "!" not() operators return different answers re There also many reasoning systems return found for; too depend the rule order even A theory revision algorithm seeking a revised rule-base whose expected accuracy, over the distribution queries optimal sh>
Label: Theory
Paper 10:  <Title: Bottom-up induction of logic programs with more than one recursive clause  .   Abstract: In this paper called MRI induce logic programs from their examples This method induce programs with a base clause from examples MRI is based saturations examples It first generates a path structure an expression a stream processed predicates The concept path structure Identam-Almquist used TIM [ Idestam-Almquist 1996 In this paper introduce extension difference path>
Label: Rule Learning
Paper 2:  <Title: Specialization of Logic Programs by Pruning SLD-Trees  .   Abstract: program w.r.t positive and negative examples can viewed the problem pruning an SLD-tree such all refutations and excluded It shown the actual pruning can applying unfolding and clause removal The algorithm spectre presented, this idea The input to, besides a logic program and positive and negative examples a computation rule determines the SLD-tree that pruned It shown generality the resulting specia>
Paper 4:  <Title: Theory-Guided Induction of Logic Programs by Inference of Regular Languages recursive clauses. merlin on the.   Abstract: resent allowed sequences resolution steps for the initial theory There, many characterizations allowed sequences resolution steps expressed a set resolvents One approach presented, the system mer-lin, an earlier technique learning finite-state automata that represent allowed sequences resolution steps merlin extends the previous technique in i negative examples considered in addition a new strategy performing generalization techn>
Paper 5:  <Title: PAC-Learning PROLOG clauses with or without errors  .   Abstract: In we describe a generic ILP problem following given a set E (positive and examples a target predicate some background B the worldusually a logic program including facts auxiliary predicates a logic program Hour hypothesis such all positive examples can B H while In review some achieved this area the techniques Moreover we prove the following new results * Predicates described non, loc>
Paper 7:  <Title: THE DISCOVERY OF ALGORITHMIC PROBABILITY  .   Abstract: covering has used and compared the covering technique a logic programming framework Covering works repeatedly specializing an overly general hypothesis on each iteration focusing finding a clause a high coverage positive examples Divide- works specializing an overly general hypothesis once focusing discriminating positive from Experimental results demonstrating cases more accurate hypotheses>
Paper 9:  <Title: Integrity Constraints in ILP using a Monte Carlo approach  .   Abstract: Many state ILP require large numbers negative examples avoid This a considerable disadvantage many ILP applications namely indu ctive program synthesis where relativelly small and sparse example sets a more realistic scenario Integrity constraints first order clauses play negative examples One integrity constraint replace ground negative examples However checking the consistency a program integrity constraints usually involves heavy the ore>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Rule Learning

Rule Learning
Prediction:  Rule Learning
Is prediction correct?  True

Prediction: 1
Processing index 403...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Landscapes, Learning Costs and Genetic Assimilation.  
Abstract: Abstract: The evolution of a population can be guided by phenotypic traits acquired by members of that population during their lifetime. This phenomenon, known as the Baldwin Effect, can speed the evolutionary process as traits that are initially acquired become genetically specified in later generations. This paper presents conditions under which this genetic assimilation can take place. As well as the benefits that lifetime adaptation can give a population, there may be a cost to be paid for that adaptive ability. It is the evolutionary trade-off between these costs and benefits that provides the selection pressure for acquired traits to become genetically specified. It is also noted that genotypic space, in which evolution operates, and phenotypic space, on which adaptive processes (such as learning) operate, are, in general, of a different nature. To guarantee an acquired characteristic can become genetically specified, then these spaces must have the property of neighbourhood correlation which means that a small distance between two individuals in phenotypic space implies that there is a small distance between the same two individuals in genotypic space.
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 6:  <Title: Mutation Rates as Adaptations  .   Abstract: In order better life beyond the envelop of A simple model coevolution was implemented the mutation rate the individual This allowed the mutation rate itself evolve a lineage The model shows when the individuals interact a sort the lineages maintain relatively high mutation rates However when individuals engage interactions have greater consequences one individual in the interaction lineages tend relatively low mutatio>
Label: Genetic Algorithms
Paper 9:  <Title: Investigating the role of diploidy in simulated populations of evolving individuals  .   Abstract: In most work applying genetic algorithms populations neural networks there no real distinction genotype In nature both the information the genotype the mapping into usually much The genotypes many organisms exhibit they include two copies: if not their sequences therefore a functional difference their productsusually proteins the expressed phenotypic feature termed dominant one the one>
Label: Genetic Algorithms
Paper 10:  <Title: A STUDY OF CROSSOVER OPERATORS IN GENETIC PROGRAMMING  .   Abstract: Holland's analysis the sources power of genetic algorithms served guidance the applications The technique applying a recombination operatorcrossover a population individuals a key that power Neverless there contradictory results concerning crossover operators with overall performance Recently for genetic algorithms design neural network modules and their control circuits In these studies a genetic algorithm without crossover outperformed>
Label: Genetic Algorithms
Paper 2:  <Title: The Evolutionary Cost of Learning  .   Abstract: Traits that acquired members an evolving population during through adaptive processes learning can become genetically specified Thus there a change the level learning the population over evolutionary time This paper explores as well benefits to be learning costs the ability It these costs supply the selection pressure the genetic assimilation acquired traits Two models presented that attempt illustrate this assertion>
Paper 3:  <Title: Genes, Phenes and the Baldwin Effect: Learning and Evolution in a Simulated Population  .   Abstract: The Baldwin Effect first suggests the course influenced individually learned behavior The existence this effect still In this paper clear evidence learning-based plasticity at and produce directed changes at This research confirms earlier experimental work done others notably Hinton & Nowlan1987 Further the amount plasticity of the learned behavior shown crucial the size the Baldwi>
Paper 4:  <Title: Guiding or Hiding: Explorations into the Effects of Learning on the Rate of Evolution.  .   Abstract: Individual lifetime learning can `guide an evolving population areas high fitness genotype space through an evolutionary phenomenon the Baldwin effect 1896 Hin-ton Nowlan It the accepted wisdom this guiding speeds By highlighting another interaction learning evolution that will termed the Hiding effect here depends the measure evolutionary speed one adopts The Hiding effect shows learning the selection pressure between individuals `hiding t>
Paper 5:  <Title: Modeling the Evolution of Motivation  .   Abstract: In order learning improve the adaptiveness thus direct evolution Baldwin suggested the learning mechanism incorporate an innate evaluation how influence its reproductive fitness For example many circumstances that damage an animal otherwise reduce painful tend avoided We refer the mechanism an animal evaluates the fitness consequences argue evolve along evaluates We describe>
Paper 7:  <Title: The Coevolution of Mutation Rates  .   Abstract: In order better life beyond the envelop of A simple model coevolution was implemented genes longevity and mutation rate the individuals This made a lineage evolve immortal It also allowed the evolution no mutation extremely high mutation rates The model shows when the individuals interact a sort the lineages maintain relatively high mutation rates However when individuals engage interactions have greater consequences one ind>
Paper 8:  <Title: Generalist and Specialist Behavior Due to Individual Energy Extracting Abilities.  .   Abstract: The emergence generalist and specialist behavior populations neural networks studied Energy extracting ability included a property an organism In artificial life simulations with organisms living, the fitness score can interpreted the combination an organisms behavior extract potential food sources distributed The energy extracting ability viewed an evolvable trait organisms a particular organism's mechanisms extracting and, the>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Genetic Algorithms

Genetic Algorithms
Prediction:  Genetic Algorithms
Is prediction correct?  True

Prediction: 1
Processing index 1680...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Making SME greedy and pragmatic  
Abstract: Abstract: The Structure-Mapping Engine (SME) has successfully modeled several aspects of human consistent interpretations of an analogy. While useful for theoretical explorations, this aspect of the algorithm is both psychologically implausible and computationally inefficient. (2) SME contains no mechanism for focusing on interpretations relevant to an analogizer's goals. This paper describes modifications to SME which overcome these flaws. We describe a greedy merge algorithm which efficiently computes an approximate "best" interpretation, and can generate alternate interpretations when necessary. We describe pragmatic marking, a technique which focuses the mapping to produce relevant, yet novel, inferences. We illustrate these techniques via example and evaluate their performance using empirical data and theoretical analysis. analogical processing. However, it has two significant drawbacks: (1) SME constructs all structurally
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 4:  <Title: Double Censoring: Characterization and Computation of the Nonparametric Maximum Likelihood Estimator  .   Abstract: In case-based planning previously generated plans stored cases memory solve CBP can save considerable time over planning from scratch (generative planning thus offering handling One drawback CBP systems has a highly structured memory that requires significant domain engineering complex memory indexing schemes enable efficient case retrieval In contrast our CBP system CaPER a massively parallel frame-ba>
Label: Probabilistic Methods
Paper 2:  <Title: The Structure-Mapping Engine: Algorithm and Examples  .   Abstract: This paper the Structure-Mapping Engine studying analogical processing SME built explore Gentner's Structure-mapping theory analogy provides a "tool kit constructing matching algorithms consistent Its flexibility enhances cognitive simulation studies experimentation Furthermore SME very efficient making useful component machine learning systems as We review the Structure-mapping theory describe the engine We analyze the complexity the algorithm demo>
Paper 3:  <Title: MAC/FAC: A Model of Similarity-based Retrieval  .   Abstract: We present similarity-based retrieval which attempts three psychological phenomena people extremely judging similarity analogy when given items to (2) Superficial remindings much (3 People sometimes experience and use purely structural analogical remindings Our model called MAC/FACformany are called but chosen consists (MAC uses filter candidates memory items That r>
Paper 5:  <Title: Modeling Case-based Planning for Repairing Reasoning Failures  .   Abstract: One application models reasoning behavior allow a reasoner introspectively detect repair failures We address the transferability such models versus the specificity the knowledge in them the kinds structured the evaluation introspective reasoning systems We present the ROBBIE system implements its planning processes improve the planner response reasoning failures We show ROBBIE's hierarchical model balances model>
Paper 6:  <Title: A Methodology for Processing Problem Constraints in Genetic Programming  .   Abstract: Search mechanisms artificial intelligence combine two elements representation the search space a search mechanism actually explores Unfortunately many searches may explore redundant and/or invalid solutions Genetic programming refers evolutionary algorithms but utilizing a parameterized representation in trees These algorithms perform searches simulation nature They face redundant/invalid subspaces These problems just recently>
Paper 7:  <Title: Data Exploration with Reflective Adaptive Models  .   Abstract: Case-Based Planning provides scaling domain-independent planning solve It replaces the detailed and lengthy search a solution the retrieval adaptation previous planning experiences In general CBP demonstrated over generative (from- planning However the performance improvements it dependent adequate judgements as problem similarity In particular although CBP substantially planning effort overall subject a mis-retrieval problem T>
Paper 8:  <Title: Proceedings of CogSci89 Structural Evaluation of Analogies: What Counts?  .   Abstract: Judgments similarity soundness human analogical processing This paper these judgments modeled SME Gentner's structure-mapping theory We focus structural evaluation explicating several principles which psychologically plausible algorithms We introduce the Specificity Conjecture claims naturalistic representations include appearance We demonstrate via computational experiments this conjecture affects how structural evaluation sho>
Paper 9:  <Title: How good are genetic algorithms at finding large cliques: an experimental study  .   Abstract: This paper genetic algorithms at solving the MAX-CLIQUE problem We measure a standard genetic algorithm an elementary set problem instances consisting embedded cliques random graphs We indicate improvement introduce the multi-phase annealed GA exhibits As scale the problem size test on "hard" benchmark instances notice a degraded performance the algorithm caused premature convergence local minima To alleviat>
Paper 10:  <Title: Selection of Distance Metrics and Feature Subsets for k-Nearest Neighbor Classifiers  .   Abstract: Prioritized sweeping is a model-based reinforcement learning method attempts focus achieve a good estimate environment states To choose effectively where a costly planning step classic prioritized sweeping uses a simple heuristic focus computation the states likely the largest errors In generalized prioritized sweeping a principled method such estimates This allows extend prioritized sweeping beyon>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Case Based

Case Based
Prediction:  Case Based
Is prediction correct?  True

Prediction: 1
Processing index 395...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Evolving Graphs and Networks with Edge Encoding: Preliminary Report  
Abstract: Abstract: We present an alternative to the cellular encoding technique [Gruau 1992] for evolving graph and network structures via genetic programming. The new technique, called edge encoding, uses edge operators rather than the node operators of cellular encoding. While both cellular encoding and edge encoding can produce all possible graphs, the two encodings bias the genetic search process in different ways; each may therefore be most useful for a different set of problems. The problems for which these techniques may be used, and for which we think edge encoding may be particularly useful, include the evolution of recurrent neural networks, finite automata, and graph-based queries to symbolic knowledge bases. In this preliminary report we present a technical description of edge encoding and an initial comparison to cellular encoding. Experimental investigation of the relative merits of these encoding schemes is currently in progress.
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 6:  <Title: A STUDY OF CROSSOVER OPERATORS IN GENETIC PROGRAMMING  .   Abstract: Holland's analysis the sources power of genetic algorithms served guidance the applications The technique applying a recombination operatorcrossover a population individuals a key that power Neverless there contradictory results concerning crossover operators with overall performance Recently for genetic algorithms design neural network modules and their control circuits In these studies a genetic algorithm without crossover outperformed>
Label: Genetic Algorithms
Paper 7:  <Title: A Comparison of Crossover and Mutation in Genetic Programming  .   Abstract: This paper a large and systematic body mutation, crossover combinations mutation genetic programming (GP The literature of traditional genetic algorithms contains related studies mutation and crossover in GP differ their traditional counterparts In this paper the equivalent approximately 12,000 typical runs a GP system systematically exploring a range parameter settings The resulting data may n>
Label: Genetic Algorithms
Paper 2:  <Title: A Study in Program Response and the Negative Effects of Introns in Genetic Programming  .   Abstract: The standard method obtaining a response in tree-based genetic programming take the value returned In non-tree representations alternate methods have explored One alternative treat a specific location indexed memory the response value when The purpose explore this technique tree-structured programs explore the intron effects these studies bring light This paper's experimental results support this memory-based program response technique>
Paper 3:  <Title: Induction of decision trees using RELIEFF  .   Abstract: An investigation the dynamics Genetic Programming applied chaotic time series prediction reported An interesting characteristic adaptive search techniques perform well many problem domains while failing Because Genetic Programming's flexible tree structure any particular problem represented myriad forms These representations variegated effects search performance Therefore an aspect fundamental engineering significance find a representation, acted Genetic Programming operators opt>
Paper 4:  <Title: Genetic Algorithms for Combinatorial Optimization: The Assembly Line Balancing Problem  .   Abstract: Genetic algorithms one example a random element within for We consider the application the genetic algorithm the Assembly Line Balancing A general description genetic algorithms, their specialized use on our test-bed problems We carry extensive computational testing find appropriate values associated this genetic algorithm These experiments underscore the correct choice a scaling parameter mutati>
Paper 5:  <Title: The Royal Road for Genetic Algorithms: Fitness Landscapes and GA Performance  .   Abstract: Genetic algorithms (GAs play many artificial-life systems often little detailed understanding why the GA performs as it little theoretical basis on characterize the types fitness landscapes lead successful GA performance In this paper addressing these issues Our strategy consists defining features of fitness landscapes particularly the GA, experimentally studying how various configurations along dime>
Paper 8:  <Title: Cultural Transmission of Information in Genetic Programming  .   Abstract: This paper shows the performance a genetic programming system through the addition mechanisms individuals (culture Teller has previously shown genetic programming systems enhanced the addition memory mechanisms for individual programs [Teller 1994 in changed communication individuals within and across generations We show the effects indexed memory culture a genetic programming>
Paper 9:  <Title: The Role of Development in Genetic Algorithms  .   Abstract: Technical Report Number CS94394 Computer Science Abstract The developmental mechanisms transforming to typically omitted formulationsGAs these two representational spaces identical We argue developmental mechanisms useful understanding the success several standard GA techniques can clarify more recently proposed enhancements We provide distinguishes two developmental mechanisms | learning an>
Paper 10:  <Title: Evolution of Mapmaking: Learning, planning, and memory using Genetic Programming  .   Abstract: An essential component an intelligent agent observe encode use Traditional approaches Genetic Programming evolving functional or reactive programs only a minimal use state This paper investigating learning, planning memory Genetic Programming The approach uses a multi-phasic fitness environment enforces the use memory allows fairly straightforward comprehension the evolved representations. An illustrative problem 'gold' col>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Genetic Algorithms

Genetic Algorithms
Prediction:  Genetic Algorithms
Is prediction correct?  True

Prediction: 1
Processing index 1130...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Dynamic Hill Climbing: Overcoming the limita- tions of optimization techniques  
Abstract: Abstract: This paper describes a novel search algorithm, called dynamic hill climbing, that borrows ideas from genetic algorithms and hill climbing techniques. Unlike both genetic and hill climbing algorithms, dynamic hill climbing has the ability to dynamically change its coordinate frame during the course of an optimization. Furthermore, the algorithm moves from a coarse-grained search to a fine-grained search of the function space by changing its mutation rate and uses a diversity-based distance metric to ensure that it searches new regions of the space. Dynamic hill climbing is empirically compared to a traditional genetic algorithm using De Jong's well-known five function test suite [4] and is shown to vastly surpass the performance of the genetic algorithm, often finding better solutions using only 1% as many function evaluations. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 2:  <Title: Hybridized Crossover-Based Search Techniques for Program Discovery  .   Abstract: In address program discovery as Genetic Programming [10 We two major results by combining a hierarchical crossover operator two traditional single point search algorithms: Simulated Annealing Stochastic Iterated Hill Climbing solved some problems fewer fitness evaluations a success Genetic Programming Second managed enhance Genetic Programming hybridizing the simple scheme hill climbing from a few individuals at a fixed interval generations T>
Label: Genetic Algorithms
Paper 4:  <Title: Improving the Performance of Evolutionary Optimization by Dynamically Scaling the Evaluation Function  .   Abstract: Traditional evolutionary optimization algorithms assume a static evaluation function according solutions evolved Incremental evolution an approach through a dynamic evaluation function scaled over improve evolutionary optimization In this paper empirical results this approach genetic programming Using two domains a two-agent pursuit-evasion game the Tracker [6] trail-following task demonstrate incremental evolution most successful applied near t>
Label: Genetic Algorithms
Paper 3:  <Title: Toward a unified theory of spatiotemporal processing in the retina  .   Abstract: Traditional evolutionary optimization algorithms assume a static evaluation function according solutions evolved Incremental evolution an approach through a dynamic evaluation function scaled over improve evolutionary optimization In this paper empirical results this approach genetic programming Using two domains a two-agent pursuit-evasion game the Tracker [6] trail-following task demonstrate incremental evolution most successful applied near t>
Paper 5:  <Title: Hierarchical Evolution of Neural Networks  .   Abstract:  most applications of neuro-evolution each individual in represents Recent work the SANE system however evolving individual neurons often produces a more efficient genetic search This paper while SANE solve easy tasks very often stalls larger problems A hierarchical approach presented that overcomes SANE's difficulties both a neuron-level exploratory search a robot arm manipulation task hiera>
Paper 6:  <Title: TD Learning of Game Evaluation Functions with Hierarchical Neural Architectures  .   Abstract: Genetic algorithms solve hard optimization problems ranging the Travelling Salesman problem the Quadratic Assignment problem We show the Simple Genetic Algorithm solve derived the 3-Conjunctive Normal Form problem By separating the populations small sub parallel genetic algorithms exploits the inherent parallelism prevents premature convergence Genetic algorithms using hill-climbing conduct genetic search in the space local optima can less>
Paper 7:  <Title: A Study of Genetic Algorithms to Find Approximate Solutions to Hard 3CNF Problems  .   Abstract: Genetic algorithms solve hard optimization problems ranging the Travelling Salesman problem the Quadratic Assignment problem We show the Simple Genetic Algorithm solve derived the 3-Conjunctive Normal Form problem By separating the populations small sub parallel genetic algorithms exploits the inherent parallelism prevents premature convergence Genetic algorithms using hill-climbing conduct genetic search in the space local optima can less>
Paper 8:  <Title: HOW TO EVOLVE AUTONOMOUS ROBOTS: DIFFERENT APPROACHES IN EVOLUTIONARY ROBOTICS  .   Abstract:  most applications of neuro-evolution each individual in represents Recent work the SANE system however evolving individual neurons often produces a more efficient genetic search This paper while SANE solve easy tasks very often stalls larger problems A hierarchical approach presented that overcomes SANE's difficulties both a neuron-level exploratory search a robot arm manipulation task hiera>
Paper 9:  <Title: How good are genetic algorithms at finding large cliques: an experimental study  .   Abstract: This paper genetic algorithms at solving the MAX-CLIQUE problem We measure a standard genetic algorithm an elementary set problem instances consisting embedded cliques random graphs We indicate improvement introduce the multi-phase annealed GA exhibits As scale the problem size test on "hard" benchmark instances notice a degraded performance the algorithm caused premature convergence local minima To alleviat>
Paper 10:  <Title: Evolving Optimal Neural Networks Using Genetic Algorithms with Occam's Razor  .   Abstract: Genetic algorithms neural networks two main ways optimize the network architecture train the weights a fixed architecture While most previous work focuses only of investigates an alternative evolutionary approach called Breeder Genetic Programming the architecture the weights optimized simultaneously The genotype each network represented whose depth and dynamically adapted the particular application by specifically defined genetic operators The weights t>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Genetic Algorithms

Genetic Algorithms
Prediction:  Genetic Algorithms
Is prediction correct?  True

Prediction: 1
Processing index 1981...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Task and Spatial Frequency Effects on Face Specialization  
Abstract: Abstract: There is strong evidence that face processing is localized in the brain. The double dissociation between prosopagnosia, a face recognition deficit occurring after brain damage, and visual object agnosia, difficulty recognizing other kinds of complex objects, indicates that face and non-face object recognition may be served by partially independent mechanisms in the brain. Is neural specialization innate or learned? We suggest that this specialization could be the result of a competitive learning mechanism that, during development, devotes neural resources to the tasks they are best at performing. Further, we suggest that the specialization arises as an interaction between task requirements and developmental constraints. In this paper, we present a feed-forward computational model of visual processing, in which two modules compete to classify input stimuli. When one module receives low spatial frequency information and the other receives high spatial frequency information, and the task is to identify the faces while simply classifying the objects, the low frequency network shows a strong specialization for faces. No other combination of tasks and inputs shows this strong specialization. We take these results as support for the idea that an innately-specified face processing module is unnecessary.
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 2:  <Title: Learning a Specialization for Face Recognition: The Effect of Spatial Frequency  .   Abstract: The double dissociation a face recognition deficit occurring brain damage visual object agnosia difficulty recognizing other kinds indicates face and non-face object recognition served partially independent mechanisms in Such a dissociation could a competitive learning mechanism, during development devotes neural resources the tasks they best performing Studies normal adult performance on face object seem prim>
Label: Neural Networks
Paper 3:  <Title: Robust Parameter Learning in Bayesian Networks with Missing Data  .   Abstract: There strong evidence face processing localized The double dissociation a face recognition deficit occurring brain damage visual object agnosia difficulty recognizing other kinds indicates served partially independent neural mechanisms In this chapter use computational models the face processing specialization apparently underlying and visual object agnosia attributed 1 a relatively simple competitive selection>
Label: Probabilistic Methods
Paper 7:  <Title: Implicit learning in 3D object recognition: The importance of temporal context  .   Abstract: A novel architecture and set learning rules cortical self-organization The model the idea multiple information channels modulate plasticity Features learned bottom-up information sources can thus influenced those learned contextual pathways vice A maximum likelihood cost function allows this scheme a biologically feasible, hierarchical neural circuit In simulations the model first demonstrate the utility temporal context plasticity The model learns repr>
Label: Neural Networks
Paper 4:  <Title: Prosopagnosia in Modular Neural Network Models  .   Abstract: There strong evidence face processing localized The double dissociation a face recognition deficit occurring brain damage visual object agnosia difficulty recognizing other kinds indicates served partially independent neural mechanisms In this chapter use computational models the face processing specialization apparently underlying and visual object agnosia attributed 1 a relatively simple competitive selection>
Paper 5:  <Title: A Mixture of Experts Model Exhibiting Prosopagnosia  .   Abstract: A considerable body from a deficit face recognition dissociable nonface object recognition indicates devotes a specialized functional area mechanisms appropriate We present a modular neural network composed two expert networks one mediating gate network the task the faces 12 individuals classifying 36 nonface objects members one While learning the task the network tends divide labor the two expert modules s>
Paper 6:  <Title: Efficient Visual Search: A Connectionist Solution  .   Abstract: Searching objects scenes a natural task people and has extensively In this paper examine this task a connectionist perspective Computational complexity arguments suggest parallel feed-forward networks perform efficiently One difficulty, distinguish the target distractors a combination features associated Often called the binding problem this requirement presents connectionist models visual processing when multiple objects>
Paper 8:  <Title: Topography And Ocular Dominance: A Model Exploring Positive Correlations  .   Abstract: The map from eye brain in topographic i.e. neighbouring points to In addition when two eyes innervate the same target structure fibres segregate ocular dominance stripes Experimental evidence the frog goldfish suggests these two phenomena subserved We present addresses the formation both topography ocular dominance The model a form competitive learning with subtractive enforcement a weight norm>
Paper 9:  <Title: Learning Algorithms with Applications to Robot Navigation and Protein Folding  .   Abstract: Using scene analysis as the task this research focuses three fundamental problems neural network systems representing schemas learning schemas The first problem arises because no practical neural network process efficiently The solution process the input parallel successively focus This strategy requires the system maintains structured knowledge for describing interpreting the gathered information The system sh>
Paper 10:  <Title: Computational Models of Sensorimotor Integration  Computational Maps and Motor Control.  .   Abstract: The sensorimotor integration system can viewed an observer attempting estimate its own state and by integrating multiple sources We describe a computational framework capturing this notion some specific models integration adaptation result Psychophysical results two sensorimotor systems subserving the integration adaptation visuo-auditory maps estimation the state the hand arm movements and within this framework These results: Spatia>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Neural Networks

Neural Networks
Prediction:  Neural Networks
Is prediction correct?  True

Prediction: 1
Processing index 499...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: IMPROVING THE MEAN FIELD APPROXIMATION VIA THE USE OF MIXTURE DISTRIBUTIONS  
Abstract: Abstract: Mean field methods provide computationally efficient approximations to posterior probability distributions for graphical models. Simple mean field methods make a completely factorized approximation to the posterior, which is unlikely to be accurate when the posterior is multimodal. Indeed, if the posterior is multi-modal, only one of the modes can be captured. To improve the mean field approximation in such cases, we employ mixture models as posterior approximations, where each mixture component is a factorized distribution. We describe efficient methods for optimizing the parameters in these models. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 5:  <Title: Gaussian Processes for Bayesian Classification via Hybrid Monte Carlo  .   Abstract: The full Bayesian method applying neural networks a prediction problem to set the prior/hyperprior structure the net perform the necessary integrals However these integrals tractable Markov methods slow especially high- Using Gaussian processes we approximate the weight space integral analytically so need integrated over by MCMC methods We applied this idea classification obtaining ex cellent r>
Label: Neural Networks
Paper 9:  <Title: Using Dirichlet Mixture Priors to Derive Hidden Markov Models for Protein Families  .   Abstract: A Bayesian method the amino acid distributions the states a hidden Markov model a protein family or the columns a multiple alignment that family introduced This method Dirichlet mixture densities priors over amino acid distributions These mixture densities determined examination previously constructed HMMs or multiple alignments It shown this Bayesian method improve HMMs produced small training sets Specific experiments the EF-hand motif reported these priors shown>
Label: Neural Networks
Paper 2:  <Title: A note on convergence rates of Gibbs sampling for nonparametric mixtures  .   Abstract: We consider a mixture model the mixing distribution random prior We describe two Gibbs sampling algorithms this problem When the kernel f(x j of the mixture bounded show resulting Gibbs sampling uniformly provide an explicit rate bound Unfortunately the bound is sharp in general improving sensibly seems however quite>
Paper 3:  <Title: Factorial Hidden Markov Models  .   Abstract: One the basic probabilistic tools the hidden Markov model In an HMM, information of the time series conveyed a single discrete variable|the hidden state We present HMMs this state factored multiple state variables therefore represented Both inference learning in this model critically computing the hidden state variables given the observations We present an exact algorithm inference this model a>
Paper 4:  <Title: On Bayesian analysis of mixtures with an unknown number of components  Summary  .   Abstract: New methodology fully Bayesian mixture analysis developed making reversible jump Markov chain Monte Carlo methods that capable jumping the parameter subspaces corresponding different numbers components A sample from the full joint distribution all unknown variables thereby generated this can a thorough presentation many aspects the posterior distribution The methodology applied here the analysis univariate normal mixtures using a hierarchical prior model offers an approach>
Paper 6:  <Title: A variational approach to Bayesian logistic regression models and their extensions  .   Abstract: We consider We show accurate variational techniques a closed form posterior distribution given the data thereby yielding The results readily extended (binary) belief networks For belief networks we also derive closed form posteriors missing values Finally show the dual of the regression problem gives a latent variable density model leads exactly sol>
Paper 7:  <Title: Monte Carlo Implementation of Gaussian Process Models for Bayesian Regression and Classification  .   Abstract: Technical Report No 9702, Department Statistics University Toronto Abstract Gaussian processes a natural way defining prior distributions over functions In a simple nonparametric regression problem where such a function gives for an observed response easily matrix computations that feasible datasets up about a thousand cases Hyperparameters that define sampled me>
Paper 8:  <Title: Inference in Dynamic Error-in-Variable-Measurement Problems  .   Abstract: Efficient algorithms have estimating model parameters from measured data even gross errors In addition point estimates parameters however assessments uncertainty needed Linear approximations provide standard errors these misleading models substantially nonlinear To overcome "profiling" methods the case in the regressor variables error In this paper extend profiling methods ErrorinVariableMeasurement models We use Laplac>
Paper 10:  <Title: A VIEW OF THE EM ALGORITHM THAT JUSTIFIES INCREMENTAL, SPARSE, AND OTHER VARIANTS  .   Abstract: The EM algorithm performs maximum likelihood estimation data which some variables We present a function resembles negative free energy show the M step maximizes this function with the model parameters the E step over From this perspective justify an incremental variant the EM algorithm the distribution for only recalculated each E step This variant shown empirically give faster convergence i>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Probabilistic Methods

Probabilistic Methods
Prediction:  Probabilistic Methods
Is prediction correct?  True

Prediction: 1
Processing index 1183...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition  
Abstract: Abstract: This paper describes the MAXQ method for hierarchical reinforcement learning based on a hierarchical decomposition of the value function and derives conditions under which the MAXQ decomposition can represent the optimal value function. We show that for certain execution models, the MAXQ decomposition will produce better policies than Feudal Q learning.
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 7:  <Title: Using Path Diagrams as a Structural Equation Modelling Tool  .   Abstract: Reinforcement learning the problem generating given of interacting it Many algorithms for work computing improved estimates the optimal value function We extend prior analyses reinforcement-learning algorithms present a powerful new theorem provide a unified analysis The usefulness the theorem lies the asynchronous convergence a complex reinforcement-lea>
Label: Probabilistic Methods
Paper 8:  <Title: Learning to Achieve Goals  .   Abstract: Temporal difference methods solve the temporal credit assignment problem reinforcement learning An important subproblem general reinforcement learning learning achieve dynamic goals Although existing temporal difference methods, Q learning this problem take advantage its special structure This paper the DG-learning algorithm learns efficiently achieve dynamically changing goals exhibits good knowledge transfer goals In addition this paper how traditional relaxation techniques appl>
Label: Reinforcement Learning
Paper 9:  <Title: An Upper Bound on the Loss from Approximate Optimal-Value Functions  .   Abstract: Many reinforcement learning (RL) approaches can formulated from Markov decision processes the associated method dynamic programming The value this theoretical understanding, tempered many practical concerns One important question DP-based approaches that function approximation rather lookup tables can avoid catastrophic effects This note presents a result in Bertsekas1987 guarantees small errors a task's optimal value function produce arbitrarily bad perf>
Label: Reinforcement Learning
Paper 2:  <Title: Approximating Value Trees in Structured Dynamic Programming  .   Abstract: We propose and examine approximate dynamic programming Markov decision processes structured problem representations We assume an MDP represented construct value functions decision trees our function representation The size the representation kept pruning these value trees leaves represent possible ranges thus produced during optimization We propose convergence prove errors bounds resulting a>
Paper 3:  <Title: Constructive Neural Network Learning Algorithms for Multi-Category Real-Valued Pattern Classification  .   Abstract: Prioritized sweeping is a model-based reinforcement learning method attempts focus achieve a good estimate environment states To choose effectively where a costly planning step classic prioritized sweeping uses a simple heuristic focus computation the states likely the largest errors In generalized prioritized sweeping a principled method such estimates This allows extend prioritized sweeping beyon>
Paper 4:  <Title: Hierarchical Explanation-Based Reinforcement Learning  .   Abstract: Explanation-Based Reinforcement Learning was Dietterich and Flann as combiningRL optimal plans the generalization abilityDi-etterich We extend this work domains the agent must order achieve a sequence subgoals Hierarchical EBRL can effectively learn optimal policies some these sequential task domains even the subgoals weakly interact We also show when planner that can achiev>
Paper 5:  <Title: Bayesian Methods for Adaptive Models  .   Abstract: Almost all the work Average-reward Re- inforcement Learning so table-based methods which do scale domains large state spaces In this paper two extensions a model-based ARL method called H-learning address We extend H-learning learn action models reward functions Bayesian networks approximate its value function local linear regression We test our algorithms several scheduling tasks for a simulated Automatic Guided Vehicle and show signi>
Paper 6:  <Title: Value Function Approximations and Job-Shop Scheduling  .   Abstract: We report a successful application TD() value function approximation the task job-shop scheduling Our scheduling problems based the problem scheduling payload processing steps The value function approximated a 2-layer feedforward network sigmoid units A one-step lookahead greedy algorithm using the learned evaluation function outperforms an iterative repair method incorporating simulated annealing To this>
Paper 10:  <Title: Selection of Distance Metrics and Feature Subsets for k-Nearest Neighbor Classifiers  .   Abstract: Prioritized sweeping is a model-based reinforcement learning method attempts focus achieve a good estimate environment states To choose effectively where a costly planning step classic prioritized sweeping uses a simple heuristic focus computation the states likely the largest errors In generalized prioritized sweeping a principled method such estimates This allows extend prioritized sweeping beyon>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Reinforcement Learning

Reinforcement Learning
Prediction:  Reinforcement Learning
Is prediction correct?  True

Prediction: 1
Processing index 186...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Adaptive state space quantisation: adding and removing neurons  
Abstract: Abstract: This paper describes a self-learning control system for a mobile robot. Based on local sensor data, a robot is taught to avoid collisions with obstacles. The only feedback to the control system is a binary-valued external reinforcement signal, which indicates whether or not a collision has occured. A reinforcement learning scheme is used to find a correct mapping from input (sensor) space to output (steering signal) space. An adaptive quantisation scheme is introduced, through which the discrete division of input space is built up from scratch by the system itself. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 2:  <Title: LEARNING TO AVOID COLLISIONS: A REINFORCEMENT LEARNING PARADIGM FOR MOBILE ROBOT NAVIGATION  .   Abstract: The paper describes a self-learning control system a mobile robot Based sensor information the control system provide a steering signal in collisions Since in our case no `examples the system learns on an external reinforcement signal negative case a collision zero otherwise We describe the adaptive algorithm which a discrete coding the state space learning the correct mapping from the input (state) vectorsteering signal>
Paper 3:  <Title: Adaptive state space quantisation for reinforcement learning of collision-free navigation  .   Abstract: The paper a mobile robot Based sensor information the control system provide a steering signal collisions Since in our case no `examples the system learns on an external reinforcement signal negative case a collision zero otherwise Rules from Temporal Difference learning are find the (discrete) sensor input space the steering signal We describe the algorithm learning the correct mapping (st>
Paper 4:  <Title: Evolving Obstacle Avoidance Behavior in a Robot Arm  .   Abstract: Existing approaches learning rely supervised methods where correct behavior explicitly given It learn avoid obstacles using such methods examples obstacle avoidance behavior hard generate This paper evolves neural network controllers through genetic algorithms No input/output examples necessary neuro-evolution learns a single performance measurement over of grasping The approach tested the OSCAR-6 rob>
Paper 5:  <Title: Evolution of Homing Navigation in a Real Mobile Robot  .   Abstract: In to control a real mobile robot In all our experiments the evolutionary procedure carried entirely the physical robot without We show the autonomous development a set behaviors for locating a battery charger periodically returning lifting constraints the robot/environment interactions were employed a preliminary experiment The emergent homing behavior based the autonomous development an internal ne>
Paper 6:  <Title: Automatic Generation of Adaptive Programs Automatic Generation of Adaptive Programs. In From Animals to Animats.   Abstract: Fuzzy rules control can effectively tuned reinforcement learning Reinforcement learning only information the control application The tuning process allows people generate fuzzy rules unable accurately perform control have them tuned rules provide This paper a new simplified method using the tuning fuzzy control rules It shown the learned fuzzy rules provide smoother control the pole balancing domain t>
Paper 7:  <Title: Biological metaphors and the design of modular artificial neural networks Master's thesis of  .   Abstract: In packet routing which embedded Only local information used at each node to accurate statistics which routing policies lead minimal routing times In simple experiments involving a 36-node irregularly-connected network this learning approach proves superior routing based>
Paper 8:  <Title: NEUROCONTROL BY REINFORCEMENT LEARNING  .   Abstract: Reinforcement learningRL a model-free tuning adaptation method control dynamic systems Contrary supervised learning based usually gradient descent techniques RL does any model or sensitivity function of the process Hence RL can applied systems poorly uncertain nonlinear for untractable with In reinforcement learning the overall controller performance evaluated a scalar measure reinforcement. Depending the control task reinforcement represent>
Paper 9:  <Title: USING MARKER-BASED GENETIC ENCODING OF NEURAL NETWORKS TO EVOLVE FINITE-STATE BEHAVIOUR  .   Abstract: A new mechanism genetic encoding neural networks is loosely the marker structure biological DNA The mechanism allows all aspects the network structure their connectivity evolved through genetic algorithms The effectiveness the encoding scheme demonstrated an object recognition task that requires artificial creatures (whose behaviour driven develop high-level finite-state exploration and discrimination strategies The task requires solving the sensory-motor groundin>
Paper 10:  <Title: AVERAGED REWARD REINFORCEMENT LEARNING APPLIED TO FUZZY RULE TUNING  .   Abstract: Fuzzy rules control can effectively tuned reinforcement learning Reinforcement learning only information the control application The tuning process allows people generate fuzzy rules unable accurately perform control have them tuned rules provide This paper a new simplified method using the tuning fuzzy control rules It shown the learned fuzzy rules provide smoother control the pole balancing domain t>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Reinforcement Learning

Reinforcement Learning
Prediction:  Reinforcement Learning
Is prediction correct?  True

Prediction: 1
Processing index 504...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: MANIAC: A Next Generation Neurally Based Autonomous Road Follower  
Abstract: Abstract: The use of artificial neural networks in the domain of autonomous vehicle navigation has produced promising results. ALVINN [Pomerleau, 1991] has shown that a neural system can drive a vehicle reliably and safely on many different types of roads, ranging from paved paths to interstate highways. Even with these impressive results, several areas within the neural paradigm for autonomous road following still need to be addressed. These include transparent navigation between roads of different type, simultaneous use of different sensors, and generalization to road types which the neural system has never seen. The system presented here addresses these issue with a modular neural architecture which uses pre-trained ALVINN networks and a connectionist superstructure to robustly drive on many dif ferent types of roads.
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 2:  <Title: The BATmobile: Towards a Bayesian Automated Taxi  .   Abstract: The problem driving highway traffic engages many areas AI research has substantial economic significance We describe work this problem based a decision-theoretic architecture using dynamic probabilistic networks The architecture provides a sound solution sensor noise sensor failure uncertainty about other vehicles about Our approach implemented a computer simulation system the autonomous vehicle successfully negotiate>
Label: Probabilistic Methods
Paper 10:  <Title: Appears in Working Notes, Integrating Multiple Learned Models for Improving and Scaling Machine Learning Algorithms.   Abstract: This paper the Plannett system artificial neural networks achieve expert- level accuracy on the difficult scientific task recognizing volcanos radar images Plannett uses ANNs that vary along the set input features used train hidden units The ANNs combined simply averaging their output activations When Plannett is used the classification module called JAR- tool end accuracy (sensitivity as>
Label: Neural Networks
Paper 3:  <Title: VECTOR ASSOCIATIVE MAPS: UNSUPERVISED REAL-TIME ERROR-BASED LEARNING AND CONTROL OF MOVEMENT TRAJECTORIES  .   Abstract: ALVINN (Autonomous Land Vehicle in a Neural Net a Backpropagation trained neural network which steering road and highway environments Although ALVINN fairly robust one has the time train As the vehicle capable online learning has drive for the network capable autonomous operation One reason the use Backprop. In this report describe the original ALVINN system then look three alternative training methods - Quic>
Paper 4:  <Title: Automated Highway System  .   Abstract: ALVINN (Autonomous Land Vehicle in a Neural Net a Backpropagation trained neural network which steering road and highway environments Although ALVINN fairly robust one has the time train As the vehicle capable online learning has drive for the network capable autonomous operation One reason the use Backprop. In this report describe the original ALVINN system then look three alternative training methods - Quic>
Paper 5:  <Title: Evolving Obstacle Avoidance Behavior in a Robot Arm  .   Abstract: Existing approaches learning rely supervised methods where correct behavior explicitly given It learn avoid obstacles using such methods examples obstacle avoidance behavior hard generate This paper evolves neural network controllers through genetic algorithms No input/output examples necessary neuro-evolution learns a single performance measurement over of grasping The approach tested the OSCAR-6 rob>
Paper 6:  <Title: Using a Genetic Algorithm to Learn Strategies for Collision Avoidance and Local Navigation  .   Abstract: Navigation through obstacles such mine fields is an important capability autonomous underwater vehicles One way produce robust behavior perform projective planning However real-time performance a critical requirement navigation What needed a truly autonomous vehicle are robust reactive rules perform also achieve In this work SAMUEL, a learning system genetic algorithms learn high-performance reactive strategies navigation collision avoidance>
Paper 7:  <Title: A comparison of neural net and conventional techniques for lighting control  .   Abstract: We compare two techniques lighting control an actual room equipped seven banks photoresistors detect four sensing points Each bank lights independently set The task the device intensity levels achieve sensor readings One technique explored uses approximate the mapping sensor readings device intensity levels The other technique examined uses a conventional feedback control loop The neural network a>
Paper 8:  <Title: An Evolutionary Approach to Learning in Robots  .   Abstract: Evolutionary learning methods found several areas in intelligent robots In the approach described evolutionary algorithms explore alternative robot behaviors within as reducing the overall knowledge engineering effort This paper some initial results applying the SAMUEL genetic learning system a collision avoidance and for mobile robots>
Paper 9:  <Title: ADAPTIVE TESTING OF CONTROLLERS FOR AUTONOMOUS VEHICLES  .   Abstract: Autonomous vehicles likely require sophisticated software controllers maintain vehicle performance the presence vehicle faults The test and complex software controllers expected a challenging task The goal this e ffort apply from arti ficial intelligence evaluating an intelligent controller The approach involves subjecting a controller an adaptively chosen set fault scenarios within a vehicle simulator searching combinations>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Neural Networks

Neural Networks
Prediction:  Neural Networks
Is prediction correct?  True

Prediction: 1
Processing index 1903...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: DNA: A New ASOCS Model With Improved Implementation Potential  
Abstract: Abstract: A new class of highspeed, self-adaptive, massively parallel computing models called ASOCS (Adaptive Self-Organizing Concurrent Systems) has been proposed. Current analysis suggests that there may be problems implementing ASOCS models in VLSI using the hierarchical network structures originally proposed. The problems are not inherent in the models, but rather in the technology used to implement them. This has led to the development of a new ASOCS model called DNA (Discriminant-Node ASOCS) that does not depend on a hierarchical node structure for success. Three areas of the DNA model are briefly discussed in this paper: DNA's flexible nodes, how DNA overcomes problems other models have allocating unused nodes, and how DNA operates during processing and learning. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 2:  <Title: Word Perfect Corp. LIA: A Location-Independent Transformation for ASOCS Adaptive Algorithm 2  .   Abstract: Most Artificial Neural Networks a fixed topology during learning often suffer shortcomings as ANNs that use dynamic topologies have shown ability overcome many Adaptive Self Organizing Concurrent Systems learning models with inherently dynamic topologies This paper introduces Location-Independent Transformations implementing learning models that use dynamic topologies efficiently parallel hardware A LIT creates location-independent nodes where>
Label: Neural Networks
Paper 3:  <Title: A VLSI Implementation of a Parallel, Self-Organizing Learning Model  .   Abstract: This paper a VLSI implementation the Priority Adaptive Self-Organizing Concurrent System learning model built a multi-chip module Many current hardware implementations neural network learning models direct implementations structures|a large number simple computing nodes connected a dense number weighted links PASOCS is one a class ASOCSAdaptive Self-Organizing Concurrent System connectionist models whose overall goal the same classical neural networks models whose func>
Label: Neural Networks
Paper 7:  <Title: ASOCS: A Multilayered Connectionist Network with Guaranteed Learning of Arbitrary Mappings  .   Abstract: This paper features multilayer connectionist architectures known ASOCS ASOCS similar most decision-making neural network models attempts learn an adaptive set arbitrary vector mappings However differs dramatically its mechanisms ASOCS based networks adaptive digital elements which self using local information Function specification is entered incrementally use rules rather complete input-output vectors such a processing network extr>
Label: Neural Networks
Paper 4:  <Title: Word Perfect Corp. A TRANSFORMATION FOR IMPLEMENTING EFFICIENT DYNAMIC BACKPROPAGATION NEURAL NETWORKS  .   Abstract: Most Artificial Neural Networks during learning often suffer shortcomings as Variations ANNs that use dynamic topologies have shown ability overcome many This paper introduces Location-Independent Transformations implementing distributed feedforward networks that dynamic topologies (dynamic ANNs efficiently parallel hardware A LIT creates location-independent nodes where computes its part the network output independent>
Paper 5:  <Title: Word Perfect Corp. A TRANSFORMATION FOR IMPLEMENTING NEURAL NETWORKS WITH LOCALIST PROPERTIES  .   Abstract: Most Artificial Neural Networks during learning typically suffer shortcomings as Variations ANNs that use dynamic topologies have shown ability overcome many This paper introduces Location-Independent Transformations implementing feedforward networks that dynamic topologies A LIT creates location-independent nodes where computes its part independent using local information This type transformation all>
Paper 6:  <Title: Digital Neural Networks  .   Abstract: Demands applications requiring massive parallelism symbolic environments given rebirth research models labeled neura l networks These models many simple nodes highly interconnected such that computation data amongst To present most models proposed nodes based simple analog functions inputs multiplied weights summed the total then optionally being transformed at Learning in these systems accomplished adjusting>
Paper 8:  <Title: Connectionist Layered Object-Oriented Network Simulator (CLONES): User's Manual minimize the learning curve for using CLONES,.   Abstract: CLONES is a object-oriented library constructing, training utilizing layered connectionist networks The CLONES library all the object classes needed a simulator with a small amount added source code ( The size experimental ANN programs greatly an object-oriented library; at these programs easier evolve The library includes database network behavior training procedures customized It designed run efficiently data parallel computerssu>
Paper 9:  <Title: LEARNING TO CONTROL FAST-WEIGHT MEMORIES: AN ALTERNATIVE TO DYNAMIC RECURRENT NETWORKS (Neural Computation, 4(1):131-139, 1992)  .   Abstract: Previous algorithms supervised sequence learning dynamic recurrent networks This paper an alternative class gradient-based systems consisting two feedforward nets learn deal temporal sequences using fast weights produce context dependent weight changes whose weights may very The method offers STM storage efficiency: A single weightinstead may temporal information Various learning methods derived. Two experi>
Paper 10:  <Title: Neural Network Applicability: Classifying the Problem Space  .   Abstract: The tremendous current effort propose neurally inspired methods computation forces closer scrutiny real world application potential This paper categorizes applications into classes particularly discusses features applications make efficiently amenable neural network methods Computational machines do deterministic mappings many computational mechanisms problem solutions Neural network features parallel execution adaptive learning generalization fault Often, much e>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Neural Networks

Neural Networks
Prediction:  Neural Networks
Is prediction correct?  True

Prediction: 1
Processing index 1...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Applications of machine learning: a medical follow up study  
Abstract: Abstract: This paper describes preliminary work that aims to apply some learning strategies to a medical follow-up study. An investigation of the application of three machine learning algorithms-1R, FOIL and InductH to identify risk factors that govern the colposuspension cure rate has been made. The goal of this study is to induce a generalised description or explanation of the classification attribute, colposuspension cure rate (completely cured, improved, unchanged and worse) from the 767 examples in the questionnaires. We looked for a set of rules that described which risk factors result in differences of cure rate. The results were encouraging, and indicate that machine learning can play a useful role in large scale medical problem solving. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 9:  <Title: Knowledge Discovery in International Conflict Databases  .   Abstract: Artificial Intelligence heavily supported military institutions while practically no effort goes the investigation possible contributions the avoidance termination crises wars This paper makes into this direction discovering knowledge international conflict databases We applied similarity-based case retrieval the KOSIMO database international conflicts Furthermore we present analyzing the CONFMAN database successful un>
Label: Case Based
Paper 10:  <Title: REPRESENTING PHYSICAL AND DESIGN KNOWLEDGE IN INNOVATIVE DESIGN  .   Abstract: This paper An important part determining constituent groups or classes which best describes some data We apply the Minimum Message Length (MML) criterion modifying an earlier such MML application We give an empirical comparison criteria prominent estimating components We conclude the Minimum Message Length criterion performs the alternatives on considered>
Label: Case Based
Paper 2:  <Title: Machine Learning Methods for International Conflict Databases: A Case Study in Predicting Mediation Outcome  .   Abstract: This paper tries identify rules factors are predictive international conflict management attempts We use C4.5 an advanced Machine Learning algorithm generating decision trees prediction rules cases the CONFMAN database The results show simple patterns rules often only understandable reliable Simple decision trees able improve the chances correctly a conflict management attempt This suggests mediation repetitive conflicts pe>
Paper 3:  <Title: Application of Neural Networks for the Classification of Diffuse Liver Disease by Quantitative Echography  .   Abstract: Three different methods investigated their ability classify various categories diffuse liver disease A statistical method a supervised neural network called and were examined The investigation performed a previously selected set acoustic and image texture parameters The limited number patients was successfully extended generating additional but independent data identical statistical properties The generated data>
Paper 4:  <Title: Induction of decision trees and Bayesian classification applied to diagnosis of sport injuries  .   Abstract: Machine learning techniques extract knowledge stored medical databases In our application various machine learning algorithms extract diagnostic knowledge to support sport injuries The applied methods include variants the Assistant algorithm top-down induction decision trees variants The available dataset insufficent reliable diagnosis all sport injuries considered the system expert-defined diagnostic rules were added used preclassifiers or g>
Paper 5:  <Title: Rule Generation and Compaction in the wwtp  .   Abstract: In learning classification rules data We sketch two modules our architecture namely LINNEO + and GAR LINNEO +, which a knowledge acquisition tool ill-structured domains automatically generating classes examples incrementally works LINNEO + 's output, a representation the conceptual structure classes the input GAR that classification rules GAR can generate both conjunctive and disjunctive r>
Paper 6:  <Title: Drug design by machine learning: Modelling drug activity  .   Abstract: This paper modelling drug activity machine learning tools Some experiments modelling using a standard, Hansch, method and Golem were already The paper describes applying two other machine learning systems Magnus Assistant Retis The results achieved the machine learning systems, are better then the Hansch method; therefore considered very>
Paper 7:  <Title: Simple Genetic Programming for Supervised Learning Problems  .   Abstract: This paper finding learning rules to several supervised tasks In this approach potential solutions represented variable length mathematical LISP S-expressions Thus similar Genetic ProgrammingGP employs non-problem-specific functions In three Monk's and parity problems tested The results indicate the usefulness the encoding schema discovering learning rules supervised learning problems with the emphasis hard learning problems The problems and f>
Paper 8:  <Title: Computation and Psychophysics of Sensorimotor Integration  .   Abstract: In learning classification rules data We sketch two modules our architecture namely LINNEO + and GAR LINNEO +, which a knowledge acquisition tool ill-structured domains automatically generating classes examples incrementally works LINNEO + 's output, a representation the conceptual structure classes the input GAR that classification rules GAR can generate both conjunctive and disjunctive r>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Rule Learning

Case Based
Prediction:  Case Based
Is prediction correct?  False

Prediction: 0
Processing index 50...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Abstract  
Abstract: Abstract: Metacognition addresses the issues of knowledge about cognition and regulating cognition. We argue that the regulation process should be improved with growing experience. Therefore mental models are needed which facilitate the re-use of previous regulation processes. We will satisfy this requirement by describing a case-based approach to Introspection Planning which utilises previous experience obtained during reasoning at the meta-level and at the object level. The introspection plans used in this approach support various metacognitive tasks which are identified by the generation of self-questions. As an example of introspection planning, the metacognitive behaviour of our system, IULIAN, is described. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 2:  <Title: Multi-Strategy Learning and Theory Revision  .   Abstract: This paper the system WHY, which learns and updates a diagnostic knowledge base domain knowledge a set examples The a-priori knowledge consists a causal model the domain stating basic phenomena a body describing the links abstract concepts their possible manifestations The phenomenological knowledge is used deductively the causal model abductively the examples The problems imperfection the theory handled al>
Paper 3:  <Title: Computation and Psychophysics of Sensorimotor Integration  .   Abstract: In learning classification rules data We sketch two modules our architecture namely LINNEO + and GAR LINNEO +, which a knowledge acquisition tool ill-structured domains automatically generating classes examples incrementally works LINNEO + 's output, a representation the conceptual structure classes the input GAR that classification rules GAR can generate both conjunctive and disjunctive r>
Paper 4:  <Title: Learning Concepts from Sensor Data of a Mobile Robot  .   Abstract: Machine learning improving the flexibility robot applications Many approaches applying robotics known Some approaches enhance the robot's high-level processing the planning capabilities. Other approaches enhance the control basic actions. In contrast the approach presented uses for enhancing the link sensing action planning The aim facilitate the communicati>
Paper 5:  <Title: Adaptive Tuning of Numerical Weather Prediction Models: Simultaneous Estimation of Weighting, Smoothing and Physical Parameters 1  .   Abstract: In case-based reasoning demonstrated problem complex domains Also mixed paradigm approaches emerged combining CBR and induction techniques aiming verifying the knowledge building an efficient case memory However in complex domains induction over the whole problem space or too time In this paper an approach which (owing a close interaction the CBR part attempts induce rules only a particular context a problem just beingor>
Paper 6:  <Title: Rule Generation and Compaction in the wwtp  .   Abstract: In learning classification rules data We sketch two modules our architecture namely LINNEO + and GAR LINNEO +, which a knowledge acquisition tool ill-structured domains automatically generating classes examples incrementally works LINNEO + 's output, a representation the conceptual structure classes the input GAR that classification rules GAR can generate both conjunctive and disjunctive r>
Paper 7:  <Title: Computer-Supported Argumentation for Cooperative Design on the World-Wide Web  .   Abstract: This paper an argumentation system cooperative design applications on The system provides experts involved such procedures means of expressing weighing their individual arguments preferences argue the selection a certain choice It supports defeasible and qualitative reasoning the presence ill-structured information Argumentation performed through discourse acts call a variety procedures the corresponding discussion graph The paper reports in>
Paper 8:  <Title: What online Machine Learning can do for Knowledge Acquisition A Case Study  .   Abstract: This paper reports a realistic knowledge-based application using the MOBAL system Some problems and requirements resulting industrial-caliber tasks formulated step account a knowledge base such a task demonstrates the interleaved use several learning algorithms concert an inference engine fulfill those requirements Design, analysis revision refinement extension combined one incremental process This illustrates the balanced cooperative mo>
Paper 9:  <Title: Lazy Induction Triggered by CBR  .   Abstract: In case-based reasoning demonstrated problem complex domains Also mixed paradigm approaches emerged combining CBR and induction techniques aiming verifying the knowledge building an efficient case memory However in complex domains induction over the whole problem space or too time In this paper an approach which (owing a close interaction the CBR part attempts induce rules only a particular context a problem just beingor>
Paper 10:  <Title: Generic Teleological Mechanisms and their Use in Case Adaptation  .   Abstract: In experience-based (or case-based) reasoning new problems retrieving adapting similar problems encountered An important issue experience-based reasoning to identify different types knowledge reasoning useful different classes case-adaptation tasks In this paper a class non-routine case-adaptation tasks patterned insertions new elements old solutions We describe solving this task the design physical devices The method uses knowledge o>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Case Based

Case Based
Prediction:  Case Based
Is prediction correct?  True

Prediction: 1
Processing index 2534...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Data Value Prediction Methods and Performance  
Abstract: Abstract: A quantitative model is provided for psychophysical data on the tracking of multiple visual elements (multielement tracking). The model employs an object-based attentional mechanism for constructing and updating object representations. The model selectively enhances neural activations to serially construct and update the internal representations of objects through correlation-based changes in synaptic weights. The correspondence problem between items in memory and elements in the visual input is resolved through a combination of top-down prediction signals and bottom-up grouping processes. Simulations of the model on image sequences used in multielement tracking experiments show that reported results are consistent with a serial tracking mechanism that is based on psychophysical and neurobiological findings. In addition, simulations show that observed effects of perceptual grouping on tracking accuracy may result from the interactions between attention-guided predictions of object location and motion and grouping processes involved in solving the motion correspondence problem. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 7:  <Title: Implicit learning in 3D object recognition: The importance of temporal context  .   Abstract: A novel architecture and set learning rules cortical self-organization The model the idea multiple information channels modulate plasticity Features learned bottom-up information sources can thus influenced those learned contextual pathways vice A maximum likelihood cost function allows this scheme a biologically feasible, hierarchical neural circuit In simulations the model first demonstrate the utility temporal context plasticity The model learns repr>
Label: Neural Networks
Paper 8:  <Title: Priming, Perceptual Reversal, and Circular Reaction in a Neural Network Model of Schema-Based Vision  .   Abstract: VISOR is a neural network system scene analysis learns visual schemas examples Processing in VISOR based cooperation, competition parallel and schema representations Similar principles appear much human visual processing VISOR therefore model This paper focuses analyzing three phenomena through simulation with VISOR priming mental perceptual reversal circular reaction The results illustrate similarity an>
Label: Neural Networks
Paper 2:  <Title: An Object-Based Neural Model of Serial Processing in Visual Multielement Tracking  .   Abstract: A quantitative model provided psychophysical data the tracking multiple visual elementsmultielement tracking The model employs constructing updating The model selectively enhances serially construct update objects through correlation-based changes synaptic weights The correspondence problem between items memory elements resolved top-down prediction signals bottom grouping proc>
Paper 3:  <Title: Solving the Temporal Binding Problem: A Neural Theory for Constructing and Updating Object Files  .   Abstract: Visual objects perceived only their parts correctly integrated A neural network theory is proposed seeks binds visual properties dispersed space of a problem known the temporal binding problem [49 30 The proposed theory neural mechanisms construct update object representations through a serial attentional mechanism for location object-based selection preattentive Gestalt-based grouping mechanisms an associative mem>
Paper 4:  <Title: Computational Models of Sensorimotor Integration  Computational Maps and Motor Control.  .   Abstract: The sensorimotor integration system can viewed an observer attempting estimate its own state and by integrating multiple sources We describe a computational framework capturing this notion some specific models integration adaptation result Psychophysical results two sensorimotor systems subserving the integration adaptation visuo-auditory maps estimation the state the hand arm movements and within this framework These results: Spatia>
Paper 5:  <Title: Cortical Mechanisms of Visual Recognition and Learning: A Hierarchical Kalman Filter Model  .   Abstract: We describe dynamic recognition based the statistical theory Kalman filtering from optimal control theory The model utilizes a hierarchical network whose successive levels implement Kalman filters operating over successively spatial Each hierarchical level predicts the current visual recognition state adapts using the residual error between>
Paper 6:  <Title: A Theory of Visual Relative Motion Perception: Grouping, Binding, and Gestalt Organization  .   Abstract: The human visual system more sensitive the relative motion to their absolute motion An understanding motion perception requires neural circuits can group moving visual elements relative based hierarchical reference frames We modeled visual relative motion perception a neural network architecture groups visual elements according Gestalt common-fate principles exploits information the behavior each group A simple competitive neural circ>
Paper 9:  <Title: A Model of Invariant Object Recognition in the Visual System  .   Abstract: Neurons the ventral stream the primate visual system exhibit responses the images objects which invariant with natural transformations such translation size view Anatomical and neurophysiological evidence this achieved hierarchical processing areas In elucidate the manner such representations established constructed cortical visual processing which seeks parallel many features this system specifically the multi-stage hierarchy with its topologically constra>
Paper 10:  <Title: A Neural Network Model of Visual Tilt Aftereffects  .   Abstract: RF-LISSOM, a self-organizing model laterally connected orientation maps in the psychological phenomenon known the tilt aftereffect The same self-organizing processes are the map its lateral connections shown result tilt aftereffects over the adult. The model allows observing large numbers neurons connections simultaneously making relate higher-level phenomena which difficult experimentally The re>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Rule Learning

Neural Networks
Prediction:  Neural Networks
Is prediction correct?  False

Prediction: 0
Processing index 122...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Tilt Aftereffects in a Self-Organizing Model of the Primary Visual Cortex  
Abstract: Abstract: Previous neural network learning algorithms for sequence processing are computationally expensive and perform poorly when it comes to long time lags. This paper first introduces a simple principle for reducing the descriptions of event sequences without loss of information. A consequence of this principle is that only unexpected inputs can be relevant. This insight leads to the construction of neural architectures that learn to `divide and conquer' by recursively decomposing sequences. I describe two architectures. The first functions as a self-organizing multi-level hierarchy of recurrent networks. The second, involving only two recurrent networks, tries to collapse a multi-level predictor hierarchy into a single recurrent net. Experiments show that the system can require less computation per time step and many fewer training sequences than conventional training algorithms for recurrent nets.
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 3:  <Title: Object Selection Based on Oscillatory Correlation  .   Abstract: 1 Technical Report OSU-CISRC-12/96 - TR67, 1996 Abstract One the classical topics neural networks winner ( widely unsupervised (competitive) learning, cortical processing Because global connectivity WTA networks, however do encode spatial relations the input, support sensory and perceptual processing where important We propose maintains spatial relations input features This selection network builds LEGION (Locally Excitatory G>
Label: Neural Networks
Paper 5:  <Title: Implicit learning in 3D object recognition: The importance of temporal context  .   Abstract: A novel architecture and set learning rules cortical self-organization The model the idea multiple information channels modulate plasticity Features learned bottom-up information sources can thus influenced those learned contextual pathways vice A maximum likelihood cost function allows this scheme a biologically feasible, hierarchical neural circuit In simulations the model first demonstrate the utility temporal context plasticity The model learns repr>
Label: Neural Networks
Paper 7:  <Title: Distributed Patterns as Hierarchical Structures  .   Abstract: Recursive Auto-Associative Memory (RAAM) structures show promise a general representation vehicle that uses distributed patterns However training which explains, only relatively small networks We show a technique transforming any collection hierarchical structures training patterns a sequential RAAM which effectively a simple (Elman-style) recurrent network Tr aining produces distributed patterns corresponding the structures.>
Label: Neural Networks
Paper 9:  <Title: Constructive Learning of Recurrent Neural Networks: Limitations of Recurrent Casade Correlation and a Simple Solution  .   Abstract: It often predict the optimal neural network size Constructive or destructive methods add neurons layers connections might offer We prove one method, Recurrent Cascade Correlation due fundamental limitations representation thus its learning capabilities It cannot represent with monotone (i.e. sigmoid) and hard-threshold activation functions certain finite state automata We give a "preliminary" approach on these limitations devis>
Label: Neural Networks
Paper 2:  <Title: LEARNING COMPLEX, EXTENDED SEQUENCES USING THE PRINCIPLE OF HISTORY COMPRESSION (Neural Computation, 4(2):234-242, 1992)  .   Abstract: Previous neural network learning algorithms for sequence processing perform it long time lags This paper first introduces a simple principle reducing the descriptions event sequences without loss A consequence this principle only unexpected inputs relevant This insight leads the construction neural architectures learn `divide decomposing sequences I describe two architectures The first functions as rec>
Paper 4:  <Title: LEARNING TO CONTROL FAST-WEIGHT MEMORIES: AN ALTERNATIVE TO DYNAMIC RECURRENT NETWORKS (Neural Computation, 4(1):131-139, 1992)  .   Abstract: Previous algorithms supervised sequence learning dynamic recurrent networks This paper an alternative class gradient-based systems consisting two feedforward nets learn deal temporal sequences using fast weights produce context dependent weight changes whose weights may very The method offers STM storage efficiency: A single weightinstead may temporal information Various learning methods derived. Two experi>
Paper 6:  <Title: Using Many-Particle Decomposition to get a Parallel Self-Organising Map  .   Abstract: We propose decreasing self-organising maps The method uses a partitioning of the neurons Teaching of the neurons occurs on a cluster-basis instead For teaching an N-neuron network with N 0 samples the computational complexity decreases O(N N 0 log Furthermore introduce a measure order a self-organising map show the introduced algorithm behaves well>
Paper 8:  <Title: Generative Models for Discovering Sparse Distributed Representations  .   Abstract: We describe viewed factor analysis can The model uses bottom-up perform Bayesian perceptual inference correctly Once perceptual inference the connection strengths updated a very simple learning rule only requires locally available information We demon strate that the network learns extract sparse distributed, hierarchical representations>
Paper 10:  <Title: Pruning Recurrent Neural Networks for Improved Generalization Performance  .   Abstract: Determining the architecture any learning task For recurrent neural networks no general methods permit the estimation layers hidden neurons layers weights We present a simple pruning heuristic which significantly trained recurrent networks We illustrate this heuristic training positive and negative strings a regular grammar We also show if rules extracted networks trained re>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Neural Networks

Neural Networks
Prediction:  Neural Networks
Is prediction correct?  True

Prediction: 1
Processing index 2335...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Function Approximation with Neural Networks and Local Methods: Bias, Variance and Smoothness  
Abstract: Abstract: We review the use of global and local methods for estimating a function mapping R m ) R n from samples of the function containing noise. The relationship between the methods is examined and an empirical comparison is performed using the multi-layer perceptron (MLP) global neural network model, the single nearest-neighbour model, a linear local approximation (LA) model, and the following commonly used datasets: the Mackey-Glass chaotic time series, the Sunspot time series, British English Vowel data, TIMIT speech phonemes, building energy prediction data, and the sonar dataset. We find that the simple local approximation models often outperform the MLP. No criterion such as classification/prediction, size of the training set, dimensionality of the training set, etc. can be used to distinguish whether the MLP or the local approximation method will be superior. However, we find that if we consider histograms of the k-NN density estimates for the training datasets then we can choose the best performing method a priori by selecting local approximation when the spread of the density histogram is large and choosing the MLP otherwise. This result correlates with the hypothesis that the global MLP model is less appropriate when the characteristics of the function to be approximated varies throughout the input space. We discuss the results, the smoothness assumption often made in function approximation, and the bias/variance dilemma. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 2:  <Title: Comparing Predictive Inference Methods for Discrete Domains  .   Abstract: Predictive inference seen here determining the predictive distribution given training examples the values the other problem domain variables We consider three approaches computing this predictive distribution assume the joint probability distribution the variables belongs distributions determined parametric models In the simplest case the predictive distribution computed with the maximum a posteriori (MAP) posterior probability In the evidence approac>
Label: Probabilistic Methods
Paper 3:  <Title: A Bootstrap Evaluation of the Effect of Data Splitting on Financial Time Series  .   Abstract: This article exposes problems the commonly used technique splitting the available data training, that held fixed warns drawing such static splits shows ignoring variability across splits Using a bootstrap or resampling compare the uncertainty the solution stemming the data splitting neural network specific uncertaintiesparameter initialization choice number hidden units We present two results data First>
Paper 4:  <Title: Packet Routing and Reinforcement Learning: Estimating Shortest Paths in Dynamic Graphs  .   Abstract: This article exposes problems the commonly used technique splitting the available data training, that held fixed warns drawing such static splits shows ignoring variability across splits Using a bootstrap or resampling compare the uncertainty the solution stemming the data splitting neural network specific uncertaintiesparameter initialization choice number hidden units We present two results data First>
Paper 5:  <Title: Comparison of Kernel Estimators, Perceptrons, and Radial-Basis Functions for OCR and Speech Classification  .   Abstract: We compare kernel estimators radial-basis functions the problems classification handwritten digits By taking two different applications employing many techniques report here whereby a domain-independent assessment these learning methods possible We consider a feed-forward network with one hidden layer As examples the local methods kernel estimatorsnn Parzen windows generalized Grow and LearnCondensed Nearest Neighbo>
Paper 6:  <Title: What Size Neural Network Gives Optimal Generalization? Convergence Properties of Backpropagation  .   Abstract: Technical Report UMIACS-TR-96-22 and CS-TR-3617 Institute Advanced Computer Studies University Maryland 20742 Abstract One any machine learning paradigm how scales according problem size Using a task with known optimal training error and a prespecified maximum number training updates investigate the convergence algorithm respect a) the complexity the required function approximation in relation required an optimal solu>
Paper 7:  <Title: Cross-Validation and the Bootstrap: Estimating the Error Rate of a Prediction Rule  .   Abstract: A training set data has used construct a rule predicting future responses. What the error rate this rule The traditional answer given cross The cross-validation estimate prediction error nearly unbiased can highly This article discusses bootstrap estimates prediction error thought smoothed versions cross A particular bootstrap method, the 632+ rule shown to substantially cross a catalog 24 simulation experiments Besides providing poin>
Paper 8:  <Title: First experiments using a mixture of nonlinear experts for time series prediction  .   Abstract: This paper investigates the advantages the mixture experts (ME) model (introduced to the connectionist community [JJNH91 applied time series analysisWM95 on two time series where the dynamics is well The first series a computer-generated series consisting a mixture between a noise-free processthe quadratic mapa composition noisy linear autoregressive and There three main results the ME model produces single networks it disco>
Paper 9:  <Title: A Comparative Study of ID3 and Backpropagation for English Text-to-Speech Mapping  .   Abstract: The performance the error backpropagation (BP and ID3 learning algorithms compared on the task mapping English text stresses Under the distributed output code developed Sejnowski Rosenberg shown BP consistently out ID3 on this task Three hypotheses explaining this difference) ID3 overfitting the training data able share hidden units across several output units hence can learn better captures statistical information ID3>
Paper 10:  <Title: Avoiding overfitting by locally matching the noise level of the data gating network discovers the.   Abstract: When trying forecast two of nonstationarity of the process, regime switching overfittingparticularly serious for noisy processes This articles shows gated experts point solutions The architecture, also society of experts mixture experts consists a (nonlinear) gating network Each expert learns a conditional meanas usual its own adaptive width The gating network learns a>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Neural Networks

Neural Networks
Prediction:  Neural Networks
Is prediction correct?  True

Prediction: 1
Processing index 540...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: A Model-Based Approach to Blame-Assignment in Design  
Abstract: Abstract: We analyze the blame-assignment task in the context of experience-based design and redesign of physical devices. We identify three types of blame-assignment tasks that differ in the types of information they take as input: the design does not achieve a desired behavior of the device, the design results in an undesirable behavior, a specific structural element in the design misbehaves. We then describe a model-based approach for solving the blame-assignment task. This approach uses structure-behavior-function models that capture a designer's comprehension of the way a device works in terms of causal explanations of how its structure results in its behaviors. We also address the issue of indexing the models in memory. We discuss how the three types of blame-assignment tasks require different types of indices for accessing the models. Finally we describe the KRITIK2 system that implements and evaluates this model-based approach to blame assignment.
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 4:  <Title: Use of Mental Models for Constraining Index Learning in Experience-Based Design  .   Abstract: The power the case-based method comes retrieve when a new problem specified This implies learning the "right" indices a case before storing for potential reuse crucial A hierarchical organization the case memory raises two distinct but related issues in index learning learning the indexing vocabulary the right level generalization In this paper structure-behavior-function (SBF) models index learning experience-based design>
Label: Case Based
Paper 2:  <Title: Learning Problem-Solving Concepts by Reflecting on Problem Solving  .   Abstract: Learning and problem intimately: problem determines the knowledge requirements the reasoner which learning must fulfill enables improved Different models problem recognize different knowledge needs, set different learning tasks Some recent models analyze problem solving generic tasks methods subtasks These models require the learning new tasks new task decompositions We view reflection a core process le>
Paper 3:  <Title: Functional Representation as Design Rationale  .   Abstract: Design rationale is a record design activity: of alternatives available choices the reasons explanations a proposed design intended We describe a representation called has how a device's functions arise causally the functions We propose FR can provide the basis capturing the causal aspects the design rationale We briefly FR for tasks which would expect the design rationale u>
Paper 5:  <Title: Generic Teleological Mechanisms and their Use in Case Adaptation  .   Abstract: In experience-based (or case-based) reasoning new problems retrieving adapting similar problems encountered An important issue experience-based reasoning to identify different types knowledge reasoning useful different classes case-adaptation tasks In this paper a class non-routine case-adaptation tasks patterned insertions new elements old solutions We describe solving this task the design physical devices The method uses knowledge o>
Paper 6:  <Title: GIT-CC-92/60 A Model-Based Approach to Analogical Reasoning and Learning in Design  .   Abstract: A major issue case-basedsystems retrieving the appropriate cases solve This implies a case indexed appropriately stored A case-based system, being dynamic stores cases reuse needs learn indices the new knowledge as the system designers envision that knowledge Irrespective indexing (structural a hierarchical organization the case memory raises index learning learning the indexing vocabulary the right leve>
Paper 7:  <Title: CABINS A Framework of Knowledge Acquisition and Iterative Revision for Schedule Improvement and Reactive Repair  .   Abstract: Mixed-initiative systems present the challenge finding an effective level interaction humans computers Machine learning presents in systems automatically adapt accommodate different users In learning user models an adaptive assistant crisis scheduling We describe the problem domain the scheduling assistant then present an initial formulation the adaptive assistant's learning task a baseline study After this re>
Paper 8:  <Title: Learning to Predict User Operations for Adaptive Scheduling  .   Abstract: Mixed-initiative systems present the challenge finding an effective level interaction humans computers Machine learning presents in systems automatically adapt accommodate different users In learning user models an adaptive assistant crisis scheduling We describe the problem domain the scheduling assistant then present an initial formulation the adaptive assistant's learning task a baseline study After this re>
Paper 9:  <Title: Problem Solving for Redesign  .   Abstract: A knowledge-level analysis complex tasks like diagnosis design can give these tasks the goals In this paper a knowledge-level analysis redesign. Redesign is viewed a family methods based some common principles a number dimensions along redesign problem solving methods vary are distinguished By examining the problem-solving behavior a number existing redesign systems approaches came a collection problemso>
Paper 10:  <Title: EXPLANATORY INTERFACE IN INTERACTIVE DESIGN ENVIRONMENTS  .   Abstract:  is an important issue building computer-based interactive design environments in a knowledge system may cooperatively We consider the two related problems explaining the system's reasoning the design generated In particular analyze explanations design reasoning and design solutions physical devices We describe two complementary languages task-method-knowledge models for explaining design reasoning devic>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Case Based

Case Based
Prediction:  Case Based
Is prediction correct?  True

Prediction: 1
Processing index 537...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Adaptive Global Optimization with Local Search  
Abstract: Abstract: DIMACS Technical Report 96-56 December 1996 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 4:  <Title: Constructive Training Methods for Feedforward Neural Networks with Binary Weights  .   Abstract:  9535 August>
Label: Neural Networks
Paper 6:  <Title: Dimension of Recurrent Neural Networks  .   Abstract:  9656 December>
Label: Neural Networks
Paper 2:  <Title: Learning and evolution in neural networks  .   Abstract:  9656 December>
Paper 3:  <Title: Design of Optimization Criteria for Multiple Sequence Alignment  .   Abstract:  9653 January>
Paper 5:  <Title: DNA Sequence Classification Using Compression-Based Induction  .   Abstract: DIMACS Technical Report 95 April>
Paper 7:  <Title: of nucleotide sites needed to accurately reconstruct large evolutionary trees 1  .   Abstract: DIMACS Technical Report 96 July>
Paper 8:  <Title: Average-Case Analysis of a Nearest Neighbor Algorithm  .   Abstract:  Combinatorial Optimization John William Prior Report AI98268 May>
Paper 9:  <Title: Recursive Automatic Algorithm Selection for Inductive Learning  .   Abstract: COINS Technical Report 94 August>
Paper 10:  <Title: Extended Selection Mechanisms in Genetic Algorithms  .   Abstract: A Genetic Algorithm Tutorial Darrell Whitley Technical Report CS-93103 1993>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Genetic Algorithms

Probabilistic Methods
Prediction:  Probabilistic Methods
Is prediction correct?  False

Prediction: 0
Processing index 2265...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: AN APPROACH TO A PROBLEM IN NETWORK DESIGN USING GENETIC ALGORITHMS  
Abstract: Abstract: Air Traffic Control is involved in the real-time planning of aircraft trajectories. This is a heavily constrained optimization problem. We concentrate on free-route planning, in which aircraft are not required to fly over way points. The choice of a proper representation for this real-world problem is non-trivial. We propose a two level representation: one level on which the evolutionary operators work, and a derived level on which we do calculations. Furthermore we show that a specific choice of the fitness function is important for finding good solutions to large problem instances. We use a hybrid approach in the sense that we use knowledge about air traffic control by using a number of heuristics. We have built a prototype of a planning tool, and this resulted in a flexible tool for generating a free-route planning of low cost, for a number of aircraft. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 3:  <Title: Evolution of Pseudo-colouring Algorithms for Image Enhancement with Interactive Genetic Programming  .   Abstract: Technical Report CSRP-97-5 School The University Birmingham Abstract the interactive development programs image enhancement Genetic Programming pseudo-colour transformations In our approach the user drives GP by deciding should the winner tournament selection The presence the user does only running GP without a fitness function transforms a very efficient search procedure capable producing effective solutions o>
Label: Genetic Algorithms
Paper 6:  <Title: The Application of a Parallel Genetic Algorithm to the n=m=P=C max Flowshop Problem  .   Abstract: Hard combinatorial problems sequencing scheduling led recently into Canonical coding can modified into the n-job m-machine flowshop problem configurates We show well known genetic operators act this coding scheme They implecitely prefer a subset solutions contain an objective We conjecture every new problem needs a determination this necessary condition a genetic algori>
Label: Genetic Algorithms
Paper 10:  <Title: An Analysis of the MAX Problem in Genetic Programming hold only in some cases, in.   Abstract: We present genetic programmingGP populations the problem finding a program returns and function set a depth limitknown the MAX problem We confirm the basic message [ Gathercole and Ross 1996 crossover together program size restrictions responsible premature convergence to We show this can retains variety show in evolution from t>
Label: Genetic Algorithms
Paper 2:  <Title: Evolutionary Computation in Air Traffic Control Planning  .   Abstract: Air Traffic Control is involved aircraft trajectories This a heavily constrained optimization problem We concentrate free-route planning in aircraft not way points The choice a proper representation this real-world problem non We propose a two level representation: on the evolutionary operators work a derived level we do calculations Furthermore we show a specific choice the fitness function finding good solutions large problem instances>
Paper 4:  <Title: Genetic Algorithm based Scheduling in a Dynamic Manufacturing Environment  .   Abstract: The application adaptive optimization strategies scheduling in manufacturing systems recently Population based approaches scheduling predominantly treat static data models whereas tends This paper briefly the dynamic job shop problem arising production scheduling First we sketch a genetic algorithm handle release times jobs In a second step a preceding simulation method improve the al>
Paper 5:  <Title: Adaptation of Genetic Algorithms for Engineering Design Optimization  .   Abstract: Genetic algorithms extensively different domains as doing global optimization However some realistic engineering design optimization domains was observed a simple classical implementation the GA based binary encoding bit mutation and crossover sometimes inefficient unable reach the global optimum Using floating point representation alone eliminate In augmenting the GA with new operators strategies take the structur>
Paper 7:  <Title: PRONOUNCING NAMES BY A COMBINATION OF RULE-BASED AND CASE-BASED REASONING  .   Abstract: We describe tuning a controller enforcing compliance a prescribed velocity profile This requires following a trajectory rather fixed set-points automobiles We synthesize a fuzzy controller tracking the velocity profile while staying We use a genetic algorithm tune the fuzzy controller's performance (the scaling factors the membership functions of significance We show this>
Paper 8:  <Title: Genetic Algorithms for Automated Tuning of Fuzzy Controllers: A Transportation Application  .   Abstract: We describe tuning a controller enforcing compliance a prescribed velocity profile This requires following a trajectory rather fixed set-points automobiles We synthesize a fuzzy controller tracking the velocity profile while staying We use a genetic algorithm tune the fuzzy controller's performance (the scaling factors the membership functions of significance We show this>
Paper 9:  <Title: Feature Selection by Means of a Feature Weighting Approach  .   Abstract: Selecting features which optimal a given classification task We address using the flexible and robust filter technique EUBAFES EUBAFES based a feature weighting approach binary feature weights therefore a solution the feature selection sense and gives feature relevance by continuous weights Moreover the user gets not one several potentially optimal feature subsets filter-based feature selection algorithms since i>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Genetic Algorithms

Genetic Algorithms
Prediction:  Genetic Algorithms
Is prediction correct?  True

Prediction: 1
Processing index 2205...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: A Genetic Local Search Approach to the Quadratic Assignment Problem  
Abstract: Abstract: Augmenting genetic algorithms with local search heuristics is a promising approach to the solution of combinatorial optimization problems. In this paper, a genetic local search approach to the quadratic assignment problem (QAP) is presented. New genetic operators for realizing the approach are described, and its performance is tested on various QAP instances containing between 30 and 256 facilities/locations. The results indicate that the proposed algorithm is able to arrive at high quality solutions in a relatively short time limit: for the largest publicly known prob lem instance, a new best solution could be found.
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 3:  <Title: A Genetic Algorithm for Continuous Design Space Search  .   Abstract: Genetic algorithms ( extensively as performing global optimization However some realistic engineering design optimization domains the simple, classical implementation a GA based binary encoding bit mutation and crossover often inefficient unable reach the global optimum In a GA for continuous design-space optimization new GA operators strategies tailored the structure properties engineering design domains the domains superson>
Label: Genetic Algorithms
Paper 4:  <Title: A GENETIC ALGORITHM FOR FRAGMENT ALLOCATION IN A DISTRIBUTED DATABASE SYSTEM  .   Abstract: In explore the distributed database allocation problem intractable. We also discuss genetic algorithms have successfully Our experimental results the GA to far the greedy heuristic obtaining optimal and near optimal fragment placements the allocation problem with various data sets>
Label: Genetic Algorithms
Paper 7:  <Title: An Evolutionary Approach to Time Constrained Routing Problems  .   Abstract: Routing problems an important class planning problems Usually there many different constraints optimization criteria involved find general methods solving routing problems We propose an evolutionary solver such planning problems An instance this solver has tested a specific routing problem time constraints The performance this evolutionary solver compared a biased random solver Results show the evolutionary solver performs significantly>
Label: Genetic Algorithms
Paper 2:  <Title: An Evolutionary Approach to Combinatorial Optimization Problems  .   Abstract:  paper reports the application genetic algorithms based the model organic evolution NP-complete combinatorial optimization problems In particular the subset sum, maximum cut minimum tardy task problems considered Except the fitness function no problem-specific changes of the genetic algorithm results of high quality even the problem instances size 100 used For constrained problems the subset sum and the minimum tardy task the constraints taken>
Paper 5:  <Title: An evolutionary tabu search algorithm and the NHL scheduling problem  .   Abstract: We present a new evolutionary procedure solving combines efficiently the mechanisms genetic algorithms tabu search In order explore the solution space properly interaction phases periods optimization An adaptation this search principle to National Hockey problem discussed The hybrid method developed well Open Shop Scheduling problemsOSSP The results obtained appear quite satisfactory>
Paper 6:  <Title: Adaptation of Genetic Algorithms for Engineering Design Optimization  .   Abstract: Genetic algorithms extensively different domains as doing global optimization However some realistic engineering design optimization domains was observed a simple classical implementation the GA based binary encoding bit mutation and crossover sometimes inefficient unable reach the global optimum Using floating point representation alone eliminate In augmenting the GA with new operators strategies take the structur>
Paper 8:  <Title: Graph Coloring with Adaptive Evolutionary Algorithms  .   Abstract: This paper solving graph coloringEA After testing different algorithm variants we conclude an asexual EA order-based representation an adaptation mechanism periodically during This adaptive EA general using no domain specific knowledge except, from (fitness function We compare this adaptive EA a powerful traditional graph coloring technique DSatur and the Grouping GA on>
Paper 9:  <Title: A Genetic Algorithm for File and Task Placement in a Distributed System  .   Abstract: In explore the distributed file and task placement problem We also discuss genetic algorithms have successfully Our experimental results the GA to far the greedy heuristic obtaining optimal and near optimal file and task placements the problem with various data sets>
Paper 10:  <Title: GENE REGULATION AND BIOLOGICAL DEVELOPMENT IN NEURAL NETWORKS: AN EXPLORATORY MODEL  .   Abstract: In explore the distributed database allocation problem intractable. We also discuss genetic algorithms have successfully Our experimental results the GA to far the greedy heuristic obtaining optimal and near optimal fragment placements the allocation problem with various data sets>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Genetic Algorithms

Genetic Algorithms
Prediction:  Genetic Algorithms
Is prediction correct?  True

Prediction: 1
Processing index 2143...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: MULTIPLE SCALES OF BRAIN-MIND INTERACTIONS  
Abstract: Abstract: Posner and Raichle's Images of Mind is an excellent educational book and very well written. Some aws as a scientific publication are: (a) the accuracy of the linear subtraction method used in PET is subject to scrutiny by further research at finer spatial-temporal resolutions; (b) lack of accuracy of the experimental paradigm used for EEG complementary studies. Images (Posner & Raichle, 1994) is an excellent introduction to interdisciplinary research in cognitive and imaging science. Well written and illustrated, it presents concepts in a manner well suited both to the layman/undergraduate and to the technical nonexpert/graduate student and postdoctoral researcher. Many, not all, people involved in interdisciplinary neuroscience research agree with the P & R's statements on page 33, on the importance of recognizing emergent properties of brain function from assemblies of neurons. It is clear from the sparse references that this book was not intended as a standalone review of a broad field. There are some aws in the scientific development, but this must be expected in such a pioneering venture. P & R hav e proposed many cognitive mechanisms deserving further study with imaging tools yet to be developed which can yield better spatial-temporal resolutions. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 3:  <Title: Application of statistical mechanics methodol- ogy to term-structure bond-pricing models, Mathl. Comput. Modelling Application of.   Abstract: The work in reported Wright Liley shows great promise primarily their experimental and simulation paradigms However their tentative conclusion macroscopic neocortex may considered (approximately a linear near-equilibrium system premature does correspond tentative conclusions At exists an interdisciplinary multidimensional gradation on published studies neocortex one primary dimension mathematical physics represented two extremes At one extreme much scie>
Label: Neural Networks
Paper 7:  <Title: Book Review New Kids on the Block way in the field of connectionist modeling. The.   Abstract: Connectionist Models is forty papers representing research topics The book distinguished a single feature the papers almost exclusively contributions graduate students active The students selected participated a two week long summer school devoted connectionism 2. As the ambitious editors state These bold claims, the reader presented sample the frontiers connectionism Their words imply two>
Label: Neural Networks
Paper 10:  <Title: Replicability of Neural Computing Experiments  .   Abstract: If an experiment requires statistical analysis establish one do a better experiment Ernest Rutherford 1930 Most proponents cold fusion reporting excess heat their electrolysis experiments claiming one its irreproducibility | 1993 78 Abstract Amid the ever increasing research various aspects neural computing much progress evident both theoretical advances On the empirical side a wealth experimental studies>
Label: Neural Networks
Paper 2:  <Title: Book Review  Introduction to the Theory of Neural Computation Reviewed by: 2  .   Abstract: Neural computation, also connectionism parallel distributed processing neural network modeling or brain-style computation grown Despite this explosion and ultimately because impressive applications a dire need a concise introduction from a theoretical perspective analyzing the strengths connectionist approaches establishing links statistics control theory The Introduction the Theory Neural Computation by Hertz Krogh Palmersubsequently referred>
Paper 4:  <Title: Evaluating and Improving Steady State Evolutionary Algorithms on Constraint Satisfaction Problems  .   Abstract: The work in reported Wright Liley shows great promise primarily their experimental and simulation paradigms However their tentative conclusion macroscopic neocortex may considered (approximately a linear near-equilibrium system premature does correspond tentative conclusions At exists an interdisciplinary multidimensional gradation on published studies neocortex one primary dimension mathematical physics represented two extremes At one extreme much scie>
Paper 5:  <Title: NONLINEAR NONEQUILIBRIUM NONQUANTUM NONCHAOTIC STATISTICAL MECHANICS OF NEOCORTICAL INTERACTIONS  .   Abstract: The work in reported Wright Liley shows great promise primarily their experimental and simulation paradigms However their tentative conclusion macroscopic neocortex may considered (approximately a linear near-equilibrium system premature does correspond tentative conclusions At exists an interdisciplinary multidimensional gradation on published studies neocortex one primary dimension mathematical physics represented two extremes At one extreme much scie>
Paper 6:  <Title: New Roles for Machine Learning in Design for Design of Educational Computing New roles for.   Abstract: Research machine learning design concentrated and techniques solve Invariably this effort, while important at the field scale up address real design problems since all existing techniques based simplifying assumptions do hold In particular they do address the dependence context multiple, often interests constitutive design This paper the present situation criticizes>
Paper 8:  <Title: A Brief History of Connectionism  .   Abstract: Connectionist research firmly within especially cognitive science This diversity, however created which makes connectionist researchers remain aware recent advances let understand the field developed This paper attempts address a brief guide connectionist research The paper begins defining the basic tenets connectionism Next the development connectionist research traced, commencing>
Paper 9:  <Title: The New Challenge: From a Century of Statistics to an Age of Causation  .   Abstract: Some the main users statistical methods - economists are discovering their fields rest not but causal foundations The blurring these foundations over follows from mathematical notation capable causal from equational relationships By providing formal and natural explication such relations graphical methods how statistics knowledge-rich applications Statisticians in response beginning realize causality>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Neural Networks

Neural Networks
Prediction:  Neural Networks
Is prediction correct?  True

Prediction: 1
Processing index 763...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: PREENS, a Parallel Research Execution Environment for Neural Systems  
Abstract: Abstract: PREENS a Parallel Research Execution Environment for Neural Systems is a distributed neurosimulator, targeted on networks of workstations and transputer systems. As current applications of neural networks often contain large amounts of data and as the neural networks involved in tasks such as vision are very large, high requirements on memory and computational resources are imposed on the target execution platforms. PREENS can be executed in a distributed environment, i.e. tools and neural network simulation programs can be running on any machine connectable via TCP/IP. Using this approach, larger tasks and more data can be examined using an efficient coarse grained parallelism. Furthermore, the design of PREENS allows for neural networks to be running on any high performance MIMD machine such as a trans-puter system. In this paper, the different features and design concepts of PREENS are discussed. These can also be used for other applications, like image processing.
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 3:  <Title: University of Nevada Reno Design Strategies for Evolutionary Robotics  .   Abstract: CuPit-2 expressing dynamic neural network learning algorithms It provides most general-purpose languages ++ more expressive It allows writing much clearer and more elegant programs in algorithms change the network topology dynamically (constructive algorithms pruning algorithms In contrast other languages CuPit-2 programs compiled efficient code for parallel machines without any changes the source program an easy start u>
Label: Genetic Algorithms
Paper 6:  <Title: A Supercomputer for Neural Computation  .   Abstract: The requirement train large neural networks quickly prompted the design using custom VLSI. This design 128 processing nodes communicating connected directly the processor chip Studies peak performance the range 160 billion arithmetic operations This paper custom hardware combines neural network-specific features a general programmable machine architecture briefly in progress>
Label: Neural Networks
Paper 9:  <Title: Evolving Turing-Complete Programs for a Register Machine with Self-modifying Code  .   Abstract: The majority commercial computers today register machines of von Neumann type We developed evolve Turing-complete programs a register machine The described implementation enables most program constructs arithmetic operators large indexed memory automatic decomposition into subfunctionsADFs conditional constructs if jumps loop structures protected string and list functions Any C-function can compiled linked the function set the system The use register mac>
Label: Genetic Algorithms
Paper 2:  <Title: CuPit-2: Portable and Efficient High-Level Parallel Programming of Neural Networks for the Systems Analysis Modelling.   Abstract: CuPit-2 expressing dynamic neural network learning algorithms It provides most general-purpose languages ++ more expressive It allows writing much clearer and more elegant programs in algorithms change the network topology dynamically (constructive algorithms pruning algorithms In contrast other languages CuPit-2 programs compiled efficient code for parallel machines without any changes the source program an easy start u>
Paper 4:  <Title: Quicknet on MultiSpert: Fast Parallel Neural Network Training  .   Abstract: The MultiSpert parallel system the Spert workstation accelerator predominantly speech recognition research at ICSI In order deliver for Artificial Neural Network training without changes the user interfaces the exisiting Quicknet ANN library modified run MultiSpert In this report the algorithms the parallelization the Quicknet code analyse their communication and computation requirements The resulting performance model yields system>
Paper 5:  <Title: Connectionist Layered Object-Oriented Network Simulator (CLONES): User's Manual minimize the learning curve for using CLONES,.   Abstract: CLONES is a object-oriented library constructing, training utilizing layered connectionist networks The CLONES library all the object classes needed a simulator with a small amount added source code ( The size experimental ANN programs greatly an object-oriented library; at these programs easier evolve The library includes database network behavior training procedures customized It designed run efficiently data parallel computerssu>
Paper 7:  <Title: Programming Environment for a High Performance Parallel Supercomputer with Intelligent Communication  .   Abstract: At the Electronics Lab of Techology (ETH the high performance Parallel Supercomputer MUSICMUlti processor System with Intelligent Communication has beed developed As applications in neural network simulation molecular dynamics show the Electronics Lab Supercomputer absolutely on a par those electric power requirements reduced 1000 wight is reduced 400 price Software development a key using such a parallel system This>
Paper 8:  <Title: 17 Massively Parallel Genetic Programming  .   Abstract: As the field Genetic Programming (GP) matures its breadth parallel implementations becomes absolutely The transputer-based system presented the chapter by Koza Andre ([11 one the rare such parallel implementations Until today no implementation parallel GP using except ([20 although others exploited workstation farms One reason certainly the apparent difficulty dealing the parallel evaluation>
Paper 10:  <Title: Simulation of Reduced Precision Arithmetic for Digital Neural Networks Using the RAP Machine  .   Abstract: This paper some our recent work computer architectures efficient execution artificial neural network algorithms Our earlier system, the Ring Array Processor based commercial DSPs with a low-latency ring interconnection scheme We used the RAP to simulate variable precision arithmetic guide us higher performance neurocomputers based custom VLSI The RAP system played this study enabling experiment much larger networks would O>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Neural Networks

Neural Networks
Prediction:  Neural Networks
Is prediction correct?  True

Prediction: 1
Processing index 732...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Statistical Queries and Faulty PAC Oracles  
Abstract: Abstract: In this paper we study learning in the PAC model of Valiant [18] in which the example oracle used for learning may be faulty in one of two ways: either by misclassifying the example or by distorting the distribution of examples. We first consider models in which examples are misclassified. Kearns [12] recently showed that efficient learning in a new model using statistical queries is a sufficient condition for PAC learning with classification noise. We show that efficient learning with statistical queries is sufficient for learning in the PAC model with malicious error rate proportional to the required statistical query accuracy. One application of this result is a new lower bound for tolerable malicious error in learning monomials of k literals. This is the first such bound which is independent of the number of irrelevant attributes n. We also use the statistical query model to give sufficient conditions for using distribution specific algorithms on distributions outside their prescribed domains. A corollary of this result expands the class of distributions on which we can weakly learn monotone Boolean formulae. We also consider new models of learning in which examples are not chosen according to the distribution on which the learner will be tested. We examine three variations of distribution noise and give necessary and sufficient conditions for polynomial time learning with such noise. We show containments and separations between the various models of faulty oracles. Finally, we examine hypothesis boosting algorithms in the context of learning with distribution noise, and show that Schapire's result regarding the strength of weak learnabil-ity [17] is in some sense tight in requiring the weak learner to be nearly distribution free. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 2:  <Title: On Learning from Noisy and Incomplete Examples  .   Abstract: We investigate learnability the PAC model used learning, attributes labels, either corrupted prove our main results define a new complexity measure on statistical query The view an SQ algorithm the maximum over all queries of input bits on We show a restricted view SQ algorithm a general sufficient condition learnability both the models attribute noise covered ( missing We further show>
Label: Theory
Paper 8:  <Title: Boosting a weak learning algorithm by majority To be published in Information and Computation  .   Abstract: We present an algorithm improving algorithms learning binary concepts The improvement achieved combining hypotheses each generated training given learning examples Our algorithm ideas presented Schapire inThe strength weak learnability represents an improvement his results The analysis our algorithm provides general upper bounds the resources required learning in Valiant's polynomial PAC learning framework are the best genera>
Label: Theory
Paper 9:  <Title: Randomly Fallible Teachers: Learning Monotone DNF with an Incomplete Membership Oracle  .   Abstract: We introduce algorithmic learning an equivalence oracle an incomplete membership oracle answers a random subset the learner's membership queries may missing We demonstrate high probability it still learn monotone DNF formulas provided the fraction missing answers bounded some constant less Even half the membership queries expected yield no information our algorithm exactly identify m-term, n-variable monotone DNF formulas an exp>
Label: Theory
Paper 3:  <Title: Improved Noise-Tolerant Learning and Generalized Statistical Queries  .   Abstract: The statistical query learning model viewed creating ( demonstrating the existence noise-tolerant learning algorithms the PAC model The complexity a statistical query algorithm conjunction simulating SQ algorithms the PAC model with noise determine the noise-tolerant PAC algorithms produced Although roughly optimal upper bounds shown the complexity statistical query learning the corresponding noise-tolerant PAC algorithms due inefficient simulations In this paper>
Paper 4:  <Title: General Bounds on Statistical Query Learning and PAC Learning with Noise via Hypothesis Boosting  .   Abstract: We derive general bounds learning the Statistical Query model the PAC model with classification noise We do so considering boosting weak learning algorithms which fall the Statistical Query model This new model Kearns [12 efficient PAC learning the presence classification noise We first show a general scheme boosting weak SQ learning algorithms proving equivalent The boosting is effi>
Paper 5:  <Title: Learning Switching Concepts  .   Abstract: We consider learning in situations the function used classify examples switch back during We examine several models such situations oblivious models in switches independent the selection examples more adversarial models a single adversary both the concept switches example selection We show relationships the more benign models the p-concepts Kearns Schapire learning switches betw>
Paper 6:  <Title: Simulating Access to Hidden Information while Learning  .   Abstract: We introduce which enables a learner without access hidden information learn nearly as We apply our technique solve an open problem of Maass and Turan [18 showing for any concept class F, the least number queries sufficient learning by an algorithm which access only arbitrary equivalence queries at most a factor 1= log4=3 more membership que>
Paper 7:  <Title: 25 Learning in Hybrid Noise Environments Using Statistical Queries  .   Abstract: We consider formal models learning from noisy data Specifically focus learning in the probability approximately correct model as Valiant Two noise this setting classification noise malicious errors However a more realistic model combining noise We define a learning environment a natural combination these two noise models We first show hypothesis testing We next describe learning in this model>
Paper 10:  <Title: On the Sample Complexity of Weakly Learning  .   Abstract: In study the sample complexity weak learning. That we ask how much data must an unknown distribution extract a small but significant advantage prediction We show it those learning algorithms that output deterministic hypotheses randomized We prove in the weak learning model any algorithm using deterministic hypotheses to weakly learn Vapnik-Chervonenkis dimension d(n requires ( examples In contrast when randomized hypotheses>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Theory

Theory
Prediction:  Theory
Is prediction correct?  True

Prediction: 1
Processing index 837...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Inductive Database Design  
Abstract: Abstract: When designing a (deductive) database, the designer has to decide for each predicate (or relation) whether it should be defined extensionally or intensionally, and what the definition should look like. An intelligent system is presented to assist the designer in this task. It starts from an example database in which all predicates are defined extensionally. It then tries to compact the database by transforming extensionally defined predicates into intensionally defined ones. The intelligent system employs techniques from the area of inductive logic programming. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 2:  <Title: Applications of a logical discovery engine  .   Abstract: The clausal discovery engine claudien presented claudien discovers regularities data a representative As such represents data regularities by first order clausal theories Because the search space clausal theories larger attribute value representation claudien also accepts the language bias determines Whereas other papers claudien focuss on the semantics or logical problem specification>
Label: Rule Learning
Paper 3:  <Title: Constructing Intermediate Concepts by Decomposition of Real Functions  .   Abstract: In learning from examples it expand an attribute-vector representation intermediate concepts The usual advantage such structuring of the learning problem easier improves induced descriptions In develop discovering useful intermediate concepts when both the class the attributes real The technique a decomposition method originally the design switching circuits recently extended handle incompletely specified multivalued>
Paper 4:  <Title: FONN: Combining First Order Logic with Connectionist Learning  .   Abstract: This paper manage structured data refine knowledge bases expressed a first order logic language The presented framework well classification problems concept de scriptions depend numerical features In fact the main goal the neural architecture that refining the numerical part without changing In particular discuss a method translate a set classification rules neural computation units Here, focus the translat>
Paper 5:  <Title: Inductive Constraint Logic and the Mutagenesis Problem  .   Abstract: A novel approach learning first order logic formulae incorporated a system named ICL In ICL examples viewed interpretations which true for the target theory present inductive logic programming systems true and false ground facts clauses Furthermore ICL uses a clausal representation corresponds a conjunctive normal form where forms a constraint positive examples classical learning techniques concentrated concept re>
Paper 6:  <Title: Computation and Psychophysics of Sensorimotor Integration  .   Abstract: In learning classification rules data We sketch two modules our architecture namely LINNEO + and GAR LINNEO +, which a knowledge acquisition tool ill-structured domains automatically generating classes examples incrementally works LINNEO + 's output, a representation the conceptual structure classes the input GAR that classification rules GAR can generate both conjunctive and disjunctive r>
Paper 7:  <Title: Rule Generation and Compaction in the wwtp  .   Abstract: In learning classification rules data We sketch two modules our architecture namely LINNEO + and GAR LINNEO +, which a knowledge acquisition tool ill-structured domains automatically generating classes examples incrementally works LINNEO + 's output, a representation the conceptual structure classes the input GAR that classification rules GAR can generate both conjunctive and disjunctive r>
Paper 8:  <Title: Multi-Strategy Learning and Theory Revision  .   Abstract: This paper the system WHY, which learns and updates a diagnostic knowledge base domain knowledge a set examples The a-priori knowledge consists a causal model the domain stating basic phenomena a body describing the links abstract concepts their possible manifestations The phenomenological knowledge is used deductively the causal model abductively the examples The problems imperfection the theory handled al>
Paper 9:  <Title: Rule Based Database Integration in HIPED Heterogeneous Intelligent Processing in Engineering Design  .   Abstract: In this paper 1 we one aspect our research the project called HIPED addressed performing design engineering devices accessing heterogeneous databases The front end the HIPED system consisted interactive KRI-TIK a multimodal reasoning system combined case based model This paper focuses the backend processing where five types queries received mapping appropriately using the "facts about "rules>
Paper 10:  <Title: Learning by Refining Algorithm Sketches  .   Abstract: In this paper suggest improves significantly a top-down inductive logic programming (ILP) learning system This improvement achieved at giving to extra information difficult formulate This information appears an algorithm sketch: an incomplete and somewhat vague representation the computation related a particular example We describe which sketches admissible give details the learning algorithm exploits The experiments carried>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Rule Learning

Rule Learning
Prediction:  Rule Learning
Is prediction correct?  True

Prediction: 1
Processing index 103...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: NEUROCONTROL BY REINFORCEMENT LEARNING  
Abstract: Abstract: Reinforcement learning (RL) is a model-free tuning and adaptation method for control of dynamic systems. Contrary to supervised learning, based usually on gradient descent techniques, RL does not require any model or sensitivity function of the process. Hence, RL can be applied to systems that are poorly understood, uncertain, nonlinear or for other reasons untractable with conventional methods. In reinforcement learning, the overall controller performance is evaluated by a scalar measure, called reinforcement. Depending on the type of the control task, reinforcement may represent an evaluation of the most recent control action or, more often, of an entire sequence of past control moves. In the latter case, the RL system learns how to predict the outcome of each individual control action. This prediction is then used to adjust the parameters of the controller. The mathematical background of RL is closely related to optimal control and dynamic programming. This paper gives a comprehensive overview of the RL methods and presents an application to the attitude control of a satellite. Some well known applications from the literature are reviewed as well. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 4:  <Title: INCREMENTAL POLYNOMIAL CONTROLLER NETWORKS: two self-organising non-linear controllers  .   Abstract: Temporal differenceTD) methods constitute learning predictions parameterized a recency factor. Currently the most important application these methods temporal credit assignment reinforcement learning Well known reinforcement learning algorithms AHC may viewed instances TD learning This paper the efficient and general implementation TD() arbitrary, for use reinforcement learning algorithms optimizing the discounted sum rewards The tradi>
Label: Neural Networks
Paper 9:  <Title: Learning Controllers for Industrial Robots  .   Abstract: One the most significant cost factors robotics applications the design real-time robot control software Control theory helps when linear controllers have to developed, sufficiently support the generation although in ( compliance control nonlinear control essential achieving This paper discusses Machine Learning has applied the design (non-)linear controllers Several alternative function approximators, Multilayer Perceptrons Radial>
Label: Neural Networks
Paper 2:  <Title: Modeling the Student with Reinforcement Learning  .   Abstract: We describe a methodology enabling an intelligent teaching system make high level strategy decisions on low level student modeling information This framework less costly construct superior hand coding teaching strategies as responsive In order accomplish reinforcement learning learn associate superior teaching actions certain states the student's knowledge Reinforcement learning (RL flexible handling noisy data does need expert domain knowledge A dr>
Paper 3:  <Title: AVERAGED REWARD REINFORCEMENT LEARNING APPLIED TO FUZZY RULE TUNING  .   Abstract: Fuzzy rules control can effectively tuned reinforcement learning Reinforcement learning only information the control application The tuning process allows people generate fuzzy rules unable accurately perform control have them tuned rules provide This paper a new simplified method using the tuning fuzzy control rules It shown the learned fuzzy rules provide smoother control the pole balancing domain t>
Paper 5:  <Title: Automatic Generation of Adaptive Programs Automatic Generation of Adaptive Programs. In From Animals to Animats.   Abstract: Fuzzy rules control can effectively tuned reinforcement learning Reinforcement learning only information the control application The tuning process allows people generate fuzzy rules unable accurately perform control have them tuned rules provide This paper a new simplified method using the tuning fuzzy control rules It shown the learned fuzzy rules provide smoother control the pole balancing domain t>
Paper 6:  <Title: A Sampling-Based Heuristic for Tree Search Applied to Grammar Induction  .   Abstract: In the field Operation Research and Artificial Intelligence several stochastic search algorithms designed based global random searchZhigljavsky 1991 Basically those techniques iteratively sample the search space with respect a probability distribution which updated according previous samples some predefined strategy Genetic Algorithms ( (Goldberg 1989 or Greedy Randomized Adaptive Search Procedures (Feo Resende two particular instances this paradigm In SAGE al>
Paper 7:  <Title: Toward Learning Systems That Integrate Different Strategies and Representations TR93-22  .   Abstract: Temporal differenceTD) methods constitute learning predictions parameterized a recency factor. Currently the most important application these methods temporal credit assignment reinforcement learning Well known reinforcement learning algorithms AHC may viewed instances TD learning This paper the efficient and general implementation TD() arbitrary, for use reinforcement learning algorithms optimizing the discounted sum rewards The tradi>
Paper 8:  <Title: Category: Control, Navigation and Planning Preference: Oral presentation Exploiting Model Uncertainty Estimates for Safe Dynamic.   Abstract: Model learning combined dynamic programming control continuous state dynamic systems The simplest method assumes the learned model applies dynamic programming many approximators provide uncertainty estimates the fit How they exploited This paper addresses where the system must prevented having catastrophic failures learning We propose adapted the dual control literature use locally weighted stochastic dynamic prog>
Paper 10:  <Title: On the Computational Economics of Reinforcement Learning  .   Abstract: Following terminology adaptive control distinguish indirect learning methods learn explicit models the dynamic structure to controlled do We compare an existing indirect method a conventional dynamic programming algorithm a closely related direct reinforcement learning method by applying both methods an infinite horizon Markov decision problem with unknown state-transition probabilities The simulations although the direct method requires much less space dramatic>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Reinforcement Learning

Reinforcement Learning
Prediction:  Reinforcement Learning
Is prediction correct?  True

Prediction: 1
Processing index 1257...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: The Schema Theorem and Price's Theorem  
Abstract: Abstract: Holland's Schema Theorem is widely taken to be the foundation for explanations of the power of genetic algorithms (GAs). Yet some dissent has been expressed as to its implications. Here, dissenting arguments are reviewed and elaborated upon, explaining why the Schema Theorem has no implications for how well a GA is performing. Interpretations of the Schema Theorem have implicitly assumed that a correlation exists between parent and offspring fitnesses, and this assumption is made explicit in results based on Price's Covariance and Selection Theorem. Schemata do not play a part in the performance theorems derived for representations and operators in general. However, schemata re-emerge when recombination operators are used. Using Geiringer's recombination distribution representation of recombination operators, a "missing" schema theorem is derived which makes explicit the intuition for when a GA should perform well. Finally, the method of "adaptive landscape" analysis is examined and counterexamples offered to the commonly used correlation statistic. Instead, an alternative statistic | the transmission function in the fitness domain | is proposed as the optimal statistic for estimating GA performance from limited samples.
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 5:  <Title: Effects of Occam's Razor in Evolving Sigma-Pi Neural Nets  .   Abstract: Several evolutionary algorithms make hierarchical representations variable size rather linear strings Variable complexity the structures provides an additional representational power which widen evolutionary algorithms The price, however that the search space open- solutions grow arbitrarily large size In study structural complexity the solutions their generalization performance analyzing the fitness landscape sigma-pi neural networks The analys>
Label: Genetic Algorithms
Paper 8:  <Title: A STUDY OF CROSSOVER OPERATORS IN GENETIC PROGRAMMING  .   Abstract: Holland's analysis the sources power of genetic algorithms served guidance the applications The technique applying a recombination operatorcrossover a population individuals a key that power Neverless there contradictory results concerning crossover operators with overall performance Recently for genetic algorithms design neural network modules and their control circuits In these studies a genetic algorithm without crossover outperformed>
Label: Genetic Algorithms
Paper 2:  <Title: The Troubling Aspects of a Building Block Hypothesis for Genetic Programming  .   Abstract: In carefully formulate a Schema Theorem Genetic ProgrammingGP a schema definition accounts the variable length GP's representation In a manner early GA research use interpretations our GP Schema Theorem obtain a GP Building Block definition state): that GP searches by hierarchically combining We report this approach convincing difficult find support the promotion combination>
Paper 3:  <Title: The Role of Development in Genetic Algorithms  .   Abstract: Technical Report Number CS94394 Computer Science Abstract The developmental mechanisms transforming to typically omitted formulationsGAs these two representational spaces identical We argue developmental mechanisms useful understanding the success several standard GA techniques can clarify more recently proposed enhancements We provide distinguishes two developmental mechanisms | learning an>
Paper 4:  <Title: Submitted to Circuits, Systems and Signal Processing Neural Network Constructive Algorithms: Trading Generalization for Learning Efficiency?  .   Abstract: There several types constructive, or growth algorithms training This paper and the main ones using The claimed convergence properties the algorithms verified just two mapping theorems consequently enables unified a basic mechanism The algorithms compared and the deficiencies some highlighted. The fundamental reasons the actual success these al>
Paper 6:  <Title: MLC Tutorial A Machine Learning library of C classes.  .   Abstract: Several evolutionary algorithms make hierarchical representations variable size rather linear strings Variable complexity the structures provides an additional representational power which widen evolutionary algorithms The price, however that the search space open- solutions grow arbitrarily large size In study structural complexity the solutions their generalization performance analyzing the fitness landscape sigma-pi neural networks The analys>
Paper 7:  <Title: A Sampling-Based Heuristic for Tree Search Applied to Grammar Induction  .   Abstract: In the field Operation Research and Artificial Intelligence several stochastic search algorithms designed based global random searchZhigljavsky 1991 Basically those techniques iteratively sample the search space with respect a probability distribution which updated according previous samples some predefined strategy Genetic Algorithms ( (Goldberg 1989 or Greedy Randomized Adaptive Search Procedures (Feo Resende two particular instances this paradigm In SAGE al>
Paper 9:  <Title: DISTRIBUTED GENETIC ALGORITHMS FOR PARTITIONING UNIFORM GRIDS  .   Abstract: The fault hierarchy representation widely expert systems the diagnosis complex mechanical devices On the assumption an appropriate bias a knowledge representation language also learning in a theory revision method operates directly a fault hierarchy This task presents several challenges: A typical training instance missing most feature values the pattern significant rather merely an effect noise Moreover the accuracy a candidate theory measure>
Paper 10:  <Title: Facing The Facts: Necessary Requirements For The Artificial Evolution of Complex Behaviour  .   Abstract: This paper sets the open-ended artificial evolution complex behaviour autonomous agents If recurrent dynamical neural networksor similar phenotypes a Genetic that employs variable length genotypes, Inman Harvey's SAGA capable evolving arbitrary levels be-havioural complexity Furthermore with simple restrictions the encoding scheme that governs how genotypes develop into guaranteed if an increase fitness requires be-havioural complexity>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Genetic Algorithms

Genetic Algorithms
Prediction:  Genetic Algorithms
Is prediction correct?  True

Prediction: 1
Processing index 1147...
Please predict the most appropriate category for the paper. Choose from the following categories:

Rule Learning
Neural Networks
Case Based
Genetic Algorithms
Theory
Reinforcement Learning
Probabilistic Methods

Title: Title: Decomposable graphical Gaussian model determination  
Abstract: Abstract: We propose a methodology for Bayesian model determination in decomposable graphical Gaussian models. To achieve this aim we consider a hyper inverse Wishart prior distribution on the concentration matrix for each given graph. To ensure compatibility across models, such prior distributions are obtained by marginalisation from the prior conditional on the complete graph. We explore alternative structures for the hyperparameters of the latter, and their consequences for the model. Model determination is carried out by implementing a reversible jump MCMC sampler. In particular, the dimension-changing move we propose involves adding or dropping an edge from the graph. We characterise the set of moves which preserve the decomposability of the graph, giving a fast algorithm for maintaining the junction tree representation of the graph at each sweep. As state variable, we propose to use the incomplete variance-covariance matrix, containing only the elements for which the corresponding element of the inverse is nonzero. This allows all computations to be performed locally, at the clique level, which is a clear advantage for the analysis of large and complex data-sets. Finally, the statistical and computational performance of the procedure is illustrated by means of both artificial and real multidimensional data-sets. 
The following are related papers to this paper, please consider the content of these papers and making a judgment.
Paper 7:  <Title: Wavelet Thresholding via a Bayesian Approach  .   Abstract: We discuss a Bayesian formalism gives wavelet threshold estimation A prior distribution imposed the wavelet coefficients the unknown response function designed capture common most applications For prior specified, the posterior median yields Our prior model the underlying function can adjusted give functions falling any specific Besov space We establish a relation the prior model tho>
Label: Probabilistic Methods
Paper 2:  <Title: On MCMC Sampling in Hierarchical Longitudinal Models  SUMMARY  .   Abstract: Markov chain Monte Carlo (MCMC) algorithms Bayesian practice In their simplest form ( when parameters updated one, however often slow converge applied high-dimensional statistical models A remedy block the parameters into groups updated simultaneously using either a Gibbs or Metropolis-Hastings step In this paper construct several (partially and fully blocked) MCMC algorithms minimizing arising important classes longitudinal data mo>
Paper 3:  <Title: Reparameterisation Issues in Mixture Modelling and their bearing on MCMC algorithms  .   Abstract: There increasing need efficient estimation mixture distributions following these as modelling tools many applied fields We propose in a Bayesian noninformative approach normal mixtures which relies a reparameterisation the secondary components in divergence As well providing an intuitively appealing representation at the modelling stage this reparameterisation important bearing both the prior distribution MCMC>
Paper 4:  <Title: On Bayesian analysis of mixtures with an unknown number of components  Summary  .   Abstract: New methodology fully Bayesian mixture analysis developed making reversible jump Markov chain Monte Carlo methods that capable jumping the parameter subspaces corresponding different numbers components A sample from the full joint distribution all unknown variables thereby generated this can a thorough presentation many aspects the posterior distribution The methodology applied here the analysis univariate normal mixtures using a hierarchical prior model offers an approach>
Paper 5:  <Title: Empirical Entropy Manipulation for Real-World Problems  .   Abstract: No finite sample, a signal directly Some assumption either the functional form the density about necessary Both amount a prior over the space possible density functions By far assume the density has By contrast we derive a differential learning rule called EMMA entropy way kernel density estimation Entropy its derivative can then sampling this density estimate The resulting p>
Paper 6:  <Title: FROM METROPOLIS TO DIFFUSIONS: GIBBS STATES AND OPTIMAL SCALING  .   Abstract: This paper the behaviour the random walk Metropolis algorithm high dimensional problems Here concentrate the components the target density is a spatially homogeneous Gibbs distribution finite range The performance the algorithm strongly linked phase transition for the Gibbs distribution; the convergence time being linear dimension for problems Related to an optimal way scale the variance the proposal distribution order t>
Paper 8:  <Title: Bayesian Regression Filters and the Issue of Priors  .   Abstract: We propose regression problems covers areas which usually dealt function approximation An online learning algorithm derived which solves regression problems a Kalman filter Its solution always improves increasing model complexity without the infinite dimension limit it approaches true Bayesian posterior The issues prior selection showing misleading The practical implementation summarised. Simulations using>
Paper 9:  <Title: Convergence controls for MCMC algorithms, with applications to hidden Markov chains  .   Abstract: In complex models like hidden Markov chains the convergence the MCMC algorithms used approximate and the Bayes estimates interest must controlled We propose in on controls, rely classical non-parametric tests independence the start-up distribution stability These tests lead graphical control spreadsheets presented normal mixture hidden Markov chains to compare the full Gibbs s>
Paper 10:  <Title: Mixtures of Probabilistic Principal Component Analysers  .   Abstract: Principal component analysis one processing, compressing visualising although its effectiveness its global linearity While nonlinear variants PCA an alternative paradigm capture data complexity a combination local linear PCA projections. However conventional PCA does correspond a probability density, there no unique way combine Previous attempts formulate mixture models PCA therefore to some extent ad In this paper PCA is form>
Do not give any reasoning or logic for your answer.
Answer: 



Ideal_answer: Probabilistic Methods

Probabilistic Methods
Prediction:  Probabilistic Methods
Is prediction correct?  True

Prediction: 1
Accuracy: 0.72
Wrong indexes: [1759, 474, 2082, 2470, 1021, 955, 1434, 2641, 2648, 2431, 1152, 975, 1139, 954, 1853, 2652, 616, 1519, 469, 2686, 417, 1827, 1534, 487, 1238, 1, 2534, 537]
Wrong list: [2, 5, 12, 16, 30, 31, 33, 34, 35, 36, 42, 44, 46, 47, 50, 54, 56, 59, 62, 63, 64, 67, 68, 70, 72, 84, 86, 90]
Wrong indexes length: 28
