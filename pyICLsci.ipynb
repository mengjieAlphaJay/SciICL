{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ogb'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m process_and_compare_predictions, load_data, sample_test_nodes\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mHQprompt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m promptGenerate\n",
      "File \u001b[1;32md:\\Download_chrome\\Classifcation-Task\\utils\\utils.py:2\u001b[0m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload_arxiv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_raw_text_arxiv\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload_cora\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_raw_text_cora\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload_pubmed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_raw_text_pubmed\n",
      "File \u001b[1;32md:\\Download_chrome\\Classifcation-Task\\utils\\load_arxiv.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mReference: https://github.com/XiaoxinHe/TAPE/blob/main/core/data_utils/load_arxiv.py\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mogb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnodeproppred\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PygNodePropPredDataset\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mT\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ogb'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from utils.utils import process_and_compare_predictions, load_data, sample_test_nodes\n",
    "import sys\n",
    "from HQprompt import promptGenerate\n",
    "import requests\n",
    "requests.packages.urllib3.disable_warnings()\n",
    "import yaml\n",
    "import json\n",
    "from txtai.embeddings import Embeddings\n",
    "from prompts import generate_system_prompt\n",
    "import re\n",
    "import torch\n",
    "\"\"\" \n",
    "[] 多数据\n",
    "[] 多LLM\n",
    "[] 标准化ICL\n",
    "[] 信息，覆盖度量\n",
    "[] 神经网络检索器实现\n",
    "[] 引文网络策略\n",
    "[] 变量：标签? prompt? others ....\n",
    "\"\"\"\n",
    "class linkInContextAgent:\n",
    "    def __init__(self):\n",
    "        # read YAML\n",
    "        with open('class_cora.yaml', 'r') as file:\n",
    "            parameters = yaml.safe_load(file)\n",
    "        ssize = parameters['samplesize']\n",
    "        self.load_embedding = parameters['load_embedding']\n",
    "        self.dataname = parameters['dataset_name']\n",
    "        self.LLM = parameters['LLM']\n",
    "        self.maxp1 = parameters['max_papers'][0]\n",
    "        self.maxp2 = parameters['max_papers'][1]\n",
    "        self.stratage = parameters['stratage']\n",
    "        self.abs_len = parameters['abstract_len']\n",
    "        retry = parameters['retry']\n",
    "        wridx = parameters['wridx']\n",
    "        ## load data\n",
    "        self.data, self.text = load_data(self.dataname, use_text=True, seed=42)\n",
    "        if ssize == \"all\":\n",
    "            self.sample_size = len(self.data.test_id)\n",
    "        else:\n",
    "            self.sample_size = ssize\n",
    "        ##\n",
    "        idx_list = list(range(self.sample_size))\n",
    "        node_indices = sample_test_nodes(self.data, self.text, self.sample_size, self.dataname)\n",
    "        if retry == 'all':\n",
    "            self.node_index_list = [node_indices[idx] for idx in idx_list]\n",
    "        else:\n",
    "            self.node_index_list = [node_indices[idx] for idx in wridx]\n",
    "    \n",
    "    def logger(self,experName):\n",
    "        self.experName = experName\n",
    "        self.jsonfile = open('jsonfile/'+self.dataname+\"__\"+self.experName+'.jsonl','a')\n",
    "        self.f = open('results/'+self.dataname+\"__\"+self.experName+'.log', 'a')\n",
    "        sys.stdout = self.f\n",
    "        print(\"experName .... \",self.experName)\n",
    "    \n",
    "    def addJson(self,data):\n",
    "        self.jsonfile.write(json.dumps(data)+'\\n')\n",
    "\n",
    "    def Infer(self,inferencer):\n",
    "        self.infer = inferencer\n",
    "\n",
    "    def closefile(self):\n",
    "        self.jsonfile.close()\n",
    "        #self.f.close()\n",
    "    \n",
    "class Paper:\n",
    "    def __init__(self,text,idx):\n",
    "        self.title = text['title'][idx]\n",
    "        self.abs = text['abs']\n",
    "\n",
    "\n",
    "\n",
    "class Inferencer:\n",
    "    def __init__(self):\n",
    "        self.result = {}\n",
    "        self.response = \"\"\n",
    "    def get_completion_from_messages(self,messages, \n",
    "                                    model=\"gpt-3.5-turbo\", \n",
    "                                    temperature=0, max_tokens=800):\n",
    "        #OPENAI_API_BASE_URL\n",
    "        #url = \"https://freegpt35.tomleung.cn/v1/chat/completions\"\n",
    "        url = \"https://api.openai-hk.com/v1/chat/completions\"\n",
    "\n",
    "        headers = {\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"Authorization\": \"Bearer hk-2q6mjv1000004445c0d228e1e0755764cd8179f916784d37\"\n",
    "            }\n",
    "\n",
    "        data = {\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"model\": model,\n",
    "                \"temperature\": temperature,\n",
    "                \"top_p\": 1,\n",
    "                \"presence_penalty\": 1,\n",
    "                'messages':messages\n",
    "            }\n",
    "        response = requests.post(url, headers=headers, data=json.dumps(data).encode('utf-8') )\n",
    "        rc = response.content\n",
    "        cont = json.loads(rc)\n",
    "        print(\"cont....\",cont)\n",
    "        #cont = ast.literal_eval(str(rc.decode('utf-8')))#[\"content\"]\n",
    "        return  cont[\"choices\"][0]['message']['content']\n",
    "\n",
    "\n",
    "    def LLMsInfer(self,message,print_out=True):\n",
    "        ## 单线程        \n",
    "        if print_out:\n",
    "            print(message[0]['content'], end=\"\\n\\n\")\n",
    "            print(message[1]['content'], end=\"\\n\\n\")      \n",
    "        \n",
    "        # Get completion message and print\n",
    "        ### run GPT\n",
    "        self.response = self.get_completion_from_messages(message)\n",
    "        if print_out:\n",
    "            print(\"GPT-response...\",self.response)\n",
    "        return\n",
    "\n",
    "    def verifyAnswer(self,text,idx):\n",
    "        ideal_answer = text['label'][idx]\n",
    "        print(\"Ideal_answer:\", ideal_answer, end=\"\\n\\n\")\n",
    "        prediction = self.response if self.response is not None else \"No answer\"\n",
    "        pattern = re.compile(re.escape(ideal_answer), re.IGNORECASE)\n",
    "\n",
    "        if prediction is not None:\n",
    "            print(\"Prediction: \", prediction)\n",
    "            # Compare the prediction with ideal_answer\n",
    "            #print(\"Is prediction correct? \", prediction == ideal_answer, end=\"\\n\\n\")\n",
    "            match = pattern.search(prediction)\n",
    "            print(\"Is prediction correct? \", bool(match), end=\"\\n\\n\")\n",
    "            return int(prediction == ideal_answer)\n",
    "        else:\n",
    "            print(\"No valid prediction could be made.\")\n",
    "        return\n",
    "    #self.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prompter(linkInContextAgent):\n",
    "    ## for one node\n",
    "    def __init__(self,nidx):\n",
    "        super().__init__()\n",
    "        self.nidx = nidx\n",
    "        self.neibor_indices = []\n",
    "        self.instructs = {\"embed\":f\"The following are related papers to this paper, please consider the content of these papers and making a judgment. Do not give any reasoning or logic for your answer.\\n\",\n",
    "        \"citation\":f\"It has following neighbor papers at hop one. It has following neighbor papers at hop one, please consider the content of these papers and making a judgment. Do not give any reasoning or logic for your answer.\\n\"} ## cites ...\n",
    "    \n",
    "    def retriever(self,*args):\n",
    "        if self.stratage == 'knn':\n",
    "            self.instruct = self.instructs[\"embed\"]\n",
    "            return self.embed_knn()\n",
    "        elif self.stratage == 'citation':\n",
    "            self.instruct = self.instructs[\"citation\"]\n",
    "            return self.retrieveCitation(*args)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def zeroShotPrompt(self,idx):\n",
    "        message =  [{'role': 'system', 'content':generate_system_prompt(self.dataname)},{'role':'user', 'content': self.concatPaper(idx)}+'\\n'+\"please consider the content of the paper and make a judgment. Do not give any reasoning or logic for your answer.\\n\"]\n",
    "        answer = self.infer.LLMsInfer(message,print_out = False)\n",
    "        return answer\n",
    "\n",
    "    def get_subgraph(self, edge_index, hop=1):\n",
    "        current_nodes = torch.tensor([self.nidx])\n",
    "        all_hops = []\n",
    "\n",
    "        for _ in range(hop):\n",
    "            mask = torch.isin(edge_index[0], current_nodes) | torch.isin(edge_index[1], current_nodes)\n",
    "            \n",
    "            # Add both the source and target nodes involved in the edges \n",
    "            new_nodes = torch.unique(torch.cat((edge_index[0][mask], edge_index[1][mask])))\n",
    "\n",
    "            # Remove the current nodes to get only the new nodes added in this hop\n",
    "            diff_nodes_set = set(new_nodes.numpy()) - set(current_nodes.numpy())\n",
    "            diff_nodes = torch.tensor(list(diff_nodes_set))  \n",
    "            \n",
    "            all_hops.append(diff_nodes.tolist())\n",
    "\n",
    "            # Update current nodes for the next iteration\n",
    "            current_nodes = torch.unique(torch.cat((current_nodes, new_nodes)))\n",
    "\n",
    "        return all_hops\n",
    "\n",
    "    def standICL(self):\n",
    "        \"\"\"\n",
    "        .....\n",
    "        ## ..... \n",
    "        paper1:\n",
    "        label:\n",
    "        paper2:\n",
    "        label:\n",
    "        query paper:\n",
    "        .....\n",
    "        \"\"\"\n",
    "        return\n",
    "    \n",
    "    def concatPaper(self,nidx,if_lable=False):\n",
    "        title = self.text['title'][nidx]\n",
    "        prompt_str = f\"<Title: {title[6:]}||| \"\n",
    "        # Include abstract if required\n",
    "        abstract = self.text['abs'][nidx]\n",
    "        prompt_str = prompt_str+f\"Abstract: {abstract[10:self.abs_len]}>\\n\" \n",
    "        if if_lable:\n",
    "           label = self.text['label'][nidx]\n",
    "           prompt_str +=  f\"Label: {label}\\n\"     \n",
    "        return prompt_str\n",
    "\n",
    "    def standard_stratage(self):\n",
    "        \"\"\"\n",
    "        for one node prompt:\n",
    "        \"\"\"\n",
    "        #instruct = f\"The following are related papers to this paper, please consider the content of these papers and making a judgment. Do not give any reasoning or logic for your answer.\\n\"\n",
    "        prompt = {}\n",
    "        prompt['index'] = self.nidx\n",
    "        prompt['task'] = self.instruct\n",
    "        ## get ori paper:\n",
    "        prompt['ori_paper'] = self.concatPaper(self.nidx)\n",
    "        prompt['relv_papers'] = []\n",
    "\n",
    "        Target_word = \"Paper\"\n",
    "        for i, neighbor_idx in enumerate(self.neibor_indices):\n",
    "            if i==0:\n",
    "                continue\n",
    "            if (self.data.train_mask[neighbor_idx] or self.data.val_mask[neighbor_idx]):\n",
    "                if_label = True\n",
    "            else:\n",
    "                if_label = False\n",
    "            prompt['relv_papers'].append(self.concatPaper(neighbor_idx,if_label))\n",
    "        return prompt\n",
    "\n",
    "    def retrieveCitation(self, hop):\n",
    "        \"\"\"\n",
    "        Handle neighbors when attention is not used.\n",
    "        Returns:\n",
    "            str: String containing information about standard neighbors.\n",
    "        \"\"\"\n",
    "        all_hops = self.get_subgraph(self.data.edge_index, hop=1)\n",
    "\n",
    "        for h in range(0, hop):\n",
    "            neighbors_at_hop = all_hops[h]\n",
    "            neighbors_at_hop = np.array(neighbors_at_hop)\n",
    "            neighbors_at_hop = np.unique(neighbors_at_hop)\n",
    "            if h == 0:\n",
    "                neighbors_at_hop = neighbors_at_hop[:self.maxp1]\n",
    "            else:\n",
    "                neighbors_at_hop = neighbors_at_hop[:self.maxp2]\n",
    "\n",
    "            if len(neighbors_at_hop) > 0:\n",
    "                self.neibor_indices = neighbors_at_hop\n",
    "            else:\n",
    "                self.neibor_indices = [self.nidx]\n",
    "        return\n",
    "\n",
    "    def concatPrompt(self,prompt):\n",
    "        prompt['sys_promt'] = generate_system_prompt(self.dataname)\n",
    "        userpt = prompt['ori_paper']+'\\n'+prompt['task']+'\\n'\n",
    "        init_str = \"\"\n",
    "        for relpp,i in zip(prompt['relv_papers'],range(len(prompt['relv_papers']))):\n",
    "            init_str+=f\"paper {i+1}: {relpp}\"\n",
    "        userpt+=init_str\n",
    "        return [{'role':'system', 'content': prompt['sys_promt']}, {'role':'user', 'content': f\"{userpt}\"}]\n",
    "\n",
    "    def createIndex(self,alltext):\n",
    "        alltxt = []\n",
    "        for i in range(len(alltext['title'])):\n",
    "            alltxt.append(alltext['title'][i]+\"|| \"+alltext['abs'][i])\n",
    "        # Create embeddings model, backed by sentence-transformers & transformers\n",
    "        embeddings = Embeddings(path=\"allenai/scibert_scivocab_uncased\")\n",
    "        # Index the list of text\n",
    "        embeddings.index(alltxt)\n",
    "        return embeddings\n",
    "    \n",
    "    def embed_knn(self):\n",
    "        query = self.text\n",
    "        if self.load_embedding:\n",
    "            embeddings = Embeddings()\n",
    "            embeddings.load(\"core_index\")\n",
    "        else:\n",
    "            embeddings = self.createIndex(query)\n",
    "            embeddings.save(\"core_index\")\n",
    "        #max_paper_1 = 3\n",
    "        all_neighbor = embeddings.search(query['title'][self.nidx]+query['abs'][self.nidx], self.maxp1)\n",
    "        self.neibor_indices = [all_neighbor[i][0] for i in range(self.maxp1)]\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = linkInContextAgent()\n",
    "agent.logger('509_dlamung')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Prompter.retriever() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m prom1 \u001b[38;5;241m=\u001b[39m Prompter(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#prom1.embed_knn()\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mprom1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretriever\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m promt \u001b[38;5;241m=\u001b[39m prom1\u001b[38;5;241m.\u001b[39mstandard_stratage()\n",
      "\u001b[1;31mTypeError\u001b[0m: Prompter.retriever() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "prom1 = Prompter(2)\n",
    "#prom1.embed_knn()\n",
    "prom1.retriever(1)\n",
    "promt = prom1.standard_stratage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.addJson(promt)\n",
    "wholepromt = prom1.concatPrompt(promt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.Infer(Inferencer())\n",
    "agent.infer.LLMsInfer(wholepromt)\n",
    "agent.infer.verifyAnswer(agent.text,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 数据分析\n",
    "import json\n",
    "\n",
    "def read_jsonl_file(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                data.append(json.loads(line))\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e}\")\n",
    "    return data\n",
    "\n",
    "file_path = 'jsonfile/cora__421_dlamung.jsonl'\n",
    "data = read_jsonl_file(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Title: Title: Towards a General Distributed Platform for Learning and Generalization and Word Perfect Corp. 1 Introduction .| Abstract: Abstract: Different learning models employ different styles of generalization on novel inputs. This paper proposes the need for multiple styles of generalization to support a broad application base. The Priority ASOCS model (Priority Adaptive Self-Organizing Concurrent System) is overviewed and presented as a potential platform which can support multiple generalization styles. PASOCS is an adaptive network composed of many simple computing elements operating asynchronously and in parallel. The PASOCS can operate in either a data processing mode or a learning mode. During data processing mode, the system acts as a parallel hardware circuit. During learning mode, the PASOCS incorporates rules, with attached priorities, which represent the application being learned. Learning is accomplished in a distributed fashion in time logarithmic in the number of rules. The new model has significant learning time and space complexity improvements over previous models. Generalization in a learning system is at best always a guess. The proper style of generalization is application dependent. Thus, one style of generalization may not be sufficient to allow a learning system to support a broad spectrum of applications [14]. Current connectionist models use one specific style of generalization which is implicit in the learning algorithm. We suggest that the type of generalization used be a self-organizing parameter of the learning system which can be discovered as learning takes place. This requires a) a model which allows flexible generalization styles, and b) mechanisms to guide the system into the best style of generalization for the problem being learned. This paper overviews a learning model which seeks to efficiently support requirement a) above. The model is called Priority ASOCS (PASOCS) [9], which is a member of a class of models called ASOCS (Adaptive Self-Organizing Concurrent Systems) [5]. Section 2 of this paper gives an example of how different generalization techniques can approach a problem. Section 3 presents an overview of PASOCS. Section 4 illustrates how flexible generalization can be supported. Section 5 concludes the paper. \\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['relv_papers'][5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
